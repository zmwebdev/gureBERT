{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gureBERT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRIGPzfVoUlL",
        "colab_type": "text"
      },
      "source": [
        "#gureBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvKEb2yhhFc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gureBERT\n",
        "\n",
        "!git clone --recursive  https://github.com/zmwebdev/gureBERT\n",
        "%cd gureBERT\n",
        "\n",
        "!pip install sentencepiece\n",
        "!install -d spModels\n",
        "# eu\n",
        "!python src/sentence-split.py --config eu.config.ini --do_lower_case \n",
        "!python src/train-sentencepiece.py --config eu.config.ini\n",
        "\n",
        "!head -n 100 spModels/eu.vocab\n",
        "\n",
        "# en-eu\n",
        "!python src/sentence-split.py --config en-eu.config.ini --do_lower_case \n",
        "!python src/train-sentencepiece.py --config en-eu.config.ini\n",
        "\n",
        "!head -n 100 spModels/en-eu.vocab\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-KNyUCP6Rtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijk66GC0hIe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# google bucket\n",
        "!gsutil cp -r gs://gurebert/gureBERT/spModels ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk8_HLFhpLYA",
        "colab_type": "code",
        "outputId": "8bd06885-251c-40e4-8cad-4b6f7184ce8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "import tokenization_sentencepiece as tokenization\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    model_file=\"spModels/eu.model\",\n",
        "    vocab_file=\"spModels/eu.vocab\",\n",
        "    do_lower_case=True)\n",
        "\n",
        "text1 = \"Nere kotxea aitonaren etxe alboan dago, bere kolorea gorria da.\"\n",
        "\n",
        "print(tokenizer.tokenize(text1))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded a trained SentencePiece model.\n",
            "['▁ne', 're', '▁', 'ko', 'txea', '▁ai', 'ton', 'aren', '▁etxe', '▁alb', 'o', 'an', '▁dago', ',', '▁bere', '▁kolore', 'a', '▁gorria', '▁da', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9BPqkOUMHiz",
        "colab_type": "code",
        "outputId": "2a427b17-6386-4dfa-c7f2-10e196b3e510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "tokenizer_en_eu = tokenization.FullTokenizer(\n",
        "    model_file=\"spModels/en-eu.model\",\n",
        "    vocab_file=\"spModels/en-eu.vocab\",\n",
        "    do_lower_case=True)\n",
        "\n",
        "text1 = \"Nere kotxea aitonaren etxe alboan dago, bere kolorea gorria da.\"\n",
        "\n",
        "print(tokenizer_en_eu.tokenize(text1))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded a trained SentencePiece model.\n",
            "['▁ne', 're', '▁ko', 'txea', '▁a', 'it', 'on', 'aren', '▁etxe', '▁', 'albo', 'an', '▁dago', ',', '▁bere', '▁kolore', 'a', '▁gorria', '▁da', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojp8yYLUMUzt",
        "colab_type": "code",
        "outputId": "9cbdb8a0-0a74-4896-8cb8-30975d37c660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "text1 = \"The Italian cities of Milan and Cortina d'Ampezzo are chosen as the joint hosts of the 2026 Winter Olympics and Winter Paralympics.\"\n",
        "\n",
        "print(\"EN-EU: {}\".format(tokenizer_en_eu.tokenize(text1)))\n",
        "print(\"EU: {}\".format(tokenizer.tokenize(text1)))\n",
        "\n",
        "print(\"EN-EU: {}\".format(len(tokenizer_en_eu.tokenize(text1))))\n",
        "print(\"EU: {}\".format(len(tokenizer.tokenize(text1))))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EN-EU: ['▁the', '▁italian', '▁cities', '▁of', '▁milan', '▁and', '▁cort', 'ina', '▁d', \"'\", 'amp', 'e', 'zzo', '▁are', '▁chosen', '▁as', '▁the', '▁joint', '▁hosts', '▁of', '▁the', '▁20', '26', '▁winter', '▁olympics', '▁and', '▁winter', '▁para', 'lympic', 's', '.']\n",
            "EU: ['▁the', '▁italia', 'n', '▁cit', 'ies', '▁of', '▁milan', '▁and', '▁cort', 'ina', '▁d', \"'\", 'amp', 'ez', 'zo', '▁are', '▁ch', 'os', 'en', '▁as', '▁the', '▁jo', 'in', 't', '▁host', 's', '▁of', '▁the', '▁20', '26', '▁win', 'ter', '▁', 'oly', 'mp', 'ics', '▁and', '▁win', 'ter', '▁para', 'ly', 'mp', 'ics', '.']\n",
            "EN-EU: 31\n",
            "EU: 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNdhng5UqP6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-Training\n",
        "# https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/pretraining.ipynb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "#Check TPU devices\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c76u8zhRog86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GS = 'gs://gurebert/gureBERT'\n",
        "  \n",
        "#!gsutil cp -r spModels $GS/\n",
        "#!gsutil cp -r corpus $GS/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFlsE3KI4Zcd",
        "colab_type": "text"
      },
      "source": [
        "## EU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlujbeOnmPC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/create_pretraining_data.py \\\n",
        "    --input_file=$GS/corpus/eu/2014wiki.eu.sent_splited \\\n",
        "    --output_file=$GS/pretraining.tf.data \\\n",
        "    --model_file=spModels/eu.model \\\n",
        "    --vocab_file=spModels/eu.vocab \\\n",
        "    --do_lower_case=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWLKYF-Hmkb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!install -d gureBERT\n",
        "\n",
        "!python src/run_pretraining.py \\\n",
        "  --config_file eu.config.ini \\\n",
        "  --input_file=$GS/pretraining.tf.data \\\n",
        "  --output_dir=$GS/eu.gureBERT/test \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=256 \\\n",
        "  --max_seq_length=128 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=100 \\\n",
        "  --num_warmup_steps=10 \\\n",
        "  --save_checkpoints_steps=100 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "#  --num_train_steps=1000000 \\\n",
        "\n",
        "# num_train_steps ? zenbatekoa?\n",
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERgIisysvIh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pretraining jarraipena \n",
        "\n",
        "GS = 'gs://gurebert/gureBERT'\n",
        "\n",
        "!python src/run_pretraining.py \\\n",
        "  --config_file eu.config.ini \\\n",
        "  --input_file=$GS/pretraining.tf.data \\\n",
        "  --output_dir=$GS/eu.gureBERT/test \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=200 \\\n",
        "  --num_warmup_steps=10 \\\n",
        "  --save_checkpoints_steps=200 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  --init_checkpoint=$GS/eu.gureBERT/test \\\n",
        "#  --num_train_steps=1000000 \\\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAdbEYKv4P1m",
        "colab_type": "text"
      },
      "source": [
        "## EN-EU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbkJONUH77-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILES = \"./corpus/en-eu/2014wiki.eu.sent_splited,./corpus/en-eu/2019wiki-10k.en.sent_splited\"\n",
        "\n",
        "!python3 src/create_pretraining_data.py \\\n",
        "    --input_file={FILES} \\\n",
        "    --output_file=$GS/pretraining-en_eu.tf.data \\\n",
        "    --model_file=spModels/en-eu.model \\\n",
        "    --vocab_file=spModels/en-eu.vocab \\\n",
        "    --do_lower_case=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p8b1EVZ79Ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python src/run_pretraining.py \\\n",
        "  --config_file en-eu.config.ini \\\n",
        "  --input_file=$GS/pretraining-en_eu.tf.data \\\n",
        "  --output_dir=$GS/en-eu.gureBERT \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=20000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  --init_checkpoint=$GS/en-eu.gureBERT/ \\\n",
        "  #--num_train_steps=1000000 \\\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne5xtoXYgOvM",
        "colab_type": "text"
      },
      "source": [
        "## SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxSQy-Gu47Bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre training SQuAD\n",
        "\n",
        "# https://github.com/google-research/bert#squad-20\n",
        "\n",
        "#!git clone https://github.com/zmwebdev/bert.git\n",
        "#%cd bert\n",
        "\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
        "# evaluation script: download file from a url that returns a save dialog box : https://superuser.com/questions/795265/download-file-from-a-url-that-returns-a-save-dialog-box#795269\n",
        "!wget --content-disposition https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA0oUJsbBTaW",
        "colab_type": "code",
        "outputId": "3d4768dc-06bd-4e9e-b63f-2afc83b27bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        }
      },
      "source": [
        "# sentenpiece tokenizer erabili behar da!!: src/run_squad.py\n",
        "\n",
        "!gsutil cp -r gs://gurebert/gureBERT/spModels .\n",
        "\n",
        " \n",
        "!python src/run_squad.py \\\n",
        "  --vocab_file=./spModels/en-eu.vocab \\\n",
        "  --model_file=./spModels/en-eu.model \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --do_lower_case=True \\\n",
        "  --do_train=True \\\n",
        "  --train_file=train-v2.0.json \\\n",
        "  --do_predict=True \\\n",
        "  --predict_file=dev-v2.0.json \\\n",
        "  --train_batch_size=24 \\\n",
        "  --learning_rate=3e-5 \\\n",
        "  --num_train_epochs=0.1 \\\n",
        "  --max_seq_length=384 \\\n",
        "  --doc_stride=128 \\\n",
        "  --output_dir=gs://gurebert/gureBERT/squad/ \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name=$TPU_NAME \\\n",
        "  --version_2_with_negative=True \\\n",
        "  --init_checkpoint=gs://gurebert/gureBERT/squad/ \\\n",
        "    "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://gurebert/gureBERT/spModels/en-eu.model...\n",
            "Copying gs://gurebert/gureBERT/spModels/en-eu.vocab...\n",
            "Copying gs://gurebert/gureBERT/spModels/eu.model...\n",
            "Copying gs://gurebert/gureBERT/spModels/eu.vocab...\n",
            "- [4 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 4 objects/2.5 MiB.                                      \n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0705 20:55:38.556497 139929338054528 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0705 20:55:38.558534 139929338054528 deprecation_wrapper.py:119] From src/run_squad.py:1293: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0705 20:55:38.559260 139929338054528 deprecation_wrapper.py:119] From src/run_squad.py:1137: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0705 20:55:38.559424 139929338054528 deprecation_wrapper.py:119] From src/run_squad.py:1137: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0705 20:55:38.559556 139929338054528 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0705 20:55:38.560704 139929338054528 deprecation_wrapper.py:119] From src/run_squad.py:1143: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "Loaded a trained SentencePiece model.\n",
            "W0705 20:55:41.070648 139929338054528 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0705 20:55:42.075873 139929338054528 deprecation_wrapper.py:119] From src/run_squad.py:239: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/run_squad.py\", line 1293, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"src/run_squad.py\", line 1169, in main\n",
            "    input_file=FLAGS.train_file, is_training=True)\n",
            "  File \"src/run_squad.py\", line 296, in read_squad_examples\n",
            "    tokenization.whitespace_tokenize(orig_answer_text))\n",
            "AttributeError: module 'tokenization_sentencepiece' has no attribute 'whitespace_tokenize'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lablZa6fsh51",
        "colab_type": "text"
      },
      "source": [
        "## wordpiece erabiliz\n",
        "\n",
        "- https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379\n",
        "- https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e5XE43Q7D9i",
        "colab_type": "text"
      },
      "source": [
        "### EU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2XwxN-Z4a3m",
        "colab_type": "code",
        "outputId": "89b71320-2768-443d-fd29-ab774a352331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format('./spModels/eu'))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 17217\n",
            "Sample tokens: ['ong', '▁tourra', '911', '▁zelako', 'oinu', '▁160', '▁andrea', 'atzeko', '▁god', 'atxi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-YOATBGsg02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hau errepasatu. adibidez @ ez dut ondo egiten... [UNK] agertzen da...\n",
        "# adibidez, \n",
        "\n",
        "#import string\n",
        "#def parse_sentencepiece_token(token):\n",
        "#    if token.startswith(\"▁\"):\n",
        "#        return token[1:]\n",
        "#    else:\n",
        "#        if token in string.punctuation:\n",
        "#            return token\n",
        "#        else:\n",
        "#            return \"##\" + token\n",
        "          \n",
        "\n",
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAx98r7zGZMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "VOC_FNAME2 = \"vocab2.txt\"\n",
        "with open(VOC_FNAME2, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC4t9HmGGWa6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "50215ead-f83d-45b5-b44e-da7a7f51b645"
      },
      "source": [
        "from bert import tokenization\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME2)\n",
        "text1 = \"Nere kotxea aitonaren etxe alboan dago!, eta bere kolorea gorria da. ni@ni.eus erabili hitzhauezdaexixtitzen !~gg _*] \"\n",
        "print(bert_tokenizer.tokenize(text1))\n",
        "print(tokenizer.tokenize(text1))\n",
        "\n",
        "# [UNK] ak agertzen dira adibidez \\koma, @, ..-rekin bert wordpiece erabiltzean\n",
        "# BERT tokenizer-ak _run_split_on_puntuaction egiten du: kaixo, -> [kaixo] [,] ; "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ne', '##re', 'ko', '##txea', 'ait', '##onar', '##en', 'etxe', 'alb', '##oan', 'dago', '!', '[UNK]', 'eta', 'bere', 'kolore', '##a', 'gorria', 'da', '[UNK]', 'ni', '[UNK]', 'ni', '[UNK]', 'eus', 'erabili', 'hitz', '##haue', '##z', '##da', '##e', '##xix', '##titz', '##en', '!', '[UNK]', 'g', '##g', '[UNK]', '*', '[UNK]']\n",
            "['▁ne', 're', '▁', 'ko', 'txea', '▁ai', 'ton', 'aren', '▁etxe', '▁alb', 'o', 'an', '▁dago', '!', ',', '▁eta', '▁bere', '▁kolore', 'a', '▁gorria', '▁da', '.', '▁ni', '@', 'ni', '.', 'eus', '▁erabili', '▁hitz', 'hau', 'ez', 'da', 'e', 'xix', 'ti', 'tzen', '▁!', '~', 'gg', '▁', '_', '*', ']']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYEtWr0D39Rf",
        "colab_type": "code",
        "outputId": "228b9325-d010-469c-ea62-bf3c1e1d8c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import string\n",
        "\n",
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "# puntuazio sinboloak gehitu \n",
        "ctrl_symbols_end = list(string.punctuation)\n",
        "#ctrl_symbols = [\"[UNK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab + ctrl_symbols_end\n",
        "\n",
        "#bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(32000 - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiGAWsMw4CLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\"\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLLaP8_04uY4",
        "colab_type": "code",
        "outputId": "bcfe1099-00f9-40c8-fd31-4a8ff418bca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from bert import tokenization\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "#text1 = \"Nere kotxea aitonaren etxe alboan dago!, eta bere kolorea gorria da. ni@ni.eus erabili hitzhauezdaexixtitzen !~gg _*] \"\n",
        "#text1 = \"44.579.000 km²ko eremua eta 4.140.336.501 biztanle ditu\"\n",
        "#text1 = \"biologia izena grezierazko bi osagai erabiliz eratua da: βίος, bios, «bizitza»; eta -λογία, -logia, «azterketa».\"\n",
        "text1 = \"手洗い\"\n",
        "print(\"num:{}, tokenak:{}\".format(len(bert_tokenizer.tokenize(text1)),bert_tokenizer.tokenize(text1)))\n",
        "print(\"num:{}, tokenak:{}\".format(len(tokenizer.tokenize(text1)),tokenizer.tokenize(text1)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num:3, tokenak:['[UNK]', '[UNK]', '[UNK]']\n",
            "num:2, tokenak:['▁', '手洗い']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qHdqxFz8rrN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7c3b01a-1365-42cd-d791-d301b34903c2"
      },
      "source": [
        "# \n",
        "def bert_tokenize_corpus(filepath):\n",
        "  tokens = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      token_line = bert_tokenizer.tokenize(line)\n",
        "      tokens += token_line\n",
        "      if '[UNK]' in token_line:\n",
        "        print(\"testua: {}\".format(line))\n",
        "        print(\"tokenak: {}\".format(token_line))\n",
        "        print(\"---------------------------------------------------------\")\n",
        "        \n",
        "  return tokens\n",
        "\n",
        "def sentencepiece_tokenize_corpus(filepath):\n",
        "  tokens = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      tokens += tokenizer.tokenize(line)      \n",
        "  return tokens\n",
        "\n",
        "\n",
        "bert_tokens = bert_tokenize_corpus(\"./corpus/eu/2014wiki.eu.sent_splited\")\n",
        "sentencepiece_tokens = sentencepiece_tokenize_corpus(\"./corpus/eu/2014wiki.eu.sent_splited\")\n",
        "\n",
        "print(\"wordpiece tokens: {}\".format(len(bert_tokens)))\n",
        "print(\"sentencepiece tokens: {}\".format(len(sentencepiece_tokens)))\n",
        "\n",
        "print(\"[UNK] kop: {}\".format(bert_tokens.count('[UNK]')))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testua: astronomia (grekerazko ἄστρον, astron; \"argizagi, zeruko objektu\" eta νόμος, nomos, \"arau, lege\" hitzetatik: \"argizagien legea\") zeruko objektu edo argizagiak (hala nola izarrak, planetak, kometak, galaxiak) eta lurraren atmosferatik kanpo gertatzen diren fenomenoak (hondoko erradiazio kosmikoa, esaterako) aztertzen dituen zientzia da.\n",
            "\n",
            "tokenak: ['astronomia', '(', 'gre', '##ker', '##azko', '[UNK]', ',', 'astron', ';', '\"', 'argizagi', ',', 'zeru', '##ko', 'objektu', '\"', 'eta', '[UNK]', ',', 'no', '##mos', ',', '\"', 'arau', ',', 'lege', '\"', 'hitze', '##tatik', ':', '\"', 'argizagie', '##n', 'legea', '\"', ')', 'zeru', '##ko', 'objektu', 'edo', 'argizagi', '##ak', '(', 'hala', 'nola', 'izarrak', ',', 'planetak', ',', 'kom', '##etak', ',', 'galaxi', '##ak', ')', 'eta', 'lurraren', 'atmosfera', '##tik', 'kanpo', 'gertatzen', 'diren', 'fenomenoak', '(', 'hond', '##oko', 'erradi', '##azio', 'kosm', '##ikoa', ',', 'esaterako', ')', 'aztertzen', 'dituen', 'zientzia', 'da', '.']\n",
            "---------------------------------------------------------\n",
            "testua: aristoteles —grezieraz ἀριστοτέλης— (estagira, mazedonia, k. a. 384 - kaltzis, eubea, k. a. 322) grezia klasikoko filosofo eta zientzialari bat izan zen.\n",
            "\n",
            "tokenak: ['aristoteles', '—', 'grezieraz', '[UNK]', '—', '(', 'estagira', ',', 'mazedonia', ',', 'k', '.', 'a', '.', '384', '-', 'kal', '##tzi', '##s', ',', 'eubea', ',', 'k', '.', 'a', '.', '322', ')', 'grezia', 'klasiko', '##ko', 'filosofo', 'eta', 'zientzialari', 'bat', 'izan', 'zen', '.']\n",
            "---------------------------------------------------------\n",
            "testua: 44.579.000 km²ko eremua eta 4.140.336.501 biztanle ditu.\n",
            "\n",
            "tokenak: ['44', '.', '57', '##9', '.', '0', '##00', '[UNK]', 'eremua', 'eta', '4', '.', '140', '.', '336', '.', '50', '##1', 'biztanle', 'ditu', '.']\n",
            "---------------------------------------------------------\n",
            "testua: asia 44 milioi km² area baino gehiago ditu, 44.936.000 km² hain zuzen.\n",
            "\n",
            "tokenak: ['asia', '44', 'milioi', '[UNK]', 'area', 'baino', 'gehiago', 'ditu', ',', '44', '.', '936', '.', '0', '##00', '[UNK]', 'hain', 'zuzen', '.']\n",
            "---------------------------------------------------------\n",
            "testua: lurralde hotzena, bestalde, siberiako ipar-ekialdeko verkhoiansk da (-66°c).\n",
            "\n",
            "tokenak: ['lurralde', 'hotz', '##ena', ',', 'bestalde', ',', 'siberia', '##ko', 'ipar', '-', 'ekialde', '##ko', 'ver', '##khoi', '##ans', '##k', 'da', '(', '-', '[UNK]', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: paleo-siberiar jatorrikoak dira siberian bizi diren herriak (tungusak, samoiedoak, ainuak,…); turko-mongolak, erdialdeko asia, mongoliako eta txina iparraldeko mongolak eta turkoak; indonesiarrak hego-ekialdeko asia eta, mongoloideekin nahasian, txina hegoaldean; indoeuroparrak, azkenik, india, afganistan eta erdialdeko asiako lurralde batzuetan egokitu ziren.\n",
            "\n",
            "tokenak: ['paleo', '-', 'siberiar', 'jatorriko', '##ak', 'dira', 'siberia', '##n', 'bizi', 'diren', 'herria', '##k', '(', 'tung', '##usa', '##k', ',', 'samo', '##ied', '##oak', ',', 'ainuak', ',', '[UNK]', ')', ';', 'turko', '-', 'mongol', '##ak', ',', 'erdialde', '##ko', 'asia', ',', 'mongolia', '##ko', 'eta', 'txina', 'iparraldeko', 'mongol', '##ak', 'eta', 'turkoak', ';', 'indonesiarrak', 'hego', '-', 'ekialde', '##ko', 'asia', 'eta', ',', 'mongoloide', '##ekin', 'nahasi', '##an', ',', 'txina', 'hegoaldean', ';', 'indoeuroparrak', ',', 'azkenik', ',', 'india', ',', 'afganistan', 'eta', 'erdialde', '##ko', 'asiak', '##o', 'lurralde', 'batzuetan', 'egoki', '##tu', 'ziren', '.']\n",
            "---------------------------------------------------------\n",
            "testua: europaren ikuspuntutik asia osoa «ekialde» izenaz ezaguna zen.\n",
            "\n",
            "tokenak: ['europar', '##en', 'ikuspuntutik', 'asia', 'osoa', '[UNK]', 'ekialde', '[UNK]', 'izenaz', 'ezaguna', 'zen', '.']\n",
            "---------------------------------------------------------\n",
            "testua: antzinako erromatarrek «afrien lurra» (afer, afri, «kartagotar») deitzen zioten kontinenteko iparraldeko zatiari (gaur egun tunisia deitzen diogunari).\n",
            "\n",
            "tokenak: ['antzinako', 'erromatarre', '##k', '[UNK]', 'afri', '##en', 'lurra', '[UNK]', '(', 'af', '##er', ',', 'afri', ',', '[UNK]', 'kartago', '##tar', '[UNK]', ')', 'deitze', '##n', 'zioten', 'kontinentek', '##o', 'iparraldeko', 'zatia', '##ri', '(', 'gaur', 'egun', 'tunisia', 'deitze', '##n', 'diogu', '##nari', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: afer izenaren jatorria, dirudienez, fenizierazko afar («hautsa») hitza da; edo, beharbada, berbererazko ifri (plurala: ifran), «haitzuloa», haitzuloetako biztanleak aipagai izanik.\n",
            "\n",
            "tokenak: ['af', '##er', 'izena', '##ren', 'jatorria', ',', 'dirudien', '##ez', ',', 'fenizi', '##era', '##zko', 'af', '##ar', '(', '[UNK]', 'hau', '##ts', '##a', '[UNK]', ')', 'hitza', 'da', ';', 'edo', ',', 'beharbada', ',', 'berber', '##era', '##zko', 'if', '##ri', '(', 'plural', '##a', ':', 'if', '##ran', ')', ',', '[UNK]', 'hai', '##tzu', '##lo', '##a', '[UNK]', ',', 'hai', '##tzu', '##lo', '##etako', 'biztanleak', 'aipaga', '##i', 'izanik', '.']\n",
            "---------------------------------------------------------\n",
            "testua: nahiz eta bere biztanle ez zuen bere lurra kontinentetzat hartzen, erdialdeko amerikako kulturek abya yala zioen bere lurrari eta gertuko aztekek cem anahuac (nahuatlezko izena, «ur handien artean dagoen lurra» esan nahi duena).\n",
            "\n",
            "tokenak: ['nahi', '##z', 'eta', 'bere', 'biztanle', 'ez', 'zuen', 'bere', 'lurra', 'kontinente', '##tzat', 'hartzen', ',', 'erdialde', '##ko', 'amerikak', '##o', 'kulturek', 'ab', '##ya', 'yal', '##a', 'zioen', 'bere', 'lurra', '##ri', 'eta', 'gertu', '##ko', 'aztek', '##ek', 'cem', 'anahuac', '(', 'nahuatl', '##ezko', 'izena', ',', '[UNK]', 'ur', 'handien', 'artean', 'dagoen', 'lurra', '[UNK]', 'esan', 'nahi', 'duena', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: bere 42.262.142 km² arearekin, amerika munduko bigarren lurralde multzorik handiena da (asia da lehendabizikoa),lurreko %8,3a eta munduko biztanleriaren %14a du.\n",
            "\n",
            "tokenak: ['bere', '42', '.', '26', '##2', '.', '14', '##2', '[UNK]', 'area', '##rekin', ',', 'amerika', 'mundu', '##ko', 'bigarren', 'lurralde', 'multzo', '##rik', 'handiena', 'da', '(', 'asia', 'da', 'lehendabiziko', '##a', ')', ',', 'lurre', '##ko', '%', '8', ',', '3', '##a', 'eta', 'mundu', '##ko', 'biztanleria', '##ren', '%', '14', '##a', 'du', '.']\n",
            "---------------------------------------------------------\n",
            "testua: iparraldeko ozeano artikoko columbia lurmuturretik (58ºi) hegoaldeko drake pasabideko diego ramirez uharteetara (56ºh) doa eta ekialdeko brasilgo branco lurmuturretik (34°47'm) mendebaldeko aleutiar uharteetako attu uhartera (173°11'e).\n",
            "\n",
            "tokenak: ['iparraldeko', 'ozeano', 'artiko', '##ko', 'columbia', 'lurmuturr', '##etik', '(', '[UNK]', ')', 'hegoaldeko', 'drake', 'pasabide', '##ko', 'diego', 'rami', '##rez', 'uharteeta', '##ra', '(', '[UNK]', ')', 'doa', 'eta', 'ekialde', '##ko', 'brasil', '##go', 'branc', '##o', 'lurmuturr', '##etik', '(', '[UNK]', \"'\", 'm', ')', 'mendebaldeko', 'ale', '##utiar', 'uharteeta', '##ko', 'att', '##u', 'uharte', '##ra', '(', '[UNK]', \"'\", 'e', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: biologia izena grezierazko bi osagai erabiliz eratua da: βίος, bios, «bizitza»; eta -λογία, -logia, «azterketa».\n",
            "\n",
            "tokenak: ['biologia', 'izena', 'grezieraz', '##ko', 'bi', 'osagai', 'erabiliz', 'eratu', '##a', 'da', ':', '[UNK]', ',', 'bios', ',', '[UNK]', 'bizitza', '[UNK]', ';', 'eta', '-', '[UNK]', ',', '-', 'log', '##ia', ',', '[UNK]', 'azterketa', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: hau da, etimologiaz, «bizitzaren azterketa» esan nahi du.\n",
            "\n",
            "tokenak: ['hau', 'da', ',', 'etimologia', '##z', ',', '[UNK]', 'bizitza', '##ren', 'azterketa', '[UNK]', 'esan', 'nahi', 'du', '.']\n",
            "---------------------------------------------------------\n",
            "testua: * berezko antolaketa biokimikoa: antzeko molekula organikoz daude osatuta izaki bizidunak (proteinak, azido nukleikoak, gluzidoak eta lipidoak, etab.), eta antzeko bide metabolikoak dituzte (gluzido, lipido eta protidoen katabolismoa, azido nukleikoen funtzionamendua, proteinen sintesia…)\n",
            "\n",
            "tokenak: ['*', 'berez', '##ko', 'antola', '##keta', 'biokimikoa', ':', 'antzeko', 'molekula', 'organiko', '##z', 'daude', 'osatuta', 'izaki', 'bizidunak', '(', 'proteina', '##k', ',', 'azido', 'nukleiko', '##ak', ',', 'glu', '##zi', '##do', '##ak', 'eta', 'lipido', '##ak', ',', 'etab', '.', ')', ',', 'eta', 'antzeko', 'bide', 'metabolikoa', '##k', 'dituzte', '(', 'glu', '##zi', '##do', ',', 'lipido', 'eta', 'prot', '##ido', '##en', 'kata', '##boli', '##sm', '##oa', ',', 'azido', 'nukleikoen', 'funtzionamendua', ',', 'protein', '##en', 'sintesia', '[UNK]', ')']\n",
            "---------------------------------------------------------\n",
            "testua: botanika (grezieraz βοτάνη edo \"belarra\") landareak aztertzen dituen biologiaren atala da, hainbat ataletan berezitua.\n",
            "\n",
            "tokenak: ['botanika', '(', 'grezieraz', '[UNK]', 'edo', '\"', 'belar', '##ra', '\"', ')', 'landare', '##ak', 'aztertzen', 'dituen', 'biologia', '##ren', 'atala', 'da', ',', 'hainbat', 'atal', '##etan', 'berezi', '##tua', '.']\n",
            "---------------------------------------------------------\n",
            "testua: zabalerari begiratuz gero, europa munduko bigarren kontinenterik txikiena da, 11.000.000 km² izanik ozeania baino apur bat handiagoa baita.\n",
            "\n",
            "tokenak: ['zabale', '##rari', 'begiratu', '##z', 'gero', ',', 'europa', 'mundu', '##ko', 'bigarren', 'kontinente', '##rik', 'txikiena', 'da', ',', '11', '.', '0', '##00', '.', '0', '##00', '[UNK]', 'izanik', 'ozeania', 'baino', 'apur', 'bat', 'handiagoa', 'baita', '.']\n",
            "---------------------------------------------------------\n",
            "testua: europak, munduko bigarren kontinenterik txikiena, 10.530.751 km² area du, hau da %7a.\n",
            "\n",
            "tokenak: ['europak', ',', 'mundu', '##ko', 'bigarren', 'kontinente', '##rik', 'txikiena', ',', '10', '.', '53', '##0', '.', '75', '##1', '[UNK]', 'area', 'du', ',', 'hau', 'da', '%', '7', '##a', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # britainia handia, 218.000 km².\n",
            "\n",
            "tokenak: ['#', 'britainia', 'handia', ',', '218', '.', '0', '##00', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # islandia, 103.000 km².\n",
            "\n",
            "tokenak: ['#', 'islandia', ',', '103', '.', '0', '##00', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # irlanda, 83.000 km².\n",
            "\n",
            "tokenak: ['#', 'irlanda', ',', '83', '.', '0', '##00', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # zembla berriko ipar uhartea, 49.000 km².\n",
            "\n",
            "tokenak: ['#', 'ze', '##mb', '##la', 'berri', '##ko', 'ipar', 'uhartea', ',', '49', '.', '0', '##00', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # zembla berriko hego uhartea 33.000 km².\n",
            "\n",
            "tokenak: ['#', 'ze', '##mb', '##la', 'berri', '##ko', 'hego', 'uhartea', '33', '.', '0', '##00', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # sizilia, 25.000 km².\n",
            "\n",
            "tokenak: ['#', 'sizilia', ',', '25', '.', '0', '##00', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # sardinia, 24.000 km².\n",
            "\n",
            "tokenak: ['#', 'sardinia', ',', '24', '.', '0', '##00', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # zipre, 9.200 km².\n",
            "\n",
            "tokenak: ['#', 'zipre', ',', '9', '.', '200', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # korsika, 8.700 km².\n",
            "\n",
            "tokenak: ['#', 'korsika', ',', '8', '.', '700', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # kreta, 8.300 km².\n",
            "\n",
            "tokenak: ['#', 'kret', '##a', ',', '8', '.', '300', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # seelandia, 7.500 km².\n",
            "\n",
            "tokenak: ['#', 'see', '##landia', ',', '7', '.', '500', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # eubea, 3.900 km²..\n",
            "\n",
            "tokenak: ['#', 'eubea', ',', '3', '.', '900', '[UNK]', '.', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # mallorca, 3.600 km².\n",
            "\n",
            "tokenak: ['#', 'mallorca', ',', '3', '.', '600', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # malta, 316 km².\n",
            "\n",
            "tokenak: ['#', 'malta', ',', '316', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # fionia, 3.400 km².\n",
            "\n",
            "tokenak: ['#', 'fi', '##onia', ',', '3', '.', '400', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # faroe uharteak, 1.390 km².\n",
            "\n",
            "tokenak: ['#', 'faroe', 'uharteak', ',', '1', '.', '39', '##0', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # eivissa, 560 km².\n",
            "\n",
            "tokenak: ['#', 'ei', '##vis', '##sa', ',', '560', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: # menorca, 690 km².\n",
            "\n",
            "tokenak: ['#', 'menor', '##ca', ',', '690', '[UNK]', '.']\n",
            "---------------------------------------------------------\n",
            "testua: periodo bakoitza atomoaren kanpoaldeko elektroi geruzan elektroi bakarra duen gai bakunetik abiatzen da (metal alkalinoak dira) eta azken geruza horretan 8 elektroi dituzten gai bakunean bukatzen (gas nobleak: argoia, kriptoia, xenoia…).\n",
            "\n",
            "tokenak: ['period', '##o', 'bakoitza', 'atomoa', '##ren', 'kanpoalde', '##ko', 'elektroi', 'geruza', '##n', 'elektroi', 'bakarra', 'duen', 'gai', 'bakune', '##tik', 'abia', '##tzen', 'da', '(', 'metal', 'alkalino', '##ak', 'dira', ')', 'eta', 'azken', 'geruza', 'horretan', '8', 'elektroi', 'dituzten', 'gai', 'bakune', '##an', 'bukatze', '##n', '(', 'gas', 'nobleak', ':', 'argo', '##ia', ',', 'kripto', '##ia', ',', 'xeno', '##ia', '[UNK]', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: sailaren zenbakiak (i, ii, iii, iv…) azken geruzako elektroi horien kopurua adierazten du.\n",
            "\n",
            "tokenak: ['saila', '##ren', 'zenbakiak', '(', 'i', ',', 'ii', ',', 'iii', ',', 'iv', '[UNK]', ')', 'azken', 'geruza', '##ko', 'elektroi', 'horien', 'kopurua', 'adierazten', 'du', '.']\n",
            "---------------------------------------------------------\n",
            "testua: l. l. zamenhof mediku okulista poloniarrak 1887an “lingvo internacia” (nazioarteko hizkuntza) izenarekin argitaratu zuen berak hamarkada batean zehar garatutako hizkuntza.\n",
            "\n",
            "tokenak: ['l', '.', 'l', '.', 'zamenhof', 'mediku', 'okul', '##ista', 'poloniarra', '##k', '1887', '##an', '[UNK]', 'ling', '##vo', 'interna', '##cia', '[UNK]', '(', 'nazioarteko', 'hizkuntza', ')', 'izenarekin', 'argitaratu', 'zuen', 'berak', 'hamarkada', 'batean', 'zehar', 'garatu', '##tako', 'hizkuntza', '.']\n",
            "---------------------------------------------------------\n",
            "testua: * 1859: abenduaren 15ean ludwik lejzer zamenhof jaio zen białystok-en (orduko errusiako inperioan).\n",
            "\n",
            "tokenak: ['*', '1859', ':', 'abendua', '##ren', '15', '##ean', 'ludwi', '##k', 'lej', '##zer', 'zamenhof', 'jaio', 'zen', '[UNK]', '-', 'en', '(', 'ordu', '##ko', 'errusiako', 'inperioa', '##n', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: historia hitzaren jatorria grezierako οιδa (\"badakit\") hitzean oinarritzen da.\n",
            "\n",
            "tokenak: ['historia', 'hitza', '##ren', 'jatorria', 'greziera', '##ko', '[UNK]', '(', '\"', 'badaki', '##t', '\"', ')', 'hitze', '##an', 'oinarritz', '##en', 'da', '.']\n",
            "---------------------------------------------------------\n",
            "testua: hortik ιστορια (\"ikerketa edo informazioa\") eratorri zuen, eta hortik latinezko historia hitza.\n",
            "\n",
            "tokenak: ['hort', '##ik', '[UNK]', '(', '\"', 'ikerketa', 'edo', 'informazioa', '\"', ')', 'eratorri', 'zuen', ',', 'eta', 'hort', '##ik', 'latinez', '##ko', 'historia', 'hitza', '.']\n",
            "---------------------------------------------------------\n",
            "testua: nolanahi ere, guztiz enpirikoak izan ziren aurkikuntza hauek eta ezin esan daiteke zientzia zenik, ezta urrik eman ere.greziarrak izan ziren zientzia honetan egiptoarren ondorengoak eta «chemia» izena eman zioten materia eraldatzeko arteari.\n",
            "\n",
            "tokenak: ['nola', '##nahi', 'ere', ',', 'guztiz', 'enp', '##iri', '##koak', 'izan', 'ziren', 'aurkikuntza', 'haue', '##k', 'eta', 'ezin', 'esan', 'daiteke', 'zientzia', 'zenik', ',', 'ezta', 'urri', '##k', 'eman', 'ere', '.', 'greziarra', '##k', 'izan', 'ziren', 'zientzia', 'honetan', 'egiptoarre', '##n', 'ondorengoak', 'eta', '[UNK]', 'chemi', '##a', '[UNK]', 'izena', 'eman', 'zioten', 'materia', 'eraldatze', '##ko', 'artea', '##ri', '.']\n",
            "---------------------------------------------------------\n",
            "testua: izen hau «chem»-etik omen dator, horrelaxe esaten ziotelako egiptoarrek bere herriari.\n",
            "\n",
            "tokenak: ['izen', 'hau', '[UNK]', 'che', '##m', '[UNK]', '-', 'et', '##ik', 'omen', 'dator', ',', 'horrela', '##xe', 'esaten', 'ziotelako', 'egiptoarre', '##k', 'bere', 'herria', '##ri', '.']\n",
            "---------------------------------------------------------\n",
            "testua: ekialdean «chemia» izenez ezagutzen zen zientzia hori arabiarrek ekarri zuten europara, «kimiya» izenez.\n",
            "\n",
            "tokenak: ['ekialdean', '[UNK]', 'chemi', '##a', '[UNK]', 'izenez', 'ezagutzen', 'zen', 'zientzia', 'hori', 'arabiar', '##rek', 'ekarri', 'zuten', 'europara', ',', '[UNK]', 'kim', '##i', '##ya', '[UNK]', 'izenez', '.']\n",
            "---------------------------------------------------------\n",
            "testua: gero, «al» artikulua jarrita, «al kimiya» bihurtu zen –»alkimia» alegia–, eta «kimika» azkenean.\n",
            "\n",
            "tokenak: ['gero', ',', '[UNK]', 'al', '[UNK]', 'artikulua', 'jarrita', ',', '[UNK]', 'al', 'kim', '##i', '##ya', '[UNK]', 'bihurtu', 'zen', '–', '[UNK]', 'alkimia', '[UNK]', 'alegia', '–', ',', 'eta', '[UNK]', 'kimika', '[UNK]', 'azkenean', '.']\n",
            "---------------------------------------------------------\n",
            "testua: ez ziren, ordea, beren helburua lortzera iritsi, baina, hala ere, artean ezezagunak ziren produktu askoren berri eman zuten, beste produktu asko lortzeko bideak zabaldu zituzten, aparatu asko diseinatu zuten… hori guztia oso lagungarria izan zen handik harako ikerkuntzan.\n",
            "\n",
            "tokenak: ['ez', 'ziren', ',', 'ordea', ',', 'bere', '##n', 'helburua', 'lortze', '##ra', 'iritsi', ',', 'baina', ',', 'hala', 'ere', ',', 'artean', 'ezezaguna', '##k', 'ziren', 'produktu', 'askoren', 'berri', 'eman', 'zuten', ',', 'beste', 'produktu', 'asko', 'lortzeko', 'bidea', '##k', 'zabaldu', 'zituzten', ',', 'apa', '##ratu', 'asko', 'diseinatu', 'zuten', '[UNK]', 'hori', 'guztia', 'oso', 'lagungarri', '##a', 'izan', 'zen', 'handik', 'hara', '##ko', 'ikerkuntza', '##n', '.']\n",
            "---------------------------------------------------------\n",
            "testua: kimikak ahaleginak egin zituen gertaerak eta teoria lotzen, kimikariak aurkikuntzak argitaratzen hasi ziren, eta lehenengo zientzia akademiak sortu ziren: napolin (1556), londresen (1606), paris (1666), bartzelona (1764)…garai hartakoa zen robert boyle irlandar fisikaria eta kimikaria (1627-1691).\n",
            "\n",
            "tokenak: ['kimikak', 'ahalegina', '##k', 'egin', 'zituen', 'gertaerak', 'eta', 'teoria', 'lotzen', ',', 'kimikaria', '##k', 'aurkikuntzak', 'argitaratze', '##n', 'hasi', 'ziren', ',', 'eta', 'lehenengo', 'zientzia', 'akademiak', 'sortu', 'ziren', ':', 'napoli', '##n', '(', '1556', ')', ',', 'londresen', '(', '160', '##6', ')', ',', 'paris', '(', '166', '##6', ')', ',', 'bartzelona', '(', '1764', ')', '[UNK]', 'garai', 'hartako', '##a', 'zen', 'robert', 'boyle', 'irlandar', 'fisikaria', 'eta', 'kimikaria', '(', '162', '##7', '-', '16', '##91', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: 1789.ean argitaratuko bere «tratado elemental de química» delakoan gaur egungo kimikaren oinarriak jarri zituen.\n",
            "\n",
            "tokenak: ['1789', '.', 'ea', '##n', 'argitaratuko', 'bere', '[UNK]', 'trata', '##do', 'element', '##al', 'de', 'qui', '##mica', '[UNK]', 'delakoa', '##n', 'gaur', 'egung', '##o', 'kimikare', '##n', 'oinarriak', 'jarri', 'zituen', '.']\n",
            "---------------------------------------------------------\n",
            "testua: literatura hitza latineko littera (“letra”) hitzetik sortua da.\n",
            "\n",
            "tokenak: ['literatura', 'hitza', 'latin', '##eko', 'litt', '##era', '(', '[UNK]', 'letra', '[UNK]', ')', 'hitze', '##tik', 'sortua', 'da', '.']\n",
            "---------------------------------------------------------\n",
            "testua: hitzaren edukiak historian zehar aldaketa handiak izan ditu, eta xviii. mendearen ia bukaeraraino “idazteko eta irakurtzeko antzea” esan nahi zuen; handik aurrera, “idazlearen jarduna eta emaitza edo idazlana” europako herri gehienetan.\n",
            "\n",
            "tokenak: ['hitza', '##ren', 'edukiak', 'historian', 'zehar', 'aldaketa', 'handiak', 'izan', 'ditu', ',', 'eta', 'xviii', '.', 'mendea', '##ren', 'ia', 'bukaera', '##raino', '[UNK]', 'idazteko', 'eta', 'irakurtzeko', 'antze', '##a', '[UNK]', 'esan', 'nahi', 'zuen', ';', 'handik', 'aurrera', ',', '[UNK]', 'idazlea', '##ren', 'jarduna', 'eta', 'emaitza', 'edo', 'idazlana', '[UNK]', 'europako', 'herri', 'gehienetan', '.']\n",
            "---------------------------------------------------------\n",
            "testua: erromantizismoak (xix. mendearen lehenengo erdialdean) ordu arte multzo berekoak izan ziren testu idatziak bereizi zituen: alde batetik filosofia, historia, ikerketa…, eta bestetik, alegiazko generoak eta literatura arte gisa.\n",
            "\n",
            "tokenak: ['erromantizismoa', '##k', '(', 'xix', '.', 'mendea', '##ren', 'lehenengo', 'erdialdean', ')', 'ordu', 'arte', 'multzo', 'berekoak', 'izan', 'ziren', 'testu', 'idatzia', '##k', 'bereizi', 'zituen', ':', 'alde', 'batetik', 'filosofia', ',', 'historia', ',', 'ikerketa', '[UNK]', ',', 'eta', 'bestetik', ',', 'alegia', '##zko', 'generoak', 'eta', 'literatura', 'arte', 'gisa', '.']\n",
            "---------------------------------------------------------\n",
            "testua: hizkuntza arruntean bezala, literaturan ere bi mota bereizten dira: ahozko literatura (erromantzeak, esaera zaharrak… poesian, ipuin eta herri kontakizunak prosan) eta idatzizko literatura (poesia eta prosa).\n",
            "\n",
            "tokenak: ['hizkuntza', 'arrunte', '##an', 'bezala', ',', 'literaturan', 'ere', 'bi', 'mota', 'bereizten', 'dira', ':', 'ahozko', 'literatura', '(', 'erromantzeak', ',', 'esaera', 'zaharrak', '[UNK]', 'poesia', '##n', ',', 'ipuin', 'eta', 'herri', 'kontakizun', '##ak', 'prosa', '##n', ')', 'eta', 'idatziz', '##ko', 'literatura', '(', 'poesia', 'eta', 'prosa', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: bestalde, literatura lanetan ezin da “mezua” aldatu eta beti idazleak idatzitako modu berean eman behar da aditzera, literaturan edukia eta forma banaezinak baitira (literatura hizkuntzako zeinuak denotatu edo esanahi jakin bat adierazteaz gainera, literatura hizkuntzak konnotatu edo bigarren mailako esanahi berriak sorrarazten baititu).\n",
            "\n",
            "tokenak: ['bestalde', ',', 'literatura', 'lanetan', 'ezin', 'da', '[UNK]', 'mezua', '[UNK]', 'aldatu', 'eta', 'beti', 'idazleak', 'idatzitako', 'modu', 'berean', 'eman', 'behar', 'da', 'aditzera', ',', 'literaturan', 'edukia', 'eta', 'forma', 'bana', '##ezin', '##ak', 'baitira', '(', 'literatura', 'hizkuntzak', '##o', 'zeinu', '##ak', 'denot', '##atu', 'edo', 'esanahi', 'jakin', 'bat', 'adieraztea', '##z', 'gainera', ',', 'literatura', 'hizkuntzak', 'kon', '##not', '##atu', 'edo', 'bigarren', 'maila', '##ko', 'esanahi', 'berriak', 'sorraraz', '##ten', 'baititu', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: literatura hizkuntzaren beste ezaugarria erabiltzen dituen erretorika baliabideak eta tropoak dira (aliterazioa, errima, errepikak, epitetoak; metafora, metonimia, hiperbatona…).\n",
            "\n",
            "tokenak: ['literatura', 'hizkuntzaren', 'beste', 'ezaugarria', 'erabiltzen', 'dituen', 'erre', '##tori', '##ka', 'baliabideak', 'eta', 'tropo', '##ak', 'dira', '(', 'ali', '##tera', '##zioa', ',', 'err', '##ima', ',', 'errepika', '##k', ',', 'epit', '##etoak', ';', 'meta', '##for', '##a', ',', 'met', '##oni', '##mia', ',', 'hiper', '##bat', '##ona', '[UNK]', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: hiru dira nagusiak: epika (epopeia, poema epikoa, gestako kantua, erromantzea; ipuina eta eleberria), lirika (oda, elegia, egloga, satira, epigrama, epitalamioa) eta dramatika (tragedia, komedia, drama, opera…).\n",
            "\n",
            "tokenak: ['hiru', 'dira', 'nagusiak', ':', 'epika', '(', 'ep', '##ope', '##ia', ',', 'poem', '##a', 'epi', '##koa', ',', 'gesta', '##ko', 'kantua', ',', 'erromantzea', ';', 'ipuina', 'eta', 'eleberria', ')', ',', 'li', '##rika', '(', 'o', '##da', ',', 'elegia', ',', 'eg', '##loga', ',', 'satira', ',', 'epigram', '##a', ',', 'epit', '##alami', '##oa', ')', 'eta', 'dramati', '##ka', '(', 'tragedi', '##a', ',', 'kom', '##ed', '##ia', ',', 'drama', ',', 'opera', '[UNK]', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: hiru era horiez gain, didaktikako lanak (epistola, saioa…), historiakoak (biografiak, kronikak…) eta oratorioakoak (sermoiak, hitzaldiak) literatura generotzat hartu izan dira.\n",
            "\n",
            "tokenak: ['hiru', 'era', 'hori', '##ez', 'gain', ',', 'did', '##ak', '##tika', '##ko', 'lanak', '(', 'epist', '##ola', ',', 'saioa', '[UNK]', ')', ',', 'historiako', '##ak', '(', 'biografia', '##k', ',', 'kronika', '##k', '[UNK]', ')', 'eta', 'oratorioa', '##koak', '(', 'ser', '##mo', '##iak', ',', 'hitzaldiak', ')', 'literatura', 'genero', '##tzat', 'hartu', 'izan', 'dira', '.']\n",
            "---------------------------------------------------------\n",
            "testua: jarraian etorri zen, vii. mendean, polifonia (grekotik dator hitza, eta «ahots asko» esan nahi du).\n",
            "\n",
            "tokenak: ['jarraian', 'etorri', 'zen', ',', 'vii', '.', 'mendean', ',', 'polifoni', '##a', '(', 'grekot', '##ik', 'dator', 'hitza', ',', 'eta', '[UNK]', 'ah', '##ots', 'asko', '[UNK]', 'esan', 'nahi', 'du', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: matematika (grekotik μάθημα máthema: zientzia, ezagutza, ikaskuntza - μαθηματικóς, mathematikós: zientzia zalea) izate abstraktuen (zenbakiak, irudi geometrikoak eta abar) propietateak aztertzen dituen zientzia da.\n",
            "\n",
            "tokenak: ['matematika', '(', 'grekot', '##ik', '[UNK]', 'math', '##ema', ':', 'zientzia', ',', 'ezagutza', ',', 'ikas', '##kuntza', '-', '[UNK]', ',', 'mathemati', '##ko', '##s', ':', 'zientzia', 'zale', '##a', ')', 'izate', 'abstraktu', '##en', '(', 'zenbakiak', ',', 'irudi', 'geometrikoak', 'eta', 'abar', ')', 'propietateak', 'aztertzen', 'dituen', 'zientzia', 'da', '.']\n",
            "---------------------------------------------------------\n",
            "testua: pakistan 804 000 km² luze-zabal da eta honako mugak ditu: iran (900km), hego-mendebaldean; afganistan (2.400 km), ipar eta mendebaldean; txina (520 km) ipar-ekialdean; eta india (2900 km), ekialdean.\n",
            "\n",
            "tokenak: ['pakistan', '80', '##4', '0', '##00', '[UNK]', 'luze', '-', 'zabal', 'da', 'eta', 'honako', 'mugak', 'ditu', ':', 'iran', '(', '900', '##km', ')', ',', 'hego', '-', 'mendebaldean', ';', 'afganistan', '(', '2', '.', '400', 'km', ')', ',', 'ipar', 'eta', 'mendebaldean', ';', 'txina', '(', '52', '##0', 'km', ')', 'ipar', '-', 'ekialdean', ';', 'eta', 'india', '(', '290', '##0', 'km', ')', ',', 'ekialdean', '.']\n",
            "---------------------------------------------------------\n",
            "testua: neguan batez beste 14 °c izaten dira indoren ibarrean, 20 °c kostaldean eta -20 °c iparraldeko mendi garaietan.\n",
            "\n",
            "tokenak: ['negu', '##an', 'batez', 'beste', '14', '[UNK]', 'izaten', 'dira', 'indo', '##ren', 'ibarrean', ',', '20', '[UNK]', 'kostaldean', 'eta', '-', '20', '[UNK]', 'iparraldeko', 'mendi', 'garaieta', '##n', '.']\n",
            "---------------------------------------------------------\n",
            "testua: udan, berriz, 29 °c kostaldean, 35 °c hego-ekialdeko basamortuan eta 0 °c iparraldeko mendietan.\n",
            "\n",
            "tokenak: ['udan', ',', 'berriz', ',', '29', '[UNK]', 'kostaldean', ',', '35', '[UNK]', 'hego', '-', 'ekialde', '##ko', 'basamortua', '##n', 'eta', '0', '[UNK]', 'iparraldeko', 'mendietan', '.']\n",
            "---------------------------------------------------------\n",
            "testua: xx. mendearen azkeneko urteetan sektore pribatuaren inbertsioak sustatu dira, baina, hala ere, estatuaren eskuetan daude oraindik produkzio lantegi handiak (zementua, ongarriak, altzairua…).\n",
            "\n",
            "tokenak: ['xx', '.', 'mendea', '##ren', 'azkeneko', 'urteetan', 'sektore', 'pribatu', '##aren', 'in', '##bertsioa', '##k', 'susta', '##tu', 'dira', ',', 'baina', ',', 'hala', 'ere', ',', 'estatua', '##ren', 'eskuetan', 'daude', 'oraindik', 'produkzio', 'lantegi', 'handiak', '(', 'ze', '##mentua', ',', 'ongarri', '##ak', ',', 'altzairua', '[UNK]', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: ingelesa ere ofiziala da, eta pakistandar elitearen eta gobernuko ministroen “lingua franca” da.\n",
            "\n",
            "tokenak: ['ingelesa', 'ere', 'ofiziala', 'da', ',', 'eta', 'pakistandar', 'elite', '##aren', 'eta', 'gobernu', '##ko', 'ministro', '##en', '[UNK]', 'ling', '##ua', 'franca', '[UNK]', 'da', '.']\n",
            "---------------------------------------------------------\n",
            "testua: («$1» orritik birbideratua)\n",
            "\n",
            "tokenak: ['(', '[UNK]', '$', '1', '[UNK]', 'orri', '##tik', 'birbiderat', '##ua', ')']\n",
            "---------------------------------------------------------\n",
            "testua: ezin izan da «$1» fitxategia ezabatu.\n",
            "\n",
            "tokenak: ['ezin', 'izan', 'da', '[UNK]', '$', '1', '[UNK]', 'fitxategia', 'ezabatu', '.']\n",
            "---------------------------------------------------------\n",
            "testua: «$1» aldatzen\n",
            "\n",
            "tokenak: ['[UNK]', '$', '1', '[UNK]', 'aldatzen']\n",
            "---------------------------------------------------------\n",
            "testua: desfasea¹\n",
            "\n",
            "tokenak: ['[UNK]']\n",
            "---------------------------------------------------------\n",
            "testua: «$1» orria erantsi da jarraitzen dituzun orrien zerrendara.\n",
            "\n",
            "tokenak: ['[UNK]', '$', '1', '[UNK]', 'orria', 'erantsi', 'da', 'jarraitzen', 'dituzun', 'orri', '##en', 'zerrenda', '##ra', '.']\n",
            "---------------------------------------------------------\n",
            "testua: ezin izan da «$1» ezabatu...\n",
            "\n",
            "tokenak: ['ezin', 'izan', 'da', '[UNK]', '$', '1', '[UNK]', 'ezabatu', '.', '.', '.']\n",
            "---------------------------------------------------------\n",
            "testua: «$1» ezabatu da\n",
            "\n",
            "tokenak: ['[UNK]', '$', '1', '[UNK]', 'ezabatu', 'da']\n",
            "---------------------------------------------------------\n",
            "testua: ezin da $2 (eztabaida) wikilariak «$1» orrian egindako azken aldaketa desegin; geroztik, besteren batek editatu du edo jada desegin du.\n",
            "\n",
            "tokenak: ['ezin', 'da', '$', '2', '(', 'eztabaida', ')', 'wikilaria', '##k', '[UNK]', '$', '1', '[UNK]', 'orrian', 'egindako', 'azken', 'aldaketa', 'desegin', ';', 'geroztik', ',', 'bester', '##en', 'batek', 'editatu', 'du', 'edo', 'jada', 'desegin', 'du', '.']\n",
            "---------------------------------------------------------\n",
            "testua: «$1» babestu da\n",
            "\n",
            "tokenak: ['[UNK]', '$', '1', '[UNK]', 'babestu', 'da']\n",
            "---------------------------------------------------------\n",
            "testua: «$1» orriari babesa kendu zaio\n",
            "\n",
            "tokenak: ['[UNK]', '$', '1', '[UNK]', 'orria', '##ri', 'babesa', 'kendu', 'zaio']\n",
            "---------------------------------------------------------\n",
            "testua: «$1» leheneratu da\n",
            "\n",
            "tokenak: ['[UNK]', '$', '1', '[UNK]', 'leheneratu', 'da']\n",
            "---------------------------------------------------------\n",
            "testua: ondorengo hauek dute «:$1» orrira etortzeko lotura:\n",
            "\n",
            "tokenak: ['ondorengo', 'haue', '##k', 'dute', '[UNK]', ':', '$', '1', '[UNK]', 'orri', '##ra', 'etortze', '##ko', 'lotura', ':']\n",
            "---------------------------------------------------------\n",
            "testua: «$1» izenburuaren ordez, «$2» ezarri da.\n",
            "\n",
            "tokenak: ['[UNK]', '$', '1', '[UNK]', 'izenburua', '##ren', 'ordez', ',', '[UNK]', '$', '2', '[UNK]', 'ezarri', 'da', '.']\n",
            "---------------------------------------------------------\n",
            "testua: «$1» izenburuaren ordez, «$2» ezarri da\n",
            "\n",
            "tokenak: ['[UNK]', '$', '1', '[UNK]', 'izenburua', '##ren', 'ordez', ',', '[UNK]', '$', '2', '[UNK]', 'ezarri', 'da']\n",
            "---------------------------------------------------------\n",
            "testua: * 2012 - ekaitz samaniego gasteizko gaztea «segiko kidea izateagatik» 8 urteko espetxe zigorra betetzeko atxilotu zuten.\n",
            "\n",
            "tokenak: ['*', '2012', '-', 'ekai', '##tz', 'samaniego', 'gasteiz', '##ko', 'gazte', '##a', '[UNK]', 'segi', '##ko', 'kidea', 'izateagatik', '[UNK]', '8', 'urteko', 'espetxe', 'zigorr', '##a', 'betetze', '##ko', 'atxilotu', 'zuten', '.']\n",
            "---------------------------------------------------------\n",
            "testua: * 1971 - jonathan davis, koяn musika taldeko abeslaria.\n",
            "\n",
            "tokenak: ['*', '1971', '-', 'jona', '##than', 'davis', ',', '[UNK]', 'musika', 'talde', '##ko', 'abeslaria', '.']\n",
            "---------------------------------------------------------\n",
            "testua: * 2012 - wisława szymborska, idazle poloniarra, 1986ko literaturako nobel sariduna (j.\n",
            "\n",
            "tokenak: ['*', '2012', '-', '[UNK]', 'szymborska', ',', 'idazle', 'poloniarra', ',', '1986', '##ko', 'literatura', '##ko', 'nobel', 'sariduna', '(', 'j', '.']\n",
            "---------------------------------------------------------\n",
            "testua: * 2014 - richard møller nielsen futbolari eta entrenatzaile daniarra (j.\n",
            "\n",
            "tokenak: ['*', '2014', '-', 'richard', '[UNK]', 'nielsen', 'futbolari', 'eta', 'entrenatzaile', 'daniarra', '(', 'j', '.']\n",
            "---------------------------------------------------------\n",
            "testua: * 1564 - galileo galilei, italiar filosofo, fisikari eta astronomoa (†1642).\n",
            "\n",
            "tokenak: ['*', '1564', '-', 'galileo', 'galilei', ',', 'italiar', 'filosofo', ',', 'fisikari', 'eta', 'astronomoa', '(', '[UNK]', '1642', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: * 1986 - 9½ weeks (\"9 aste t'erdi\") filma estreinatu zen, adrian lynek zuzendu eta kim basinger eta mickey rourke aktoreek anteztu zutelarik.\n",
            "\n",
            "tokenak: ['*', '1986', '-', '[UNK]', 'weeks', '(', '\"', '9', 'as', '##te', 't', \"'\", 'erdi', '\"', ')', 'filma', 'estreinat', '##u', 'zen', ',', 'adrian', 'lyne', '##k', 'zuzendu', 'eta', 'kim', 'basing', '##er', 'eta', 'mick', '##ey', 'rou', '##rke', 'aktoree', '##k', 'ante', '##ztu', 'zutela', '##rik', '.']\n",
            "---------------------------------------------------------\n",
            "testua: 675.417 km²ko eremua hartzen du eta 2013an 65.951.611 biztanle zituen.\n",
            "\n",
            "tokenak: ['67', '##5', '.', '41', '##7', '[UNK]', 'eremua', 'hartzen', 'du', 'eta', '2013', '##an', '65', '.', '95', '##1', '.', '61', '##1', 'biztanle', 'zituen', '.']\n",
            "---------------------------------------------------------\n",
            "testua: 2. platon (antzinako grezieraz πλάτων; atenas, c. k. a. 427 - ibidem, c. k. a. 347) grezia klasikoaren filosofo garrantzitsu bat izan zen.\n",
            "\n",
            "tokenak: ['2', '.', 'platon', '(', 'antzinako', 'grezieraz', '[UNK]', ';', 'atenas', ',', 'c', '.', 'k', '.', 'a', '.', '42', '##7', '-', 'ib', '##idem', ',', 'c', '.', 'k', '.', 'a', '.', '347', ')', 'grezia', 'klasikoa', '##ren', 'filosofo', 'garrantzitsu', 'bat', 'izan', 'zen', '.']\n",
            "---------------------------------------------------------\n",
            "testua: : -theklan · eztabaida · 18px 11:50, 26 iraila 2014 (utc) azpian azaldutakoagatik.\n",
            "\n",
            "tokenak: [':', '-', 'theklan', '[UNK]', 'eztabaida', '[UNK]', '18', '##px', '11', ':', '50', ',', '26', 'iraila', '2014', '(', 'ut', '##c', ')', 'azpian', 'azaldu', '##takoa', '##gatik', '.']\n",
            "---------------------------------------------------------\n",
            "testua: gizonezkoak ez gara izan inbisibleak historian zehar eta, horregatik, kategoria horrek ez du hainbeste zentzurik. -theklan · eztabaida · 18px 10:13, 26 iraila 2014 (utc)\n",
            "\n",
            "tokenak: ['gizonezkoa', '##k', 'ez', 'gara', 'izan', 'inbisib', '##leak', 'historian', 'zehar', 'eta', ',', 'horregatik', ',', 'kategoria', 'horrek', 'ez', 'du', 'hain', '##beste', 'zentzurik', '.', '-', 'theklan', '[UNK]', 'eztabaida', '[UNK]', '18', '##px', '10', ':', '13', ',', '26', 'iraila', '2014', '(', 'ut', '##c', ')']\n",
            "---------------------------------------------------------\n",
            "testua: pentsa lilatoian: parte-hartzaileak emakumeak bakarrik dira, baina bereziki emakumeak bultzatu nahi direlako berez azpi-ordezkatuak dauden eremu batean parte-hartzera. -theklan · eztabaida · 18px 11:49, 26 iraila 2014 (utc)\n",
            "\n",
            "tokenak: ['pentsa', 'lilatoian', ':', 'parte', '-', 'hart', '##zaile', '##ak', 'emakumeak', 'bakarrik', 'dira', ',', 'baina', 'bereziki', 'emakumeak', 'bultzatu', 'nahi', 'direlako', 'berez', 'azpi', '-', 'ordezkatu', '##ak', 'dauden', 'eremu', 'batean', 'parte', '-', 'hartze', '##ra', '.', '-', 'theklan', '[UNK]', 'eztabaida', '[UNK]', '18', '##px', '11', ':', '49', ',', '26', 'iraila', '2014', '(', 'ut', '##c', ')']\n",
            "---------------------------------------------------------\n",
            "testua: ikusi ingelesezko artikuluaren sarrera, euskarazkoa baino egokiagoa dela uste dut: hypatia (/haɪˈpeɪʃə/ hy-pay-shə; greek: ὑπατία hypatía) (born c. ad 350 – 370; killed by christians in 415) was a greek alexandrian neoplatonist philosopher in egypt.\n",
            "\n",
            "tokenak: ['ikusi', 'ingelesezko', 'artikuluare', '##n', 'sarrera', ',', 'euskarazko', '##a', 'baino', 'egokia', '##goa', 'dela', 'uste', 'dut', ':', 'hypat', '##ia', '(', '/', '[UNK]', '/', 'hy', '-', 'pay', '-', '[UNK]', ';', 'greek', ':', '[UNK]', 'hypat', '##ia', ')', '(', 'born', 'c', '.', 'ad', '350', '–', '370', ';', 'killed', 'by', 'christians', 'in', '415', ')', 'was', 'a', 'greek', 'alexandria', '##n', 'neoplatonis', '##t', 'philosopher', 'in', 'egypt', '.']\n",
            "---------------------------------------------------------\n",
            "testua: be:філосафы\n",
            "\n",
            "tokenak: ['be', ':', '[UNK]']\n",
            "---------------------------------------------------------\n",
            "testua: be-x-old:філёзаф\n",
            "\n",
            "tokenak: ['be', '-', 'x', '-', 'old', ':', '[UNK]']\n",
            "---------------------------------------------------------\n",
            "testua: * martxoaren 12a - zoran đinđić, serbia eta montenegroko presidentea, erahila (j. ).\n",
            "\n",
            "tokenak: ['*', 'martxoa', '##ren', '12', '##a', '-', 'zor', '##an', '[UNK]', ',', 'serbia', 'eta', 'montenegro', '##ko', 'presidentea', ',', 'era', '##hila', '(', 'j', '.', ')', '.']\n",
            "---------------------------------------------------------\n",
            "testua: :1983 lech wałęsa\n",
            "\n",
            "tokenak: [':', '1983', 'lech', '[UNK]']\n",
            "---------------------------------------------------------\n",
            "testua: * bernard etxeparek bordelen argitaratu zuen “linguae vasconum primitiae”, lehenengo euskarazko liburua.\n",
            "\n",
            "tokenak: ['*', 'bernard', 'etxepare', '##k', 'bordele', '##n', 'argitaratu', 'zuen', '[UNK]', 'ling', '##ua', '##e', 'vascon', '##um', 'primiti', '##ae', '[UNK]', ',', 'lehenengo', 'euskarazko', 'liburua', '.']\n",
            "---------------------------------------------------------\n",
            "testua: * joanes leizarragak “iesus christ gure iaunaren testamentu berria” liburua karrikaratu zuen ipar euskal herrian.\n",
            "\n",
            "tokenak: ['*', 'joane', '##s', 'leizarragak', '[UNK]', 'i', '##es', '##us', 'christ', 'gur', '##e', 'ia', '##unaren', 'testamentu', 'berria', '[UNK]', 'liburua', 'kar', '##rika', '##ratu', 'zuen', 'ipar', 'euskal', 'herrian', '.']\n",
            "---------------------------------------------------------\n",
            "wordpiece tokens: 117974\n",
            "sentencepiece tokens: 117055\n",
            "[UNK] kop: 167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn54GJq95Uf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python bert/create_pretraining_data.py \\\n",
        "  --input_file=$GS/corpus/eu/2014wiki.eu.sent_splited \\\n",
        "  --output_file=$GS/wordpiece/pretraining.tf.data \\\n",
        "  --vocab_file=vocab.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=666 \\\n",
        "  --do_whole_word_mask=True \\\n",
        "  #--dupe_factor=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvMUE9sT6P1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "!python src/run_pretraining.py \\\n",
        "  --config_file eu.congif.ini \\\n",
        "  --input_file=$GS/pretraining.tf.data \\\n",
        "  --output_dir=$GS/eu.gureBERT \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=1000000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "'''\n",
        "\n",
        "!python bert/run_pretraining.py \\\n",
        "  --input_file=$GS/wordpiece/pretraining.tf.data \\ \\\n",
        "  --output_dir=$GS/wordpiece/model \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=10000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  #--num_train_steps=1000000 \\\n",
        "  #--init_checkpoint=$GS/wordpiece/model \\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNV42_z-O92_",
        "colab_type": "text"
      },
      "source": [
        "### EN-EU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0de34369-b7c5-4468-c452-313f8f13c0a7",
        "id": "k-kNc1NmPg55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format('./spModels/en-eu'))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 44999\n",
            "Sample tokens: ['set', '▁gmt', 'rma', 'slapp', 'function', '▁via', 'rs', 'steinsaltz', '▁padding', '▁substantial']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IMHIP2dDPg7M",
        "colab": {}
      },
      "source": [
        "# hau errepasatu. adibidez @ ez dut ondo egiten... [UNK] agertzen da...\n",
        "# adibidez, \n",
        "\n",
        "#import string\n",
        "#def parse_sentencepiece_token(token):\n",
        "#    if token.startswith(\"▁\"):\n",
        "#        return token[1:]\n",
        "#    else:\n",
        "#        if token in string.punctuation:\n",
        "#            return token\n",
        "#        else:\n",
        "#            return \"##\" + token\n",
        "          \n",
        "\n",
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a12f72c9-4929-450b-ca16-3b6a48892440",
        "id": "TYrllb6wPg7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import string\n",
        "\n",
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "# puntuazio sinboloak gehitu \n",
        "ctrl_symbols_end = list(string.punctuation)\n",
        "#ctrl_symbols = [\"[UNK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab + ctrl_symbols_end\n",
        "\n",
        "#bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(32000 - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RQVCJ3izPg72",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab-en_eu.txt\"\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "076d5775-e53b-488c-997f-7a67b49e1fd2",
        "id": "Lm-A4Er3Pg8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from bert import tokenization\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "print(bert_tokenizer.tokenize(\"Nere kotxea aitonaren etxe alboan dago!, eta bere kolorea gorria da. ni@ni.eus erabili \"))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ne', '##re', 'ko', '##txea', 'ai', '##ton', '##aren', 'etxe', 'alb', '##oa', '##n', 'dago', '!', ',', 'eta', 'bere', 'kolore', '##a', 'gorria', 'da', '.', 'ni', '@', 'ni', '.', 'eu', '##s', 'erabili']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q0P25Z_LPg8W",
        "colab": {}
      },
      "source": [
        "FILES = \"./corpus/en-eu/2014wiki.eu.sent_splited,./corpus/en-eu/2019wiki-10k.en.sent_splited\"\n",
        "\n",
        "!python bert/create_pretraining_data.py \\\n",
        "  --input_file={FILES} \\\n",
        "  --output_file=$GS/wordpiece/pretraining-en_eu.tf.data \\\n",
        "  --vocab_file=vocab-en_eu.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=666 \\\n",
        "  --do_whole_word_mask=True \\\n",
        "  #--dupe_factor=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o1hyibNBPg8o",
        "colab": {}
      },
      "source": [
        "'''\n",
        "!python src/run_pretraining.py \\\n",
        "  --config_file eu.congif.ini \\\n",
        "  --input_file=$GS/pretraining.tf.data \\\n",
        "  --output_dir=$GS/eu.gureBERT \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=1000000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "'''\n",
        "\n",
        "!python bert/run_pretraining.py \\\n",
        "  --input_file=$GS/wordpiece/pretraining-en_eu.tf.data \\ \\\n",
        "  --output_dir=$GS/wordpiece/model-en_eu \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=20000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  --init_checkpoint=$GS/wordpiece/model-en_eu/ \\\n",
        "  #--num_train_steps=1000000 \\\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zAbTlosgZiP",
        "colab_type": "text"
      },
      "source": [
        "### SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrHSiJmsa_up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data English da!! baino kodea badabil \n",
        "\n",
        "!python bert/run_squad.py \\\n",
        "  --vocab_file=vocab-en_eu.txt \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --do_lower_case=True \\\n",
        "  --do_train=True \\\n",
        "  --train_file=train-v2.0.json \\\n",
        "  --do_predict=True \\\n",
        "  --predict_file=dev-v2.0.json \\\n",
        "  --train_batch_size=24 \\\n",
        "  --learning_rate=3e-5 \\\n",
        "  --num_train_epochs=0.1 \\\n",
        "  --max_seq_length=384 \\\n",
        "  --doc_stride=128 \\\n",
        "  --output_dir=gs://gurebert/gureBERT/wordpiece/squad/ \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name=$TPU_NAME \\\n",
        "  --version_2_with_negative=True \\\n",
        "  --init_checkpoint=gs://gurebert/gureBERT/wordpiece/model-en_eu/ \\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUcKt7Ehn6lV",
        "colab_type": "text"
      },
      "source": [
        "# gureBERT (japanese) erabiliz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqPX8zgt6lAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################################################\n",
        "# gureBERT japanese\n",
        "\n",
        "!cd /content/\n",
        "\n",
        "!git clone --recursive https://github.com/zmwebdev/bert-japanese\n",
        "%cd bert-japanese\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d68MKTOjQxVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrhObi5n8xYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/data-download-and-extract.py\n",
        "!bash src/file-preprocessing.sh\n",
        "\n",
        "#!gsutil cp -r gs://gurebert/gureBERT/data/wiki/AA/wiki_00 data/wiki/AA/wiki_00\n",
        "\n",
        "# Oharra: esaldiak lerro bakarrean jarri behar dira\n",
        "#!cat data/wiki/AA/wiki_00"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m0o4LUCJ8ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wiki-eu.vocab eta wiki-eu.model sortzen ditu\n",
        "\n",
        "!pip install sentencepiece\n",
        "!python3 src/train-sentencepiece.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJFc3jAkOyHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp -r data/ gs://gurebert/gureBERT/\n",
        "!gsutil cp -r model/ gs://gurebert/gureBERT/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_qxPQAgP1sA",
        "colab_type": "code",
        "outputId": "358ed8f7-dd3c-4ef6-aee1-74edd53a49ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "!mkdir model\n",
        "!gsutil cp gs://gurebert/gureBERT/model/wiki-eu.vocab model\n",
        "!gsutil cp gs://gurebert/gureBERT/model/wiki-eu.model model\n",
        "!head -n 50 model/wiki-eu.vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>\t0\n",
            "<s>\t0\n",
            "</s>\t0\n",
            "[PAD]\t0\n",
            "[CLS]\t0\n",
            "[SEP]\t0\n",
            "[MASK]\t0\n",
            ",\t-2.87014\n",
            ".\t-2.98912\n",
            "▁eta\t-3.5406\n",
            "▁ziren\t-4.15683\n",
            "-\t-4.26943\n",
            "an\t-4.4068\n",
            "▁da\t-4.40907\n",
            "a\t-4.47601\n",
            "▁zen\t-4.51427\n",
            "ko\t-4.52925\n",
            "▁\t-4.67755\n",
            "▁zeuden\t-4.76555\n",
            "▁bat\t-4.84966\n",
            "▁\"\t-4.95145\n",
            "\"\t-4.95743\n",
            "▁1\t-5.04005\n",
            "ak\t-5.2203\n",
            "▁izan\t-5.32102\n",
            "▁zuen\t-5.34175\n",
            "en\t-5.36722\n",
            "ren\t-5.38606\n",
            ":\t-5.53342\n",
            "k\t-5.56319\n",
            "▁(\t-5.58916\n",
            "▁zituzten\t-5.58963\n",
            "▁zuten\t-5.59859\n",
            "▁etxek\t-5.66741\n",
            ")\t-5.66971\n",
            "▁du\t-5.70751\n",
            "▁zituen\t-5.72891\n",
            "▁enpresak\t-5.73378\n",
            "▁bere\t-5.7694\n",
            "▁2\t-5.78944\n",
            "▁dago\t-5.79301\n",
            "▁2009\t-5.8295\n",
            "▁ere\t-5.84878\n",
            "▁2007\t-5.87422\n",
            "n\t-5.87535\n",
            "▁pertsona\t-5.88342\n",
            "▁biztanle\t-5.93646\n",
            "aren\t-5.97756\n",
            "▁dira\t-5.97904\n",
            "▁ditu\t-6.0186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl457PDf8r-y",
        "colab_type": "code",
        "outputId": "320787c0-4f0e-4d29-f1af-3adecece4172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#https://github.com/zmwebdev/bert-japanese/blob/master/notebook/check-trained-tokenizer.ipynb\n",
        "!pip install sentencepiece\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "import tokenization_sentencepiece as tokenization\n",
        "\n",
        "text1 = \"Nere kotxea aitonaren etxe alboan dago\"\n",
        "text2 = \"Gorria da gure etxearen inguruan dagoen lorearen kolorea\"\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    model_file=\"model/wiki-eu.model\",\n",
        "    vocab_file=\"model/wiki-eu.vocab\",\n",
        "    do_lower_case=True)\n",
        "\n",
        "tokenizer.tokenize(text1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.82)\n",
            "Loaded a trained SentencePiece model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁nere', '▁kotxe', 'a', '▁aitona', 'ren', '▁etxe', '▁alboan', '▁dago']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ExyRkNqGpvi",
        "colab_type": "code",
        "outputId": "96fb8001-d24d-47ea-ae3b-dc80469fca0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "tokenizer.tokenize(text2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁gorria',\n",
              " '▁da',\n",
              " '▁gure',\n",
              " '▁etxearen',\n",
              " '▁inguruan',\n",
              " '▁dagoen',\n",
              " '▁lore',\n",
              " 'aren',\n",
              " '▁kolorea']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u9yUjU0HHwx",
        "colab_type": "code",
        "outputId": "e63d3131-22f9-47e5-8437-1527aac1446f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#do_lower_case=False jarrita tokenizazioa okerragoa da\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    model_file=\"model/wiki-eu.model\",\n",
        "    vocab_file=\"model/wiki-eu.vocab\",\n",
        "    do_lower_case=False)\n",
        "\n",
        "tokenizer.tokenize(text1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁', 'N', 'ere', '▁kotxe', 'a', '▁aitona', 'ren', '▁etxe', '▁alboan', '▁dago']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZMfqyxoHZnG",
        "colab_type": "code",
        "outputId": "d0cf6574-bcd8-4e35-e295-d0fee84fe64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "tokenizer.tokenize(text2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁',\n",
              " 'G',\n",
              " 'orri',\n",
              " 'a',\n",
              " '▁da',\n",
              " '▁gure',\n",
              " '▁etxearen',\n",
              " '▁inguruan',\n",
              " '▁dagoen',\n",
              " '▁lore',\n",
              " 'aren',\n",
              " '▁kolorea']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wosrOAdz2gQs",
        "colab_type": "code",
        "outputId": "d85a3585-d40c-43f9-930b-13475e0b2c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "# convert to WordPiece (kodean errorea dago). begiratu https://github.com/kwonmha/bert-vocab-builder/pull/4#issue-291306156\n",
        "\n",
        "!git clone https://github.com/kwonmha/bert-vocab-builder.git\n",
        "\n",
        "!python bert-vocab-builder/subword_builder.py \\\n",
        "--corpus_filepattern model/wiki-eu.vocab \\\n",
        "--output_filename model/wiki-eu-wordpiece.vocab\n",
        "#--min_count {minimum_subtoken_counts}\n",
        "\n",
        "# https://github.com/kwonmha/bert-vocab-builder/pull/4#issue-291306156"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'bert-vocab-builder' already exists and is not an empty directory.\n",
            "Traceback (most recent call last):\n",
            "  File \"bert-vocab-builder/subword_builder.py\", line 33, in <module>\n",
            "    import text_encoder\n",
            "  File \"/content/bert-japanese/bert-vocab-builder/text_encoder.py\", line 647\n",
            "    <<<<<<< HEAD\n",
            "     ^\n",
            "SyntaxError: invalid syntax\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBDprGbXRt4l",
        "colab_type": "code",
        "outputId": "1093ee9f-3fc7-48f5-96de-55fe03096bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "#Creating data for pretraining\n",
        "#Create .tfrecord files for pretraining. For longer sentence data, replace the value of max_seq_length with 512.\n",
        "!cat creating_data_for_pretraining.sh\n",
        "!bash creating_data_for_pretraining.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for DIR in $( find data/wiki/ -mindepth 1 -type d ); do \n",
            "  python3 src/create_pretraining_data.py \\\n",
            "    --input_file=${DIR}/all.txt \\\n",
            "    --output_file=${DIR}/all-maxseq128.tfrecord \\\n",
            "    --model_file=./model/wiki-eu.model \\\n",
            "    --vocab_file=./model/wiki-eu.vocab \\\n",
            "    --do_lower_case=True \\\n",
            "    --max_seq_length=128 \\\n",
            "    --max_predictions_per_seq=20 \\\n",
            "    --masked_lm_prob=0.15 \\\n",
            "    --random_seed=12345 \\\n",
            "    --dupe_factor=5\n",
            "done"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlBYBR1_G9Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy to gs\n",
        "!gsutil cp -r data gs://gurebert/gureBERT\n",
        "!gsutil cp -r model gs://gurebert/gureBERT\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29sDsZvBgFLj",
        "colab_type": "code",
        "outputId": "d69c0256-6298-492b-9d3e-b1fa8e086803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# Pre-Training\n",
        "# https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/pretraining.ipynb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "#Check TPU devices\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.16.115.170:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 5460104680638402674),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16051411686286296001),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1933247979704415977),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11046566402910946519),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13421296677076221120),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5352660506872180047),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7874427115731877787),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15986719012258325089),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7612787266547945911),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6745551487070946188),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14981037038192380193)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AzYwFXw8iJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DATA_GCS = 'gs://gurebert/gureBERT/data/wiki'\n",
        "TARGET_DIRS = [\n",
        "  'AA',\n",
        "  'AB',\n",
        "  'AC'\n",
        "]\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "#MAX_SEQ_LEN = 512\n",
        "\n",
        "\n",
        "INPUT_FILE = ','.join( [ '{}/{}/all-maxseq{}.tfrecord'.format(INPUT_DATA_GCS, elem, MAX_SEQ_LEN) for elem in TARGET_DIRS] )\n",
        "\n",
        "OUTPUT_GCS = 'gs://gurebert/gureBERT/model'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRRjRvl19NDV",
        "colab_type": "code",
        "outputId": "7135ff35-5f86-46c3-afe0-5f4e4312dddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Adding whole word masking aldaketa egin dute BERT-en. update egin behar da!!!\n",
        "# Begiratu https://github.com/google-research/bert/commit/0fce551b55caabcfba52c61e18f34b541aef186a\n",
        "\n",
        "!python src/run_pretraining.py \\\n",
        "  --input_file={INPUT_FILE} \\\n",
        "  --output_dir={OUTPUT_GCS} \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  --num_tpu_cores=8 \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length={MAX_SEQ_LEN} \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=1400000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0625 19:38:15.208214 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0625 19:38:15.209448 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:497: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0625 19:38:15.210220 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:412: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0625 19:38:15.210398 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:412: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0625 19:38:15.210564 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0625 19:38:15.211427 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:419: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0625 19:38:16.644103 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:423: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0625 19:38:17.434032 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:425: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0625 19:38:17.434276 140282338469760 run_pretraining.py:425] *** Input Files ***\n",
            "I0625 19:38:17.434366 140282338469760 run_pretraining.py:427]   gs://gurebert/gureBERT/data/wiki/AA/all-maxseq128.tfrecord\n",
            "I0625 19:38:17.434440 140282338469760 run_pretraining.py:427]   gs://gurebert/gureBERT/data/wiki/AB/all-maxseq128.tfrecord\n",
            "I0625 19:38:17.434512 140282338469760 run_pretraining.py:427]   gs://gurebert/gureBERT/data/wiki/AC/all-maxseq128.tfrecord\n",
            "W0625 19:38:18.462627 140282338469760 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0625 19:38:19.468373 140282338469760 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f95d11146a8>) includes params argument, but params are not passed to Estimator.\n",
            "I0625 19:38:19.469875 140282338469760 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.16.115.170:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f95d10b3588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.16.115.170:8470', '_evaluation_master': 'grpc://10.16.115.170:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f95dd593be0>}\n",
            "I0625 19:38:19.470217 140282338469760 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0625 19:38:19.470873 140282338469760 run_pretraining.py:464] ***** Running training *****\n",
            "I0625 19:38:19.470968 140282338469760 run_pretraining.py:465]   Batch size = 64\n",
            "I0625 19:38:23.258418 140282338469760 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.16.115.170:8470) for TPU system metadata.\n",
            "2019-06-25 19:38:23.259746: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0625 19:38:23.274165 140282338469760 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0625 19:38:23.274393 140282338469760 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0625 19:38:23.274487 140282338469760 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0625 19:38:23.274553 140282338469760 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0625 19:38:23.274617 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5460104680638402674)\n",
            "I0625 19:38:23.275380 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1933247979704415977)\n",
            "I0625 19:38:23.275457 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11046566402910946519)\n",
            "I0625 19:38:23.275525 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13421296677076221120)\n",
            "I0625 19:38:23.275590 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5352660506872180047)\n",
            "I0625 19:38:23.275657 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7874427115731877787)\n",
            "I0625 19:38:23.275721 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15986719012258325089)\n",
            "I0625 19:38:23.275784 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7612787266547945911)\n",
            "I0625 19:38:23.275844 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6745551487070946188)\n",
            "I0625 19:38:23.275904 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14981037038192380193)\n",
            "I0625 19:38:23.275966 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16051411686286296001)\n",
            "W0625 19:38:23.282766 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0625 19:38:23.299888 140282338469760 estimator.py:1145] Calling model_fn.\n",
            "W0625 19:38:23.300519 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:342: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0625 19:38:23.306573 140282338469760 deprecation.py:323] From src/run_pretraining.py:373: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0625 19:38:23.306786 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0625 19:38:23.335085 140282338469760 deprecation.py:323] From src/run_pretraining.py:390: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0625 19:38:23.335338 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0625 19:38:23.336897 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:398: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0625 19:38:23.342769 140282338469760 deprecation.py:323] From src/run_pretraining.py:405: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0625 19:38:23.426860 140282338469760 run_pretraining.py:122] *** Features ***\n",
            "I0625 19:38:23.427149 140282338469760 run_pretraining.py:124]   name = input_ids, shape = (8, 128)\n",
            "I0625 19:38:23.427274 140282338469760 run_pretraining.py:124]   name = input_mask, shape = (8, 128)\n",
            "I0625 19:38:23.427362 140282338469760 run_pretraining.py:124]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0625 19:38:23.427445 140282338469760 run_pretraining.py:124]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0625 19:38:23.427529 140282338469760 run_pretraining.py:124]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0625 19:38:23.427610 140282338469760 run_pretraining.py:124]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0625 19:38:23.427688 140282338469760 run_pretraining.py:124]   name = segment_ids, shape = (8, 128)\n",
            "W0625 19:38:23.427910 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:172: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0625 19:38:23.430187 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:411: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0625 19:38:23.466429 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:492: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0625 19:38:23.524997 140282338469760 deprecation.py:506] From /content/bert-japanese/src/../bert/modeling.py:359: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0625 19:38:23.546822 140282338469760 deprecation.py:323] From /content/bert-japanese/src/../bert/modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0625 19:38:27.546400 140282338469760 run_pretraining.py:172] **** Trainable Variables ****\n",
            "I0625 19:38:27.546652 140282338469760 run_pretraining.py:178]   name = bert/embeddings/word_embeddings:0, shape = (32000, 768)\n",
            "I0625 19:38:27.546783 140282338469760 run_pretraining.py:178]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0625 19:38:27.546877 140282338469760 run_pretraining.py:178]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0625 19:38:27.546964 140282338469760 run_pretraining.py:178]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.547065 140282338469760 run_pretraining.py:178]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.547148 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547240 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547317 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547397 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547473 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547553 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547628 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547708 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547784 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.547859 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.547933 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.548025 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.548103 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.548182 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548265 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.548341 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.548416 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548494 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548570 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548649 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548723 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548801 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548876 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548953 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.549039 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.549117 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.549198 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.549278 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.549353 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.549464 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.549555 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.549630 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.549705 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.549783 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.549859 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.549938 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550027 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.550108 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550183 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.550268 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550342 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.550415 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.550496 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.550579 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.550655 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.550734 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550811 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.550886 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.550960 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551053 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551129 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551213 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551289 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551367 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551442 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551520 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551597 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.551673 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.551748 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.551826 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.551902 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.551980 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552069 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.552144 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.552223 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552304 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552379 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552458 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552532 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552613 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552688 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552767 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552842 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.552917 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.552991 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.553102 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.553179 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.553265 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553341 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.553414 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.553490 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.553569 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553642 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.553722 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553798 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.553876 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553951 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.554041 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.554118 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.554199 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.554275 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.554352 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.554429 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.554506 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.554581 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.554655 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.554730 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.554809 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.554883 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.554962 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555051 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.555130 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555211 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.555290 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555366 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.555443 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.555517 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.555596 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.555672 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.555750 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555824 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.555898 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.555972 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.556064 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.556140 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.556225 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.556300 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.586935 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.587312 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.587467 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.587575 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.587675 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.587777 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.587887 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.587991 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.588119 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.588230 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.588329 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.588429 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.588537 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.588637 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.588743 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.588844 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.588951 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.589072 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.589181 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.589300 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.589403 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.589504 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.589615 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.589721 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.589831 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.589935 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.590058 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.590166 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590286 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.590391 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590503 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.590605 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590713 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.590820 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590930 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.591052 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.591159 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.591277 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.591384 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.591493 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.591601 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.591722 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.591833 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.591942 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592070 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592176 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592298 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592424 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592534 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592635 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592743 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592846 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.592947 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.593068 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.593182 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.593296 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.593404 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.593508 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.593610 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.593711 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.593819 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.593922 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.594047 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.594153 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.594272 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.594377 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.594484 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.594584 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.594686 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.594788 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.594895 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.594999 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.595129 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.595242 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.595343 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.595443 140282338469760 run_pretraining.py:178]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.595552 140282338469760 run_pretraining.py:178]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.595655 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.595763 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.595866 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.595969 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.596090 140282338469760 run_pretraining.py:178]   name = cls/predictions/output_bias:0, shape = (32000,)\n",
            "I0625 19:38:27.596205 140282338469760 run_pretraining.py:178]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0625 19:38:27.596317 140282338469760 run_pretraining.py:178]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0625 19:38:27.596510 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0625 19:38:27.598321 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0625 19:38:27.606181 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0625 19:38:32.398603 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0625 19:38:41.775387 140282338469760 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0625 19:38:42.164427 140282338469760 estimator.py:1147] Done calling model_fn.\n",
            "I0625 19:38:46.161324 140282338469760 tpu_estimator.py:499] TPU job name worker\n",
            "I0625 19:38:47.572438 140282338469760 monitored_session.py:240] Graph was finalized.\n",
            "W0625 19:38:47.744163 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0625 19:38:47.907891 140282338469760 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/model/model.ckpt-610000\n",
            "W0625 19:39:14.770554 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0625 19:39:17.352350 140282338469760 session_manager.py:500] Running local_init_op.\n",
            "I0625 19:39:17.997242 140282338469760 session_manager.py:502] Done running local_init_op.\n",
            "I0625 19:39:29.001356 140282338469760 basic_session_run_hooks.py:606] Saving checkpoints for 610000 into gs://gurebert/gureBERT/model/model.ckpt.\n",
            "W0625 19:39:56.630694 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0625 19:39:58.038233 140282338469760 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0625 19:39:58.039453 140282338469760 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-06-25 19:39:58.039894: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0625 19:39:58.044787 140282338469760 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0625 19:39:58.047351 140282338469760 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0625 19:39:58.051084 140282338469760 tpu_estimator.py:557] Init TPU system\n",
            "I0625 19:40:05.458847 140282338469760 tpu_estimator.py:566] Initialized TPU in 7 seconds\n",
            "I0625 19:40:05.459836 140281195702016 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0625 19:40:05.460223 140281178138368 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0625 19:40:06.142425 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:40:06.143549 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:40:40.743851 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0625 19:41:40.791741 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (0, 892)\n",
            "I0625 19:41:49.880066 140282338469760 basic_session_run_hooks.py:262] loss = 0.6504045, step = 611000\n",
            "I0625 19:41:49.883220 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:41:49.883571 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:42:40.830728 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (1, 646)\n",
            "I0625 19:43:05.396509 140282338469760 basic_session_run_hooks.py:260] loss = 0.50328124, step = 612000 (75.516 sec)\n",
            "I0625 19:43:05.398332 140282338469760 tpu_estimator.py:2159] global_step/sec: 13.2422\n",
            "I0625 19:43:05.399384 140282338469760 tpu_estimator.py:2160] examples/sec: 847.5\n",
            "I0625 19:43:05.400921 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:43:05.401206 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:43:40.897238 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (2, 509)\n",
            "I0625 19:44:14.648333 140282338469760 basic_session_run_hooks.py:260] loss = 1.0970724, step = 613000 (69.252 sec)\n",
            "I0625 19:44:14.650186 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4401\n",
            "I0625 19:44:14.650439 140282338469760 tpu_estimator.py:2160] examples/sec: 924.163\n",
            "I0625 19:44:15.609725 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:44:15.610541 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:44:40.963851 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (3, 359)\n",
            "I0625 19:45:24.749946 140282338469760 basic_session_run_hooks.py:260] loss = 1.6517268, step = 614000 (70.102 sec)\n",
            "I0625 19:45:24.751514 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.2651\n",
            "I0625 19:45:24.751736 140282338469760 tpu_estimator.py:2160] examples/sec: 912.964\n",
            "I0625 19:45:24.752844 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:45:24.753101 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:45:40.979294 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (4, 223)\n",
            "I0625 19:46:34.015049 140282338469760 basic_session_run_hooks.py:260] loss = 1.376927, step = 615000 (69.265 sec)\n",
            "I0625 19:46:34.016614 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4373\n",
            "I0625 19:46:34.016878 140282338469760 tpu_estimator.py:2160] examples/sec: 923.986\n",
            "I0625 19:46:34.874679 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:46:34.875390 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:46:40.998494 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (5, 73)\n",
            "I0625 19:47:41.048521 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (5, 965)\n",
            "I0625 19:47:44.127715 140282338469760 basic_session_run_hooks.py:260] loss = 1.4815391, step = 616000 (70.113 sec)\n",
            "I0625 19:47:44.129233 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.2628\n",
            "I0625 19:47:44.129462 140282338469760 tpu_estimator.py:2160] examples/sec: 912.818\n",
            "I0625 19:47:44.130714 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:47:44.130944 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:48:41.099297 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (6, 829)\n",
            "I0625 19:48:54.730155 140282338469760 basic_session_run_hooks.py:260] loss = 0.98817366, step = 617000 (70.602 sec)\n",
            "I0625 19:48:54.731993 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.1637\n",
            "I0625 19:48:54.732534 140282338469760 tpu_estimator.py:2160] examples/sec: 906.479\n",
            "I0625 19:48:54.733717 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:48:54.733918 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:49:41.108434 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (7, 671)\n",
            "I0625 19:50:03.977603 140282338469760 basic_session_run_hooks.py:260] loss = 1.0366956, step = 618000 (69.247 sec)\n",
            "I0625 19:50:03.979490 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.441\n",
            "I0625 19:50:03.979758 140282338469760 tpu_estimator.py:2160] examples/sec: 924.222\n",
            "I0625 19:50:03.981185 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:50:03.981428 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:50:41.152918 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (8, 534)\n",
            "I0625 19:51:13.211040 140282338469760 basic_session_run_hooks.py:260] loss = 0.9602766, step = 619000 (69.233 sec)\n",
            "I0625 19:51:13.215205 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4434\n",
            "I0625 19:51:13.215537 140282338469760 tpu_estimator.py:2160] examples/sec: 924.379\n",
            "I0625 19:51:14.192095 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:51:14.192481 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:51:41.192183 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (9, 383)\n",
            "I0625 19:52:23.419371 140282338469760 basic_session_run_hooks.py:606] Saving checkpoints for 620000 into gs://gurebert/gureBERT/model/model.ckpt.\n",
            "W0625 19:52:46.765204 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "I0625 19:52:51.234167 140282338469760 basic_session_run_hooks.py:260] loss = 0.8747845, step = 620000 (98.023 sec)\n",
            "I0625 19:52:51.235810 140282338469760 tpu_estimator.py:2159] global_step/sec: 10.2019\n",
            "I0625 19:52:51.236226 140282338469760 tpu_estimator.py:2160] examples/sec: 652.923\n",
            "I0625 19:52:51.237428 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:52:51.237631 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:52:52.456125 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (10, 0)\n",
            "I0625 19:53:52.494190 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (10, 892)\n",
            "I0625 19:54:00.463160 140282338469760 basic_session_run_hooks.py:260] loss = 1.1868722, step = 621000 (69.229 sec)\n",
            "I0625 19:54:00.464810 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4448\n",
            "I0625 19:54:02.102097 140282338469760 tpu_estimator.py:2160] examples/sec: 924.469\n",
            "I0625 19:54:02.104723 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:54:02.105079 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:54:52.541748 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (11, 731)\n",
            "I0625 19:55:11.311046 140282338469760 basic_session_run_hooks.py:260] loss = 0.7853336, step = 622000 (70.848 sec)\n",
            "I0625 19:55:11.313117 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.1147\n",
            "I0625 19:55:11.313430 140282338469760 tpu_estimator.py:2160] examples/sec: 903.339\n",
            "I0625 19:55:11.314776 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:55:11.315082 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:55:52.561795 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (12, 595)\n",
            "I0625 19:56:21.423934 140282338469760 basic_session_run_hooks.py:260] loss = 1.150667, step = 623000 (70.113 sec)\n",
            "I0625 19:56:21.425779 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.2628\n",
            "I0625 19:56:21.426093 140282338469760 tpu_estimator.py:2160] examples/sec: 912.816\n",
            "I0625 19:56:21.427527 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:56:21.427800 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MGl6QPM4B9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################\n",
        "##########################################\n",
        "\n",
        "# BERT-ekin konpatiblea den vocab sortzeko prozedura. wordpiece _ -> ##. \n",
        "# Honen abantaila BERT kodearen aldaketak eta run_squad.py, ... ezer aldatu gabe ibiliko dela da.\n",
        "# berez ez da wordpiece, sentencepiece baizik baina sintaktikoki konpatiblea\n",
        "# ikusi: https://colab.research.google.com/drive/1-uLyGTnz2K4gx3su6Qc00in9QCa4b5Kh#scrollTo=9S4CiOh3RzFW\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "559f9136-b061-48b5-fc01-bac5a987c93f",
        "id": "psBPQwy2y8UO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format('model/wiki-eu'))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 31999\n",
            "Sample tokens: ['.036', '▁ukuilu', 'lok', '▁ahaltsua', '▁batzuei', '▁erabili', '▁chapman', '▁tv', '▁pe', '▁polemika']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mia3KTjty8Vp",
        "colab": {}
      },
      "source": [
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "63bd9984-804d-47ea-dbff-1dc6aa131f45",
        "id": "9LbsNuney8V-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab\n",
        "\n",
        "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(32000 - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jxBd_J30y8WV",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c9f23d61-8211-4ac2-cef4-f963e7f8db16",
        "id": "OgRdyQpEy8Wh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from bert import tokenization\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(\"Nere kotxea aitonaren etxe alboan dago\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nere', 'kotxe', '##a', 'aitona', '##ren', 'etxe', 'alboan', 'dago']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bfd2e057-bd2b-4704-8cf8-ada627a37600",
        "id": "dAZRoG-Jy8W1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#!gsutil cp -r gs://gurebert/gureBERT/data/wiki/AA/wiki_00 wiki_00\n",
        "# ez du wiki-eu.model behar, vocab.txt-ekin nahikoa da\n",
        "\n",
        "!python bert/create_pretraining_data.py \\\n",
        "  --input_file=wiki_00 \\\n",
        "  --output_file=/tmp/tf_examples.tfrecord \\\n",
        "  --vocab_file=vocab.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=128 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=12345 \\\n",
        "  --dupe_factor=5\n",
        "\n",
        "#############################################################\n",
        "#############################################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf_examples.tfrecord  tmpcqwLhN\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}