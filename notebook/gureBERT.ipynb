{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gureBERT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRIGPzfVoUlL",
        "colab_type": "text"
      },
      "source": [
        "#gureBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvKEb2yhhFc-",
        "colab_type": "code",
        "outputId": "65010b7b-1c18-4076-fed9-40280e046849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# gureBERT\n",
        "\n",
        "!git clone --recursive  https://github.com/zmwebdev/gureBERT\n",
        "%cd gureBERT\n",
        "\n",
        "!pip install sentencepiece\n",
        "!install -d spModels\n",
        "# eu\n",
        "!python src/sentence-split.py --config eu.config.ini --do_lower_case \n",
        "!python src/train-sentencepiece.py --config eu.config.ini\n",
        "\n",
        "!head -n 100 spModels/eu.vocab\n",
        "\n",
        "# en-eu\n",
        "!python src/sentence-split.py --config en-eu.config.ini --do_lower_case \n",
        "!python src/train-sentencepiece.py --config en-eu.config.ini\n",
        "\n",
        "!head -n 100 spModels/en-eu.vocab\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk8_HLFhpLYA",
        "colab_type": "code",
        "outputId": "cad6905b-44c9-4a6e-f5da-170a1933131e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "import tokenization_sentencepiece as tokenization\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    model_file=\"spModels/eu.model\",\n",
        "    vocab_file=\"spModels/eu.vocab\",\n",
        "    do_lower_case=True)\n",
        "\n",
        "text1 = \"Nere kotxea aitonaren etxe alboan dago, bere kolorea gorria da.\"\n",
        "\n",
        "tokenizer.tokenize(text1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded a trained SentencePiece model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁ne',\n",
              " 're',\n",
              " '▁',\n",
              " 'ko',\n",
              " 'txea',\n",
              " '▁ai',\n",
              " 'ton',\n",
              " 'aren',\n",
              " '▁etxe',\n",
              " '▁alb',\n",
              " 'o',\n",
              " 'an',\n",
              " '▁dago',\n",
              " ',',\n",
              " '▁bere',\n",
              " '▁kolore',\n",
              " 'a',\n",
              " '▁gorria',\n",
              " '▁da',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9BPqkOUMHiz",
        "colab_type": "code",
        "outputId": "544d1afd-af8b-491f-d0e8-e47fd485adf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "tokenizer_en_eu = tokenization.FullTokenizer(\n",
        "    model_file=\"spModels/en-eu.model\",\n",
        "    vocab_file=\"spModels/en-eu.vocab\",\n",
        "    do_lower_case=True)\n",
        "\n",
        "text1 = \"Nere kotxea aitonaren etxe alboan dago, bere kolorea gorria da.\"\n",
        "\n",
        "tokenizer_en_eu.tokenize(text1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded a trained SentencePiece model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁ne',\n",
              " 're',\n",
              " '▁ko',\n",
              " 'txea',\n",
              " '▁a',\n",
              " 'it',\n",
              " 'on',\n",
              " 'aren',\n",
              " '▁etxe',\n",
              " '▁',\n",
              " 'albo',\n",
              " 'an',\n",
              " '▁dago',\n",
              " ',',\n",
              " '▁bere',\n",
              " '▁kolore',\n",
              " 'a',\n",
              " '▁gorria',\n",
              " '▁da',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojp8yYLUMUzt",
        "colab_type": "code",
        "outputId": "d64d8867-8b65-40fd-9d61-d59002940170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "text1 = \"The Italian cities of Milan and Cortina d'Ampezzo are chosen as the joint hosts of the 2026 Winter Olympics and Winter Paralympics.\"\n",
        "\n",
        "print(\"EN-EU: {}\".format(len(tokenizer_en_eu.tokenize(text1))))\n",
        "print(\"EU: {}\".format(len(tokenizer.tokenize(text1))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EN-EU: 31\n",
            "EU: 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLvpxWmnofck",
        "colab_type": "code",
        "outputId": "f7037dd7-d8aa-4eb1-f605-f1ff7b6011c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0630 08:14:24.231455 139752177215360 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNdhng5UqP6R",
        "colab_type": "code",
        "outputId": "4251a723-7e90-4fb6-d15b-82c970aeffbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# Pre-Training\n",
        "# https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/pretraining.ipynb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "#Check TPU devices\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.30.16.66:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 17613849957824031905),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6524659045245914494),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8070855191348950394),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12268692455479306683),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 8908837453655379245),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16055691405506848287),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 4099686479529614108),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 10999481038331115317),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2154121044158495901),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8819380928745696767),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 18162153919962359497)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c76u8zhRog86",
        "colab_type": "code",
        "outputId": "373cc1fd-6dd5-467a-8c7c-b3ab31e88e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "GS = 'gs://gurebert/gureBERT'\n",
        "  \n",
        "#!gsutil cp -r spModels $GS/\n",
        "#!gsutil cp -r corpus $GS/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://spModels/en-eu.vocab [Content-Type=application/octet-stream]...\n",
            "Copying file://spModels/eu.model [Content-Type=application/octet-stream]...\n",
            "Copying file://spModels/en-eu.model [Content-Type=application/octet-stream]...\n",
            "Copying file://spModels/eu.vocab [Content-Type=application/octet-stream]...\n",
            "\\ [4 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 4 objects/2.5 MiB.                                      \n",
            "Copying file://corpus/eu/2014wiki.eu.sent_splited [Content-Type=application/octet-stream]...\n",
            "Copying file://corpus/eu/2014wiki.eu.sent_splited.sent_splited [Content-Type=application/octet-stream]...\n",
            "Copying file://corpus/eu/2014wiki.eu.sent_splited.sent_splited.sent_splited [Content-Type=application/octet-stream]...\n",
            "Copying file://corpus/eu/2014wiki.eu [Content-Type=application/octet-stream]...\n",
            "\\\n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying file://corpus/en-eu/2014wiki.eu.sent_splited [Content-Type=application/octet-stream]...\n",
            "Copying file://corpus/en-eu/2019wiki-10k.en [Content-Type=application/octet-stream]...\n",
            "Copying file://corpus/en-eu/2019wiki-10k.en.sent_splited [Content-Type=application/octet-stream]...\n",
            "Copying file://corpus/en-eu/2014wiki.eu [Content-Type=application/octet-stream]...\n",
            "- [8 files][ 11.1 MiB/ 11.1 MiB]                                                \n",
            "Operation completed over 8 objects/11.1 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFlsE3KI4Zcd",
        "colab_type": "text"
      },
      "source": [
        "## EU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlujbeOnmPC_",
        "colab_type": "code",
        "outputId": "1ae350c1-ba94-4f1f-ef3b-43560383057b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 src/create_pretraining_data.py \\\n",
        "    --input_file=$GS/corpus/eu/2014wiki.eu.sent_splited \\\n",
        "    --output_file=$GS/pretraining.tf.data \\\n",
        "    --model_file=spModels/eu.model \\\n",
        "    --vocab_file=spModels/eu.vocab \\\n",
        "    --do_lower_case=True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 14:23:14.996464 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:458: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 14:23:14.997114 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:424: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 14:23:14.997257 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:424: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "Loaded a trained SentencePiece model.\n",
            "W0629 14:23:15.023280 140356343957376 deprecation_wrapper.py:119] From /content/gureBERT/src/tokenization_sentencepiece.py:115: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 14:23:15.085362 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:432: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 14:23:16.296864 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:434: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 14:23:16.297110 140356343957376 create_pretraining_data.py:434] *** Reading from input files ***\n",
            "I0629 14:23:16.297206 140356343957376 create_pretraining_data.py:436]   gs://gurebert/gureBERT/corpus/eu/2014wiki.eu.sent_splited\n",
            "I0629 14:23:26.074661 140356343957376 create_pretraining_data.py:445] *** Writing to output files ***\n",
            "I0629 14:23:26.074900 140356343957376 create_pretraining_data.py:447]   gs://gurebert/gureBERT/pretraining.tf.data\n",
            "W0629 14:23:26.075123 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:91: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0629 14:23:26.076873 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.077040 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁helbide ▁hau [MASK] [SEP] ▁su bject [SEP]\n",
            "I0629 14:23:26.077317 140356343957376 create_pretraining_data.py:151] input_ids: 4 1270 91 6 5 398 6526 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.077567 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.077784 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.077873 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.077956 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 1622 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.078046 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.078120 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.079127 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.079411 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁norvegia ko ▁nobel ▁komite aren ▁bilera ▁gela ▁bakea ren ▁nobel ▁saria ▁( norvegieraz ▁eta ▁suedieraz : ▁) ▁alfred ▁nobel ek ▁sortu riko ▁5 ▁nobel ▁sari etako ▁bat ▁da . [MASK] ▁horretan ▁bakea ren ▁alde , ▁nazio en ▁arteko ▁elkartasuna ren ▁alde ▁eta ▁ger ren ▁aurka ▁lana ▁egin ▁duen ▁norbait i ▁ematen ▁zaio . ▁beste ▁sari ek ▁ez [MASK] ▁norvegia ko ▁parlamentu ko ▁bost ▁kide en ▁komisio ▁batek ▁erabakitze n ▁du ▁nori ▁eman . ▁saria ▁sortu ▁zen [MASK] ▁suedia ▁eta ▁norvegia ▁herrialde [MASK] ▁ziren ▁eta ▁horregatik ▁bana tzerakoan ▁norvegia ri ▁eman ▁zitzaion ▁aukeratze ko ▁eskubidea . ▁saria ▁oslo n ▁banatzen ▁da ▁abendua ren ▁10 ean , ▁eta ▁stockholmen ▁ematen ▁ez ▁den ▁nobel ▁sari ▁bakarra ▁da . ▁bakea ren ▁nobel ▁saria ren ▁domina ▁gustav ▁vi geland ▁norvegiar ▁eskultorea [MASK] ▁diseinatu ▁zuen . ▁2011 [MASK] [MASK] ▁:201 4 ▁mal ala ▁y ousa f zai ▁eta [MASK] ▁mito ▁sa ty art hi . ▁:201 3 ▁arma ▁kimikoak ▁debekatze ko ▁erakundea ▁:201 2 ▁europar ▁batasuna ▁:201 1 ▁ellen [MASK] [MASK] [MASK] , ▁ leyma h ▁g bowe e ▁eta ▁t awa kul ▁karm an ▁2001 -2010 ▁:201 0 ▁li u ▁xiao bo ▁:200 9 ▁bar ack ▁obama ▁:200 8 ▁mart ti ▁a h tisa ari ▁:200 7 ▁al ▁gore ▁eta ▁klima ▁aldaketari ▁buruzko ▁gobernu ▁arteko ▁taldea ▁:200 6 ▁muhammad ▁y unus ▁eta ▁gram e en ▁bankua . ▁:200 5 ▁energia ▁atomiko rako ▁nazioarteko ▁agentzia ▁eta ▁mo ham ed ▁elbar ade j ▁:200 4 ▁w angari ▁maathai ▁:200 3 ▁shir in ▁e badi ▁:200 2 ▁jim my ▁carter ▁:200 1 ▁nazio ▁batu en ▁erakundea , ▁k ofi ▁annan ▁1991 - 2000 ▁:200 0 ▁kim [MASK] [MASK] ▁j ung ▁:199 9 ▁muga rik ▁gabeko ▁mediku ak ▁:199 8 ▁john ▁hum e , ▁david ▁tri mble ▁:199 7 ▁pertson en ▁aurkako ▁min ak ▁galaraz teko [MASK] ▁kanpain a ▁( ic bl ), ▁jo dy ▁williams ▁:199 6 ▁carlo s ▁filip e ▁ ximenes ▁belo , ▁jo sé ▁ramos - horta ▁:199 5 ▁joseph ▁rot bla t , ▁pug wash ▁con ferences ▁on ▁sci ence ▁and ▁world ▁aff air s ▁:199 4 ▁y asser ▁arafat , ▁shi mon ▁per es , ▁y itz h ak ▁rabin ▁:19 93 ▁n elson [MASK] ▁nevill ▁frederi k ▁will em ▁de ▁k lerk [SEP] ▁jarraipen ▁zerrenda tik ▁eskatutako ak ▁ezabatzen ... [SEP]\n",
            "I0629 14:23:26.079677 140356343957376 create_pretraining_data.py:151] input_ids: 4 1718 13 115 7536 23 3089 10193 659 17 115 285 15 10790 10 5312 22 1949 1227 115 38 94 990 429 115 1075 226 36 24 7 6 593 659 17 249 8 402 21 212 8708 17 249 10 728 17 390 1105 59 130 8861 46 356 660 7 57 1075 38 32 6 1718 13 2287 13 779 1343 21 7293 136 5784 18 65 8893 135 7 285 94 20 6 2028 10 1718 351 6 44 10 3752 4362 2243 1718 79 135 908 6144 13 8781 7 285 5412 18 2147 24 1397 17 467 104 8 10 7235 356 32 117 115 1075 690 24 7 659 17 115 285 17 5935 2497 569 9958 4363 1742 6 5470 26 7 311 6 6 993 300 2067 1333 579 13474 309 3605 10 6 8277 453 4354 2033 3441 7 993 179 2178 1908 3495 13 910 993 113 369 491 993 111 3785 6 6 6 8 14 5441 33 275 11477 40 10 361 4594 5765 10415 30 443 4461 993 210 1175 45 8831 2646 321 167 883 2330 4996 321 169 6304 749 73 33 13400 149 321 237 162 13202 10 798 9540 623 737 212 6142 321 246 7202 579 14322 10 9203 40 21 6322 7 321 215 2623 4808 338 461 7814 10 684 3461 657 9052 4013 27 321 300 315 6167 4908 321 179 5690 207 84 7010 321 113 957 1033 2118 321 111 402 498 21 910 8 87 9936 5525 713 25 6370 321 210 2382 6 6 191 6037 622 167 1119 48 939 1384 19 622 169 112 2286 40 8 406 2265 12353 622 237 2371 21 268 2410 19 11988 1173 6 3764 12 15 6549 10469 105 241 1722 2007 622 246 1168 43 1497 40 14 8600 13295 8 241 1552 7824 25 11353 622 215 560 8852 4492 114 8 9604 12392 2228 7749 2081 8944 6225 1106 2110 8060 6729 43 622 300 579 2240 2932 8 3472 4682 1019 195 8 579 2224 33 19 9522 72 1583 559 3205 6 7679 12938 16 3184 1958 75 87 9736 5 811 174 55 5327 19 8071 372 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.079903 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.080115 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.080209 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 30 58 77 82 126 127 131 132 133 143 144 164 165 166 167 269 270 298 362 363\n",
            "I0629 14:23:26.080295 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 175 171 2208 489 1742 16 311 6130 1157 3530 13479 3072 25 7428 8 24 40 461 3866 8\n",
            "I0629 14:23:26.080395 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.080477 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.081332 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.081463 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁ordu [MASK] [SEP] ▁1597 [SEP]\n",
            "I0629 14:23:26.081689 140356343957376 create_pretraining_data.py:151] input_ids: 4 1017 6 5 5186 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.081910 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.082123 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.177016 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.177336 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 2803 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.177518 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.177643 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.179102 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.179255 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁2014 ). [SEP] ▁otsaila ren [MASK] [SEP]\n",
            "I0629 14:23:26.179545 140356343957376 create_pretraining_data.py:151] input_ids: 4 199 29 5 154 17 6 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.179779 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.179994 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.180085 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.180168 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 550 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.180265 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.180341 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.181242 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.181365 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁* ▁se haska ▁egutegi ko ▁izend egia : ▁ar aitz , ▁ laxari / l azaro [MASK] [MASK] [MASK] [MASK] [SEP] ▁erabiltzaile ari ▁e - posta ▁bidali [SEP]\n",
            "I0629 14:23:26.181617 140356343957376 create_pretraining_data.py:151] input_ids: 4 9 108 166 86 13 172 157 22 519 3636 8 14 7611 77 201 8763 6 6 6 6 5 403 149 84 25 875 500 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.181838 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.182051 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.182141 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 17 18 19 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.182230 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 10 3271 9314 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.182317 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.182406 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.183287 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.183591 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁longpage warn ing [SEP] ▁hiztegia ▁hiztegia ▁hizkuntza ▁asko tatik ▁dator . ▁zenbait ▁hitz [MASK] ▁europa z ▁kanpo ko ▁hizkuntzetatik [MASK] ▁dira , ▁adibidez ▁japonieratik , ▁nazioarteko ak [MASK] [MASK] ▁baina ▁ele ▁gehienak ▁hizkuntza ▁erromantze etatik ▁( gehienbat ▁latinetik ▁eta ▁frantsesetik ) ▁datoz , ▁alemanetik ▁eta ▁ingelesetik . ▁* [MASK] ▁latino etatik : ▁* * ▁latinetik : ▁a bio , ▁sed , ▁tam en , ▁okul o , ▁a kvo ▁* * ▁frantsesetik : ▁di man ĉ o , ▁fermi , ▁ ĉ e , ▁fra pi , ▁ ĉ e valo , ▁b utiko ▁* * ▁italiera tik : ▁ ĉ ielo , ▁f ari , ▁vo ĉ o ▁* * ▁portugesa tik : ▁sa ŭ dado ▁* * ▁beste : ▁faci la , ▁fer o , ▁tra , ▁verd a ▁* ▁hizkuntza ▁germaniarre tatik : ▁* * ▁alemanetik : ▁balda ŭ , ▁bed a ŭ ri , ▁ha ŭ to , ▁jaro , [MASK] ▁* * ▁ingelesetik : ▁bi rdo , ▁m itingo , ▁spit e , ▁sun o , ▁ ŝ arko , ▁te amo ▁* * ▁beste : [MASK] [MASK] [MASK] ▁fi ŝ o , ▁fr em da , ▁gr undo , ▁hal ti , ▁hast i , ▁h undo , ▁of ta , ▁s omero , ▁ ŝ ipo , ▁vi ntro ▁* ▁hizkuntza ▁eslabiarr etatik : ▁* * ▁poloniera tik : ▁cel o , ▁ ĉ u , ▁k rado , ▁ luti , ▁mo ŝ to ▁* * ▁errusiera tik : ▁bar ak ti , ▁serp o , ▁vos to ▁* * ▁beste : ▁klo po di , ▁krom , ▁pr ava ▁* ▁beste ▁indoeuropar ▁hizkuntzetatik : ▁* * ▁ grekotik : ▁hepato , ▁kaj , ▁biologi o , ▁politiko ▁* * ▁lituaniera tik : ▁du , ▁ju , ▁tu j ▁* * ▁san s k ri to tik : [MASK] [MASK] [MASK] ▁gardner [MASK] [MASK] ▁pa do ▁* ▁hizkuntza ▁fin o - ug rikoetatik : ▁* * ▁l aponieratik : ▁sam ea , ▁bo aco , ▁jo j ko ▁* * ▁finlandiera tik : ▁li rli , ▁sa ŭ no ▁* * ▁hungariera tik : ▁c inci , ▁ ĉ ako , ▁ ĉ ardo , ▁ ĉ arda ŝ o , ▁ ĉ uro ▁* ▁semit ar ▁hizkuntzetatik : ▁* * ▁hebreera tik : [MASK] [MASK] ▁* * ▁arabiera tik : ▁ka dio , ▁kaid o , ▁a ŭ ▁* ▁beste [MASK] [MASK] ▁japonieratik : ▁cun amo , ▁ha ŝ io , ▁ha j ko , ▁ uta o , ▁zen o ▁* * ▁txinera tik : ▁to ŭ fu o ▁* * ▁euskara tik : ▁e ŭ ska ▁erreferentzia k ▁kanpo ▁loturak ▁* ▁* ▁* ▁* [MASK] ▁* ▁* ▁* ▁* ▁kategoria : esperantoa [SEP]\n",
            "I0629 14:23:26.183861 140356343957376 create_pretraining_data.py:151] input_ids: 4 8661 2568 330 5 4849 4849 119 280 2090 2429 7 757 456 6 425 41 242 13 5206 6 37 8 562 7946 8 461 19 6 6 88 6148 603 119 6822 416 15 11791 5213 10 5247 28 4175 8 7709 10 8416 7 9 6 14518 416 22 9 64 5213 22 73 3431 8 10781 8 10582 21 8 8699 35 8 73 11056 9 64 5247 22 343 499 0 35 8 5033 8 14 0 40 8 2036 3389 8 14 0 40 12985 8 128 11768 9 64 2142 55 22 14 0 10735 8 313 149 8 2726 0 35 9 64 2705 55 22 453 0 12975 9 64 57 22 8480 333 8 6402 35 8 1338 8 8919 12 9 119 9412 2090 22 9 64 7709 22 10007 0 8 10704 12 0 79 8 342 0 383 8 13269 8 6 9 64 8416 22 95 6779 8 134 12640 8 9054 40 8 4087 35 8 14 0 6560 8 962 4126 9 64 57 22 6 6 6 1189 0 35 8 2682 1958 553 8 6455 8651 8 1993 749 8 2798 46 8 147 8651 8 238 101 8 137 3376 8 14 0 5877 8 569 14288 9 119 7141 416 22 9 64 6948 55 22 1978 35 8 14 0 45 8 87 9793 8 14 9582 8 684 0 383 9 64 3577 55 22 883 19 749 8 8732 35 8 6342 383 9 64 57 22 6647 1967 841 8 5978 8 1784 11222 9 57 12014 5206 22 9 64 14 5527 22 7798 8 9241 8 2709 35 8 2655 9 64 7857 55 22 65 8 2366 8 3248 27 9 64 433 43 16 79 383 55 22 6 6 6 7486 6 6 464 729 9 119 1714 35 25 11345 12634 22 9 64 331 12234 22 1778 943 8 1790 6611 8 241 27 13 9 64 7972 55 22 1175 9695 8 453 0 535 9 64 5119 55 22 209 13953 8 14 0 452 8 14 0 1461 8 14 0 13908 0 35 8 14 0 3545 9 8788 316 5206 22 9 64 7926 55 22 6 6 9 64 2489 55 22 954 2380 8 4291 35 8 73 0 9 57 6 6 7946 22 5831 4126 8 342 0 522 8 342 27 13 8 14 6011 35 8 20 35 9 64 5317 55 22 1366 0 8278 35 9 64 229 55 22 84 0 2764 542 16 242 488 9 9 9 9 6 9 9 9 9 78 22 11732 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.184086 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.184309 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.280735 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 14 20 28 29 49 155 182 183 184 307 308 309 310 311 312 383 384 400 401 446\n",
            "I0629 14:23:26.280960 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 257 125 1525 8 119 3911 14 9709 8 128 8865 8 7534 35 8 954 10883 9 64 9\n",
            "I0629 14:23:26.281090 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.281188 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.282682 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.282880 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁gertaerak ▁arte ▁eta ▁kultura ▁zientzia ▁gaztee ▁teknologia ▁kirolak [SEP] ▁sitesupport [SEP]\n",
            "I0629 14:23:26.283317 140356343957376 create_pretraining_data.py:151] input_ids: 4 68 47 10 54 50 14559 67 70 5 12146 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.283725 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.284065 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.286158 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.286304 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 68 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.286466 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.287182 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.289069 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.289453 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁urtarrila ren ▁15 a ▁gregoriotar ▁egutegi aren ▁urteko ▁hamabos garren ▁eguna ▁da . ▁35 0 ▁egun ▁falt a ▁dira ▁urtea ▁amaitzeko , ▁35 1 ▁egun ▁bis urteetan . ▁gertaerak [SEP] ▁euskal ▁herria ▁* ▁1979 ▁- [MASK] , ▁francisc o ▁mota ▁calv o ▁go ardia ▁zibila ▁erail ▁zuen ▁donostian . ▁mundua ▁* ▁2006 ▁- ▁michel le ▁bache let , ▁txile ko ▁presidente ▁aukeratu ▁zuten . ▁* ▁2009 [MASK] ▁israelek ▁hamase ko ▁barne ▁ministro ▁hiltzea z ▁gain , ▁n ben ▁gune ▁bat , ▁erie txe ▁bat ▁eta ▁pr entsa ▁egoitza ▁bat ▁bonba rdatu ▁zituen . ▁ekonomia ▁* ▁2006 ▁- ▁b b va ▁banku ak ▁bere ▁2005 . eko ▁emaitza k ▁aurkeztu ▁zituen , ▁380 6 ▁milioi ▁euro ko ▁irabaziekin . ▁arte ▁eta ▁kultura ▁* ▁1937 ▁- ▁the ▁plo ugh ▁and ▁the ▁star s ▁(\" go ldea ▁eta ▁izarrak \") ▁filma ▁estreinat u ▁zen , ▁john ▁ford ek ▁zuzendu ▁eta ▁barbar a ▁stan wy ck , ▁prest on ▁foster ▁eta ▁barry ▁fitzgerald ▁aktoree k ▁antzeztu [MASK] [MASK] . ▁* ▁1953 ▁- ▁k night s ▁of ▁the ▁ round ▁table ▁(\" mahai ▁borob ile ko ▁zaldun ak \") ▁filma ▁estreinat u ▁zen , ▁richard ▁tho rpek ▁zuzendu ▁eta ▁robert ▁taylor , ▁ava ▁gardner ▁eta [MASK] ▁ferr er ▁aktoree k ▁antzeztu ▁zuten . ▁* ▁1974 ▁- ▁ha pp y ▁da ys ▁(\" egun ▁ala iak \") ▁telesaila ▁aeb ko ▁ab c ▁telebista ▁katea n ▁aire tatzen ▁hasi ▁zen . ▁zientzia ▁eta ▁teknologia ▁* ▁2001 ▁- ▁wikipedia ren ▁lehen ▁bertsioa , ▁ingelesezkoa , [MASK] ▁jarri ▁zen . ▁kirolak ▁jaiotza k ▁euskal ▁herria ▁* ▁1931 ▁- ▁ro ger ▁idi art , ▁apaiz ▁eta ▁euskal ▁idazlea . ▁( h . ▁2009 ). ▁mundua ▁* ▁1891 ▁- ▁iv or ▁nov ello , ▁galestar ▁konposatzailea , ▁abeslaria ▁eta ▁aktorea ▁( h . ▁1951 ). ▁* ▁1965 ▁- ▁james ▁n esbit t , ▁ipar irlandar ▁aktorea . ▁heriotzak [MASK] ▁herria ▁* [MASK] ▁- ▁imanol ▁ceci aga ▁arizaga ▁euskal ▁futbolaria . ▁( j . ▁1929) . ▁mundua ▁* ▁1519 ▁- ▁vasco ▁n ú ñez ▁de ▁bal boa ▁espainiar [MASK] ▁eta ▁konkistatzailea . ▁( j . ▁1 475) . ▁* ▁1919 ▁- ▁rosa [MASK] [MASK] [MASK] ▁iraultzaile ▁eta ▁teoria lari ▁alemaniarra ▁( j . ▁1870 ). ▁* ▁1919 ▁- ▁karl ▁lieb k necht , ▁alemaniar ▁sozialista [MASK] j [MASK] ▁1871) . ▁* ▁2012 ▁- ▁ manuel ▁frag a , ▁galiziar ▁politikari ▁ultraeskuindar ▁eta ▁frankista ▁( j . ▁1912) . [MASK] ▁eta ▁urteurrena k ▁* ▁se haska ▁egutegi ko ▁izend egia : [MASK] [MASK] ▁eta ▁maur o . [SEP]\n",
            "I0629 14:23:26.289876 140356343957376 create_pretraining_data.py:151] input_ids: 4 236 17 240 12 165 86 23 107 2512 273 145 24 7 754 210 52 159 12 37 143 164 8 754 111 52 288 335 7 68 5 34 51 9 326 11 6 8 1074 35 485 9423 35 492 1018 575 473 26 1666 7 53 9 323 11 2168 282 9673 4005 8 1327 13 324 507 39 7 9 373 6 4541 8970 13 468 484 9472 41 460 8 559 10509 1736 36 8 5687 3216 36 10 1784 13371 2904 36 804 3214 61 7 614 9 323 11 128 528 2394 4340 19 49 346 7 74 745 16 918 61 8 5868 246 703 996 13 5414 7 47 10 54 9 1530 11 103 6457 6097 1106 103 5639 43 231 56 13391 10 5252 206 120 133 45 20 8 112 3343 38 129 10 3228 12 1683 12727 1730 8 6191 180 2969 10 2134 3666 319 16 146 6 6 7 9 1052 11 87 8484 43 238 103 14 13040 7028 231 4731 8227 4776 13 3834 19 206 120 133 45 20 8 693 2324 13960 129 10 197 1838 8 6014 7486 10 6 8960 139 319 16 146 39 7 9 1305 11 342 5781 153 24 10383 231 6701 1213 2049 206 1836 341 13 1076 302 1230 2538 18 4330 5809 92 20 7 50 10 67 9 443 11 291 17 69 1918 8 6523 8 6 348 20 7 70 82 16 34 51 9 689 11 633 847 4819 2033 8 1997 10 34 93 7 15 33 7 373 29 53 9 1263 11 838 901 6319 2012 8 4769 2459 8 385 10 109 15 33 7 922 29 9 760 11 258 559 11113 114 8 251 6875 109 7 66 6 51 9 6 11 2100 3803 1691 7532 34 399 7 15 27 7 3311 7 53 9 8121 11 5071 559 0 8118 75 1676 13828 395 6 10 3341 7 15 27 7 214 6375 7 9 723 11 3088 6 6 6 1856 10 368 1298 869 15 27 7 4064 29 9 723 11 929 7662 16 7836 8 576 1307 6 27 6 9464 7 9 308 11 14 554 6481 12 8 3654 265 4941 10 5938 15 27 7 2050 7 6 10 163 16 9 108 166 86 13 172 157 22 6 6 10 8386 35 7 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.290147 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.290369 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.290493 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 36 37 67 163 164 165 201 248 308 311 337 351 352 353 374 375 376 397 409 410\n",
            "I0629 14:23:26.290646 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 441 8 11 322 48 7 2184 3902 34 332 1390 7862 7809 8 15 27 7 168 596 1854\n",
            "I0629 14:23:26.290795 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.290894 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 14:23:26.291864 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.291985 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁ezb . [SEP] [MASK] [SEP]\n",
            "I0629 14:23:26.292261 140356343957376 create_pretraining_data.py:151] input_ids: 4 5740 7 5 6 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.292608 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.292955 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.293084 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.293188 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 7971 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.293298 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.293406 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.294872 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.295220 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁martxoa ren [MASK] [MASK] ▁gregoriotar ▁egutegi aren ▁urteko ▁hirurogei garren ▁eguna ▁da , ▁61 . a ▁bis urteetan . ▁305 ▁egun ▁falt a ▁dira ▁urtea ▁amaitzeko . ▁gertaerak ▁euskal ▁herria ▁* ▁1976 ▁- [MASK] ▁e milio ▁g ezala ▁aran buru ▁autobus etako ▁in spe ktorea ▁hil [MASK] ▁lezo n . ▁* ▁2005 ▁- ▁2 ▁hildako ▁bilbao ▁ex hibit ion ▁centre en ▁egon dako ▁lan ▁istripuan . ▁* ▁2009 ▁- ▁eus ko ▁legebiltzarre rako ▁hauteskundeak . ▁eajk ▁irabazi ▁zituen ▁30 ▁es er leku rekin , ▁baina ▁ez ▁zion ▁lehendakaritza ▁ziur tatu ▁ibarretxe ri . ▁* ▁2011 ▁- ▁bilbo ▁eta ▁galdakao n ▁eta rekin ▁zerikusi a ▁zutelakoan ▁hainbat ▁pertsona ▁atxilotu ▁zituzten . ▁mundua ▁* ▁1936 ▁- ▁ ll uí s ▁company s ▁bartzelona ra ▁itzuli ▁zen [MASK] [MASK] ▁kargu ▁egiteko . ▁* ▁1958 ▁- ▁ful ge ncio ▁batista k ▁u ko ▁egin ▁zion ▁kuba n ▁batasun ▁nazionale ko ▁gobernu ▁bat ▁osatzeko ▁apezpiku tza ▁katoliko ak ▁egin ▁zion ▁de iari . ▁* ▁1992 ▁- ▁bosnia - her zego vin ak ▁independentzia ▁lortu ▁zuen . ▁* ▁2011 ▁- [MASK] [MASK] [MASK] ▁b h atti ▁gutxiengo en ▁ministro ▁pakistandar ra ▁erail ▁zuten . ▁arte ▁eta ▁kultura ▁* ▁1991 ▁- ▁the ▁door s ▁filma ▁estreinat u ▁zen , ▁olive r ▁stone k [MASK] ▁eta ▁val ▁kil mer , ▁me g ▁ryan , ▁k yle ▁macl ach lan , ▁frank ▁wha ley , ▁k evin ▁d illon , ▁kath leen ▁quin lan , ▁bi lly ▁id ol ▁eta ▁josh ▁ev ans ▁aktoree k [MASK] ▁zuten . ▁zientzia ▁eta [MASK] ▁kirolak ▁jaiotza k ▁euskal ▁herria ▁* ▁1884 ▁- ▁angel [MASK] [MASK] ▁ole aga ▁txistularia . ▁( h . ▁1972) ▁* ▁1899 ▁- ▁aingeru ▁irigara y ▁euskal ▁idazle . [MASK] [MASK] [MASK] [MASK] ▁* ▁1915 ▁- ▁anjel ▁go enaga ▁euskal ▁idazlea . ▁( h . ▁1974) ▁* ▁1943 ▁- ▁jose ▁angel ▁iri bar ▁euskal ▁futbolari ▁ohia ▁eta ▁euskal ▁futbol ▁selekzio aren ▁entrenatzailea . ▁* ▁1978 [MASK] ▁mari sol ▁gil ▁zinemagilea . [SEP] ▁datu - basea ▁blokeatu ▁egin ▁da [SEP]\n",
            "I0629 14:23:26.297648 140356343957376 create_pretraining_data.py:151] input_ids: 4 510 17 6 6 165 86 23 107 2982 273 145 24 8 8426 7 12 288 335 7 8730 52 159 12 37 143 164 7 68 34 51 9 508 11 6 84 2787 275 11081 9085 2378 7656 226 497 6328 5486 76 6 9269 18 7 9 346 11 418 140 1346 1961 5058 1206 7386 21 272 945 178 4061 7 9 373 11 2750 13 9050 338 595 7 6201 250 61 426 1161 139 6785 161 8 88 32 243 3776 2651 2624 2958 79 7 9 311 11 959 10 7372 18 10 161 1470 12 2995 305 263 591 118 7 53 9 1135 11 14 3435 13438 43 5005 43 2907 71 897 20 6 6 1810 472 7 9 1164 11 5790 1612 6315 5992 16 1480 13 59 243 790 18 676 1805 13 737 36 4004 2844 619 1314 19 59 243 75 10325 7 9 917 11 10099 25 1793 13238 3436 19 347 440 26 7 9 311 11 6 6 6 128 33 6289 5055 21 484 5643 71 473 39 7 47 10 54 9 713 11 103 8185 43 120 133 45 20 8 3468 89 5676 16 6 10 3416 13868 1406 8 1108 299 3843 8 87 4280 8762 6071 1956 8 874 8951 768 8 87 3277 194 5325 8 5699 9597 4376 1956 8 95 1759 4697 2201 10 5521 6384 4688 319 16 6 39 7 50 10 6 70 82 16 34 51 9 2292 11 1376 6 6 8937 1691 7245 7 15 33 7 4479 9 2494 11 7470 8254 153 34 85 7 6 6 6 6 9 1734 11 7687 492 13453 34 93 7 15 33 7 10264 9 704 11 192 1376 6110 1428 34 725 777 10 34 476 4287 23 2533 7 9 781 6 829 14100 2253 3093 7 5 336 25 705 948 59 24 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.297954 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.298188 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.298286 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 3 4 34 47 125 126 175 176 177 207 223 247 252 262 263 281 282 283 284 317\n",
            "I0629 14:23:26.298373 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 214 12 441 26 7459 23 3304 528 12830 129 874 146 67 11429 2612 15 33 7 8499 11\n",
            "I0629 14:23:26.298478 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.298552 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.299551 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.299873 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁otsaila ren ▁25 a ▁gregoriotar ▁egutegi aren ▁urteko ▁berrogei ta ▁hamasei ren ▁eguna ▁da . ▁30 9 ▁egun ▁falt a ▁dira ▁urtea ▁amaitzeko , ▁31 0 ▁egun ▁bis urteetan . ▁gertaerak ▁euskal ▁herria ▁* ▁11 19 ▁- ▁tutera ko ▁errendit ze ▁agiria . ▁ordura ▁arte ▁musulmana ▁zen ▁hiria , ▁nafarroako ▁erregea ren ▁esku tan ▁ger atu ▁zen . ▁* ▁1984 ▁- ▁franko ti ratzaile ▁batek ▁eugenio ▁gut i é rrez ▁salazar ▁\" ti gre \" ▁etakidea ▁hil ▁zuen ▁ida uze - mendi n . ▁* ▁2005 ▁- ▁ger n ikako ▁arbola ▁berria ▁landatu ▁zuten ▁ger n ikako ▁batzar ▁etxean , ▁aurrekoa ▁2004 ean ▁i hartu ▁baitzen . ▁mundua ▁* ▁1570 ▁- ▁pio ▁v . a ▁aita ▁santua k ▁elisabet ▁i . a ▁es ko m ikatu ▁zuen . ▁* ▁1921 ▁- [MASK] ▁georgia ▁okupatu ▁zuen . ▁* ▁1932 ▁- ▁adolf ▁hitler ▁hiritar ▁alemaniar ▁bihurtu ▁zen , ▁natura lizazio ▁bidez . ▁* ▁1945 ▁- ▁turkiak ▁gerra ▁dekl aratu [MASK] ▁alemania ri , ▁bigarren ▁mundu ▁gerra ren ▁baitan . ▁* ▁1947 ▁- ▁prusia ko ▁estatua ▁desegin ▁zen . ▁* ▁1948 [MASK] ▁txekoslovakia ko ▁gobernua ▁txekoslovakia ko ▁alderdi ▁komunistaren ▁menpe ▁gelditu ▁zen . ▁* ▁1954 ▁- ▁ga mal ▁ab del ▁n asser ▁egipto ko [MASK] ▁bihurtu ▁zen . ▁* ▁1956 ▁- ▁adierazpen [MASK] ▁buruzagi ▁nik ita ▁khr us txe ve k ▁i os if ▁stalin en ▁pertsona reki ko ▁gur tza ▁kritika tu ▁zuen , ▁eta ▁des esta lin izazioa ▁abiaraz i ▁zuen . ▁* ▁1983 ▁- ▁bale ar ▁uharteeta ko ▁autonomi a ▁estatu tua ▁indarre an ▁sartu ▁zen . ▁* ▁2011 ▁- ▁irlanda ko ▁errepublika ko ▁hauteskunde ▁orokorretan , ▁end a ▁ken ny k ▁gidatuta ko ▁fine ▁gael ▁alderdiak ▁gorakada ▁nabarmena ▁egin ▁zuen , ▁eta ▁gobernu ko ▁fi anna ▁f á il ▁alderdiak , ▁aldiz , ▁inoiz ko ▁porrot ik [MASK] ▁izan ▁zuen . ▁* ▁2012 ▁- ▁iñ aki ▁urdan garin en ▁aurkako ▁epaiketa ▁hasi ▁zen ▁mallorca n . ▁arte ▁eta ▁kultura [MASK] ▁1965 ▁- ▁lor d ▁jim ▁filma ▁estreinat u ▁zen , ▁richard ▁brooks ek ▁zuzendu ▁eta ▁peter ▁o ' to ole , ▁james ▁ma son , ▁cur d ▁j ür gens 1642) ▁eli ▁wallac h ▁aktoree k ▁antzeztu [MASK] [MASK] [MASK] ▁* ▁2004 ▁- ▁the ▁pass ion ▁of ▁the ▁christ ▁(\" kristo ren ▁pas ioa \") ▁filma ▁estreinat u ▁zen , ▁mel ▁gibson ek ▁zuzendu ▁eta ▁jim ▁cav ie zel , ▁ monica ▁bell ucci ▁eta ▁maia ▁morg en stern ▁aktoree k ▁antzeztu ▁zutela rik . ▁zientzia ▁eta ▁teknologia ▁kirolak ▁jaiotza k ▁euskal ▁herria ▁* [MASK] ▁- ▁tx ema ▁vitoria ▁txir ibi ton , ▁euskal ▁pa il azoa ▁eta ▁matematika ▁irakaslea . ▁mundua ▁* ▁ 1778 ▁- ▁jose ▁de ▁san ▁martin ▁militar ▁eta ▁independentist a ▁argentinarra . ▁( h . ▁1850 ) ▁* ▁1875 ▁- ▁enri co ▁caruso ▁tenore ▁italiarra . ▁( h . ▁1921) ▁* ▁1890 ▁- ▁vi atx eslav [MASK] ▁landaredia [MASK] [MASK] ▁sobietarra . ▁( h . ▁1986) ▁* ▁1943 ▁- ▁george ▁harrison ▁musikaria [MASK] ▁* ▁1953 ▁- ▁jose ▁maria ▁aznar [MASK] ▁espainiarra . [SEP] ▁aldaketak [SEP]\n",
            "I0629 14:23:26.300141 140356343957376 create_pretraining_data.py:151] input_ids: 4 154 17 352 12 165 86 23 107 712 101 6076 17 145 24 7 426 167 52 159 12 37 143 164 8 477 210 52 288 335 7 68 34 51 9 408 1881 11 5275 13 5730 1245 5364 7 1688 47 8433 20 639 8 581 527 17 610 170 728 176 20 7 9 1199 11 2714 749 9998 136 2478 6032 46 944 9033 7598 63 749 6988 60 2087 76 26 8803 7019 25 1610 18 7 9 346 11 728 18 4230 4365 404 10441 39 728 18 4230 3579 3131 8 9250 594 104 121 6949 2549 7 53 9 5226 11 1914 387 7 12 270 936 16 2097 121 7 12 1161 13 360 5805 26 7 9 1549 11 6 5666 2119 26 7 9 762 11 1228 4422 10222 576 306 20 8 1196 14018 493 7 9 831 11 4632 234 1999 1905 6 276 79 8 337 132 234 17 1711 7 9 842 11 2587 13 1707 1350 20 7 9 567 6 3684 13 1399 3684 13 397 3309 3469 5323 20 7 9 995 11 1202 5956 1076 1950 559 2240 866 13 6 306 20 7 9 912 11 5178 6 1942 680 1539 13185 196 3216 1576 16 121 664 6995 5070 21 263 13417 13 1994 619 6096 97 26 8 10 1830 14050 1215 9692 3992 46 26 7 9 848 11 6034 316 3170 13 2558 12 81 2210 983 30 366 20 7 9 311 11 1117 13 219 13 2080 8066 8 9475 12 3174 577 16 8733 13 10237 12739 4195 5249 1777 59 26 8 10 737 13 1189 11975 313 2401 679 4195 8 203 8 1475 13 1682 186 6 31 26 7 9 308 11 2136 2665 5426 6378 21 268 1875 92 20 7045 18 7 47 10 54 6 760 11 2387 228 957 120 133 45 20 8 693 5014 38 129 10 632 224 400 383 10074 8 258 596 548 8 2753 228 191 9549 5931 6506 1324 3838 33 319 16 146 6 6 6 9 594 11 103 9968 1206 238 103 4402 231 6932 17 3961 2766 206 120 133 45 20 8 2184 5043 38 129 10 957 9350 1400 3229 8 14 5517 6632 5934 10 4294 13281 21 9090 319 16 146 322 48 7 50 10 67 70 82 16 34 51 9 6 11 1739 4643 9017 6448 13360 968 8 34 464 679 14001 10 819 3154 7 53 9 14 3245 11 192 75 433 345 1311 10 3024 12 2154 7 15 33 7 9289 28 9 3117 11 6341 772 7425 5268 532 7 15 33 7 2018 9 2829 11 569 10627 9923 6 3827 6 6 5415 7 15 33 7 3355 9 704 11 367 3794 649 6 9 1052 11 192 417 7637 6 1042 7 5 608 5 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.300370 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.300603 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.300695 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 132 158 179 202 209 210 299 321 352 359 360 361 416 472 473 474 475 487 488 495\n",
            "I0629 14:23:26.300787 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 4430 243 11 1942 10666 13 512 9 10 322 48 7 704 684 5970 2051 265 649 7 265\n",
            "I0629 14:23:26.300872 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.300944 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.301816 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.301933 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁\" . $1\" [MASK] ▁da ▁gomenda tutako [MASK] ▁fitxategi ▁formatua . [SEP] ▁searchresults head [SEP]\n",
            "I0629 14:23:26.302165 140356343957376 create_pretraining_data.py:151] input_ids: 4 63 7 9557 6 24 9771 1907 6 937 3880 7 5 7830 11705 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.302401 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.302620 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.398494 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 4 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.398756 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 32 1453 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.399047 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.399208 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.400994 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.401296 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁gero ▁etorri ▁zen ▁h aren ▁ordez koak , ▁viena ko ▁eskolak ▁(1770-18 30) , ▁sinfonia [MASK] ▁ka mara ▁musika ren ▁hainbat ▁forma ▁bideratu ▁zituen . ▁garai ▁hartako ▁musika ▁ sorkuntza ko ▁ahalmen ▁handia ren [MASK] [MASK] ▁bikain ▁ditugu ▁ha ydn , ▁mozart [MASK] ▁beethoven . ▁erromantizismo ko ▁musika n [MASK] [MASK] ▁subjektib ismoa , ▁poesia ▁eta ▁irudi mena ▁ditugu . ▁schubert ekin ▁hasi ▁zen ▁aldi ▁hark ▁wagner ▁eta ▁lis zt ▁izan go ▁zituen ▁goren ean , ▁eta ▁korronte ▁nazionalista ▁erromantikoa k ▁iriste an ▁amaitu ▁zen . ▁xx . ▁mendean , ▁musika ren ▁joera ▁ohiko ▁arau ▁formala k ▁eta ▁konposizio aren ak ▁hauste a ▁izan ▁da , ▁at onalitatea ▁eta ▁dodekafonismo a ▁bezala ko ▁joe ren ▁bidez . ▁estatu ▁batuetan ▁jazz a ▁sortzea rekin ▁eta ▁aurrerapen ▁teknologiko ekin , ▁musika ▁konkretu a ▁eta ▁elektronikoa ▁etorri [MASK] [MASK] ▁1950 eko ▁hamarkadan ▁berrikuntz a ▁handiak ▁izan ▁ziren , ▁eta ▁disko ▁jo gailu aren ▁eta ▁irrati aren ▁zabal kundeak ▁maisu ▁zahar ▁eta ▁konpositore ▁berrien ▁obra k ▁hedatu ▁zituzten ▁mundu ▁osoan . ▁musika ren [MASK] ▁kontinentek a ▁amerika ko ▁musika ▁amerika ko ▁musikari 2 ▁lehen ▁azterketa k ▁xix . ▁mendea ren ▁bukaeran ▁hasi ▁ziren ; ▁ordura ko ▁europako ▁musika ren ▁eragin ▁handia ▁zuen [MASK] en ▁musikak . ▁dena [MASK] [MASK] ▁arkeolog ia ▁aztarn egi etan ▁aurkitu riko ▁tresnen , ▁xvi . ▁eta ▁xvii . ▁mende etako ▁misiolari en ▁kronik en ▁eta ▁batez ▁ere ▁beste ▁biztanle ekin ▁nahastu ▁ez ▁diren ▁indiar ▁talde [MASK] ▁gaur ▁egun go ▁musika ren [MASK] [MASK] ▁aztertu ▁ahal ▁izan ▁da ▁bertako ▁musika ren ▁bilakaera . ▁europarra k ▁heldu ▁aurretik ▁india rrek ▁ez ▁zuten ▁ezagutzen ▁musika ▁notazio rako ▁sistema rik , ▁musika ▁tresne k ▁lagun tzen rei ▁kantua ri , ▁eta [MASK] ▁eta ▁dantza ▁batera ▁egiten [MASK] ▁beti . ▁sort aldeko ▁musika ▁hindu en ▁musikak ▁ved a ▁testu en ▁edo ▁testu ▁sakratu en ▁irakur k etan ▁du ▁jatorria . ▁india ko ▁musikak ▁erabateko ▁aldaketa ▁izan ▁zuen ▁k . a . ▁vii . ▁mendean , ▁budismoa ren ▁eta ▁helen iar ▁kultura ren ▁eraginez ; ▁hori etatik ▁sortu ▁zen ▁indiar ▁esti lo ▁klasikoa , ▁rag a ▁izenekoa . ▁txinako ▁musika , ▁han ▁dinastia ren ▁garaian ▁( k . a . ▁20 6 ▁- ▁k . o . ▁220 ), ▁errit uzkoa ▁zen ; ▁laur ogeita ▁hamar ▁musikari z ▁osatutako ▁orkestr ek ▁jo tzen ▁zuten . [SEP] ▁orri ▁hone n ▁eztabaida ▁orria ▁ere ▁mugitu ▁da . [SEP]\n",
            "I0629 14:23:26.401612 140356343957376 create_pretraining_data.py:151] input_ids: 4 181 896 20 147 23 1150 628 8 8953 13 13869 7058 6398 8 7513 6 954 2758 158 17 305 1171 5429 61 7 339 2170 158 14 8337 13 4762 260 17 6 6 5090 1009 342 8582 8 3725 6 3665 7 6053 13 158 18 6 6 11542 1043 8 2519 10 1453 3276 1009 7 4919 193 92 20 1212 3329 3713 10 5914 8547 31 56 61 8894 104 8 10 7182 5130 8137 16 6094 30 667 20 7 877 7 325 8 158 17 1648 2344 1421 5590 16 10 4322 23 19 10772 12 31 24 8 999 12897 10 4884 12 171 13 2388 17 493 7 81 1649 4055 12 938 161 10 3039 8825 193 8 158 3018 12 10 1339 896 6 6 644 74 2537 4011 12 388 31 44 8 10 8847 241 3179 23 10 2315 23 932 4862 1732 2864 10 2446 3404 1495 16 1463 118 132 934 7 158 17 6 9219 12 262 13 158 262 13 845 113 69 3069 16 706 7 411 17 2544 92 44 58 1688 13 235 158 17 301 260 26 6 21 2772 7 482 6 6 7011 182 4617 2052 83 223 990 8509 8 1367 7 10 976 7 312 226 2116 21 6427 21 10 409 42 57 213 193 8939 32 152 3210 233 6 253 52 56 158 17 6 6 1930 787 31 24 980 158 17 2452 7 4833 16 1118 1646 370 2678 32 39 969 158 7823 338 423 48 8 158 4727 16 503 100 10782 4332 79 8 10 6 10 6566 296 222 6 1459 7 4567 3260 158 6116 21 2772 9159 12 1090 21 62 1090 7765 21 10858 16 83 65 1528 7 370 13 2772 3457 389 31 26 87 7 12 7 821 7 325 8 3717 17 10 4520 1437 54 17 1247 58 124 416 94 20 3210 5743 739 1744 8 5849 12 3171 7 1395 158 8 1082 3534 17 2208 15 16 7 12 7 462 246 11 87 7 35 7 13456 105 10536 13271 20 58 9166 11049 1062 845 41 1887 10738 38 241 100 39 7 5 442 1087 18 375 914 42 2487 24 7 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.401857 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.402079 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.402172 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 16 35 36 43 50 51 135 136 170 179 199 200 204 205 237 243 244 274 279 284\n",
            "I0629 14:23:26.402266 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 10 7004 6453 10 7659 11343 44 7 142 623 980 21 117 8 1520 493 8 533 158 44\n",
            "I0629 14:23:26.402358 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.402449 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.403336 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.403466 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁* [MASK] [MASK] [SEP] ▁kanpo ▁loturak ▁* ▁erreferentzia k [SEP]\n",
            "I0629 14:23:26.403706 140356343957376 create_pretraining_data.py:151] input_ids: 4 9 6 6 5 242 488 9 542 16 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.403926 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.404144 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.404249 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.404333 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 115 285 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.404442 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.404522 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 14:23:26.405406 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.405529 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁155 1 [SEP] [MASK] [SEP]\n",
            "I0629 14:23:26.405759 140356343957376 create_pretraining_data.py:151] input_ids: 4 4854 111 5 6 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.405980 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.406194 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.406291 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.503809 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 1641 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.504071 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.504188 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.505302 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.505477 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁links here [SEP] ▁next n [SEP]\n",
            "I0629 14:23:26.505749 140356343957376 create_pretraining_data.py:151] input_ids: 4 9655 5629 5 8064 18 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.505972 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.506184 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.506279 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.506360 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.506467 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.506541 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.507830 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.507946 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁saioa ▁hasi ▁behar ▁neurri [MASK] ▁aldatzeko . [SEP] ▁2003 [SEP]\n",
            "I0629 14:23:26.508175 140356343957376 create_pretraining_data.py:151] input_ids: 4 525 92 102 2649 6 3071 7 5 1044 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.508431 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.508660 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.508751 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.508836 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 650 2098 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.508923 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.508998 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.509860 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.509972 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁orrialde ▁bisitatu en ak [SEP] [MASK] [MASK] ▁mendea ren ▁taula [SEP]\n",
            "I0629 14:23:26.510198 140356343957376 create_pretraining_data.py:151] input_ids: 4 185 9143 21 19 5 6 6 411 17 668 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.510438 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.510658 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.510747 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 6 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.510830 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 706 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.510918 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.510991 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.511833 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.511941 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁not logg edin [SEP] ▁sphe ading [SEP]\n",
            "I0629 14:23:26.512171 140356343957376 create_pretraining_data.py:151] input_ids: 4 11046 8038 8361 5 5944 10507 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.512411 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.512624 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.603824 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.604072 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.604237 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.604367 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.605872 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.606020 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁un watchthis page [SEP] ▁revert page [SEP]\n",
            "I0629 14:23:26.606286 140356343957376 create_pretraining_data.py:151] input_ids: 4 1210 11678 558 5 5428 558 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.606531 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.606758 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.606849 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.606933 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.607022 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.607096 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:42.175292 140356343957376 create_pretraining_data.py:156] Wrote 20651 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWLKYF-Hmkb2",
        "colab_type": "code",
        "outputId": "ab40b681-91ce-4289-dc3f-8f299e64832a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!install -d gureBERT\n",
        "\n",
        "!python src/run_pretraining.py \\\n",
        "  --config_file eu.config.ini \\\n",
        "  --input_file=$GS/pretraining.tf.data \\\n",
        "  --output_dir=$GS/eu.gureBERT \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=10000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "#  --num_train_steps=1000000 \\\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 12:19:57.805771 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 12:19:57.808184 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:498: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 12:19:57.808862 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:413: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 12:19:57.809011 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:413: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 12:19:57.809142 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 12:19:57.809830 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:420: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0629 12:19:59.198010 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:424: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 12:19:59.352377 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:426: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 12:19:59.352607 139887738599296 run_pretraining.py:426] *** Input Files ***\n",
            "I0629 12:19:59.352693 139887738599296 run_pretraining.py:428]   gs://gurebert/gureBERT/pretraining.tf.data\n",
            "W0629 12:20:00.362826 139887738599296 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 12:20:01.368843 139887738599296 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f39f0dc2c80>) includes params argument, but params are not passed to Estimator.\n",
            "I0629 12:20:01.370452 139887738599296 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/eu.gureBERT', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.10.160.194:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f39fd2432e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.10.160.194:8470', '_evaluation_master': 'grpc://10.10.160.194:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f39fd24ffd0>}\n",
            "I0629 12:20:01.370841 139887738599296 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0629 12:20:01.371613 139887738599296 run_pretraining.py:465] ***** Running training *****\n",
            "I0629 12:20:01.371721 139887738599296 run_pretraining.py:466]   Batch size = 64\n",
            "I0629 12:20:03.858192 139887738599296 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.10.160.194:8470) for TPU system metadata.\n",
            "2019-06-29 12:20:03.859898: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 12:20:03.881084 139887738599296 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0629 12:20:03.881365 139887738599296 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0629 12:20:03.881476 139887738599296 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0629 12:20:03.881558 139887738599296 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0629 12:20:03.881658 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5315554842738034793)\n",
            "I0629 12:20:03.882470 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 12012256443778481517)\n",
            "I0629 12:20:03.882557 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4561923752582128113)\n",
            "I0629 12:20:03.882643 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12806759333023036689)\n",
            "I0629 12:20:03.882716 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6527302475489399346)\n",
            "I0629 12:20:03.882784 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2817395607489107592)\n",
            "I0629 12:20:03.882852 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 4079461139510367575)\n",
            "I0629 12:20:03.882913 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 380354458380127376)\n",
            "I0629 12:20:03.882972 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6785183797655118652)\n",
            "I0629 12:20:03.883038 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12102718425508620686)\n",
            "I0629 12:20:03.883100 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5403939343175850055)\n",
            "W0629 12:20:03.889518 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0629 12:20:03.903289 139887738599296 estimator.py:1145] Calling model_fn.\n",
            "W0629 12:20:03.903932 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:343: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0629 12:20:03.910025 139887738599296 deprecation.py:323] From src/run_pretraining.py:374: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0629 12:20:03.910292 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0629 12:20:03.938652 139887738599296 deprecation.py:323] From src/run_pretraining.py:391: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0629 12:20:03.938914 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0629 12:20:03.940707 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:399: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0629 12:20:03.946725 139887738599296 deprecation.py:323] From src/run_pretraining.py:406: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0629 12:20:04.038428 139887738599296 run_pretraining.py:123] *** Features ***\n",
            "I0629 12:20:04.038700 139887738599296 run_pretraining.py:125]   name = input_ids, shape = (8, 512)\n",
            "I0629 12:20:04.038825 139887738599296 run_pretraining.py:125]   name = input_mask, shape = (8, 512)\n",
            "I0629 12:20:04.038925 139887738599296 run_pretraining.py:125]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0629 12:20:04.039020 139887738599296 run_pretraining.py:125]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0629 12:20:04.039129 139887738599296 run_pretraining.py:125]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0629 12:20:04.039216 139887738599296 run_pretraining.py:125]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0629 12:20:04.039453 139887738599296 run_pretraining.py:125]   name = segment_ids, shape = (8, 512)\n",
            "W0629 12:20:04.039717 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:172: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0629 12:20:04.042016 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:411: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0629 12:20:04.078561 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:492: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0629 12:20:04.254380 139887738599296 deprecation.py:506] From /content/gureBERT/src/../bert/modeling.py:359: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0629 12:20:04.276711 139887738599296 deprecation.py:323] From /content/gureBERT/src/../bert/modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0629 12:20:08.228111 139887738599296 run_pretraining.py:173] **** Trainable Variables ****\n",
            "I0629 12:20:08.228388 139887738599296 run_pretraining.py:179]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768)\n",
            "I0629 12:20:08.228539 139887738599296 run_pretraining.py:179]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0629 12:20:08.228651 139887738599296 run_pretraining.py:179]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0629 12:20:08.228754 139887738599296 run_pretraining.py:179]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.228858 139887738599296 run_pretraining.py:179]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.228946 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.229029 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.229107 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.229186 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.229276 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.229355 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.229429 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.229506 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.229581 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.229661 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.229735 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.229812 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.229886 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.229964 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.230037 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.230112 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.230185 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.230275 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.230348 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.230425 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.230499 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.230576 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.230655 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.230733 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.230807 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.230880 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.230955 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.231032 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.231106 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.231186 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.231273 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.231348 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.231422 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.231499 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.231573 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.231657 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.231730 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.231807 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.231881 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.231956 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.232029 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.232102 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.232176 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.232265 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.232342 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.232419 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.232493 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.232567 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.232648 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.232724 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.232797 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.232873 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.232946 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.233022 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.233094 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.233171 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.233260 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.233338 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.233413 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.233491 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.233566 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.233651 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.233726 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.233798 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.233871 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.233949 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.234022 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.234098 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.234173 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.234265 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.234342 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.234420 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.234493 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.234565 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.234647 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.234723 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.234797 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.234874 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.234949 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.235021 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.235094 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.235171 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.235257 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.235335 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.235409 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.235485 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.235560 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.235643 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.235716 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.235788 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.235861 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.235938 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.236013 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.236089 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.236163 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.236251 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.236326 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.236403 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.236477 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.236553 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.236633 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.236710 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.236783 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.236861 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.236935 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.237007 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.237080 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.237158 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.237244 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.237323 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.237398 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.237470 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.237542 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.237626 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.237699 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.237776 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.237851 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.293427 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.293770 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.293917 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.294020 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.294120 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.294220 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.294351 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.294456 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.294564 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.294680 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.294785 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.294890 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.294999 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.295101 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.295207 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.295329 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.295445 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.295546 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.295665 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.295770 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.295880 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.295981 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.296090 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.296195 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.296319 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.296425 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.296538 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.296651 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.296759 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.296862 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.296971 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.297072 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.297194 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.297319 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.297428 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.297529 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.297642 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.297747 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.297856 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.297959 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.298066 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.298168 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.298286 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.298393 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.298519 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.298631 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.298739 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.298857 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.298964 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.299066 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.299173 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.299407 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.299533 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.299648 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.299758 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.299860 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.299967 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.300068 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.300170 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.300286 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.300397 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.300500 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.300617 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.300720 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.300826 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.300930 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.301035 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.301136 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.301265 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.301373 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.301481 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.301583 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.301703 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.301804 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.301902 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.302002 139887738599296 run_pretraining.py:179]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.302110 139887738599296 run_pretraining.py:179]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.302213 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.302351 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.302456 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.302557 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.302678 139887738599296 run_pretraining.py:179]   name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0629 12:20:08.302782 139887738599296 run_pretraining.py:179]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0629 12:20:08.302889 139887738599296 run_pretraining.py:179]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0629 12:20:08.303078 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0629 12:20:08.304887 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0629 12:20:08.312771 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0629 12:20:12.988123 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0629 12:20:22.340407 139887738599296 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0629 12:20:22.723875 139887738599296 estimator.py:1147] Done calling model_fn.\n",
            "I0629 12:20:26.487624 139887738599296 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 12:20:27.913854 139887738599296 monitored_session.py:240] Graph was finalized.\n",
            "W0629 12:20:28.055055 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0629 12:20:28.229808 139887738599296 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/eu.gureBERT/model.ckpt-0\n",
            "W0629 12:20:56.797613 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0629 12:20:58.530132 139887738599296 session_manager.py:500] Running local_init_op.\n",
            "I0629 12:20:59.630610 139887738599296 session_manager.py:502] Done running local_init_op.\n",
            "I0629 12:21:11.542770 139887738599296 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://gurebert/gureBERT/eu.gureBERT/model.ckpt.\n",
            "W0629 12:21:42.218657 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0629 12:21:44.502938 139887738599296 util.py:98] Initialized dataset iterators in 1 seconds\n",
            "I0629 12:21:44.504098 139887738599296 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-06-29 12:21:44.504481: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 12:21:44.510127 139887738599296 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0629 12:21:44.512408 139887738599296 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0629 12:21:44.516939 139887738599296 tpu_estimator.py:557] Init TPU system\n",
            "I0629 12:21:56.298977 139887738599296 tpu_estimator.py:566] Initialized TPU in 11 seconds\n",
            "I0629 12:21:56.299902 139886600554240 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 12:21:56.300327 139886574077696 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 12:21:57.523996 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:21:57.525045 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:22:33.871097 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 12:23:33.945027 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (0, 237)\n",
            "I0629 12:24:34.020742 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (0, 474)\n",
            "I0629 12:25:34.096276 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (0, 711)\n",
            "I0629 12:26:34.174379 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (0, 948)\n",
            "I0629 12:26:49.075427 139887738599296 basic_session_run_hooks.py:262] loss = 6.937065, step = 1000\n",
            "I0629 12:26:49.077914 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:26:49.078168 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:27:34.265166 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (1, 140)\n",
            "I0629 12:28:34.341797 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (1, 377)\n",
            "I0629 12:29:34.418697 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (1, 614)\n",
            "I0629 12:30:34.495788 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (1, 851)\n",
            "I0629 12:31:12.829446 139887738599296 basic_session_run_hooks.py:260] loss = 7.0191164, step = 2000 (263.754 sec)\n",
            "I0629 12:31:12.831134 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.79141\n",
            "I0629 12:31:12.832075 139887738599296 tpu_estimator.py:2160] examples/sec: 242.65\n",
            "I0629 12:31:13.653395 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:31:13.653923 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:31:34.502065 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (2, 77)\n",
            "I0629 12:32:34.580137 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (2, 314)\n",
            "I0629 12:33:34.656750 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (2, 551)\n",
            "I0629 12:34:34.734082 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (2, 788)\n",
            "I0629 12:35:28.993658 139887738599296 basic_session_run_hooks.py:260] loss = 6.616879, step = 3000 (256.164 sec)\n",
            "I0629 12:35:28.994962 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.90375\n",
            "I0629 12:35:28.995130 139887738599296 tpu_estimator.py:2160] examples/sec: 249.84\n",
            "I0629 12:35:30.022647 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:35:30.023137 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:35:34.954321 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (3, 14)\n",
            "I0629 12:36:35.031526 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (3, 251)\n",
            "I0629 12:37:35.106886 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (3, 488)\n",
            "I0629 12:38:35.182509 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (3, 725)\n",
            "I0629 12:39:35.258756 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (3, 962)\n",
            "I0629 12:39:46.361016 139887738599296 basic_session_run_hooks.py:260] loss = 5.333578, step = 4000 (257.367 sec)\n",
            "I0629 12:39:46.362700 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.88549\n",
            "I0629 12:39:46.363071 139887738599296 tpu_estimator.py:2160] examples/sec: 248.671\n",
            "I0629 12:39:46.364535 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:39:46.364724 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:40:35.332487 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (4, 188)\n",
            "I0629 12:41:35.412887 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (4, 425)\n",
            "I0629 12:42:35.493352 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (4, 662)\n",
            "I0629 12:43:35.574719 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (4, 899)\n",
            "I0629 12:44:01.747989 139887738599296 basic_session_run_hooks.py:260] loss = 3.9896374, step = 5000 (255.387 sec)\n",
            "I0629 12:44:01.749690 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.91563\n",
            "I0629 12:44:01.749899 139887738599296 tpu_estimator.py:2160] examples/sec: 250.6\n",
            "I0629 12:44:02.695559 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:44:02.696082 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:44:35.732580 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (5, 125)\n",
            "I0629 12:45:35.810064 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (5, 362)\n",
            "I0629 12:46:35.888416 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (5, 599)\n",
            "I0629 12:47:35.966619 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (5, 836)\n",
            "I0629 12:48:18.042087 139887738599296 basic_session_run_hooks.py:260] loss = 1.4737389, step = 6000 (256.294 sec)\n",
            "I0629 12:48:18.043930 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.90177\n",
            "I0629 12:48:18.044143 139887738599296 tpu_estimator.py:2160] examples/sec: 249.713\n",
            "I0629 12:48:18.903822 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:48:18.904337 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:48:36.199195 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (6, 63)\n",
            "I0629 12:49:36.276869 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (6, 300)\n",
            "I0629 12:50:36.352848 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (6, 537)\n",
            "I0629 12:51:36.429468 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (6, 774)\n",
            "I0629 12:52:35.242589 139887738599296 basic_session_run_hooks.py:260] loss = 3.5686815, step = 7000 (257.200 sec)\n",
            "I0629 12:52:35.244260 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.88802\n",
            "I0629 12:52:35.244764 139887738599296 tpu_estimator.py:2160] examples/sec: 248.833\n",
            "I0629 12:52:35.245944 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:52:35.246130 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:52:36.624135 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (7, 0)\n",
            "I0629 12:53:36.701828 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (7, 237)\n",
            "I0629 12:54:36.778469 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (7, 474)\n",
            "I0629 12:55:36.854178 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (7, 711)\n",
            "I0629 12:56:36.932058 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (7, 948)\n",
            "I0629 12:56:50.684499 139887738599296 basic_session_run_hooks.py:260] loss = 0.7635531, step = 8000 (255.442 sec)\n",
            "I0629 12:56:50.686144 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.91478\n",
            "I0629 12:56:50.686382 139887738599296 tpu_estimator.py:2160] examples/sec: 250.546\n",
            "I0629 12:56:51.666585 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:56:51.667099 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:57:37.106355 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (8, 174)\n",
            "I0629 12:58:37.182964 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (8, 411)\n",
            "I0629 12:59:37.259992 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (8, 648)\n",
            "I0629 13:00:37.340672 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (8, 885)\n",
            "I0629 13:01:07.060887 139887738599296 basic_session_run_hooks.py:260] loss = 1.0859506, step = 9000 (256.376 sec)\n",
            "I0629 13:01:07.062442 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.90052\n",
            "I0629 13:01:07.062622 139887738599296 tpu_estimator.py:2160] examples/sec: 249.633\n",
            "I0629 13:01:08.013406 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 13:01:08.013932 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 13:01:37.543004 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (9, 111)\n",
            "I0629 13:02:37.618957 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (9, 348)\n",
            "I0629 13:03:37.695111 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (9, 585)\n",
            "I0629 13:04:37.771129 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (9, 822)\n",
            "I0629 13:05:23.505219 139887738599296 basic_session_run_hooks.py:606] Saving checkpoints for 10000 into gs://gurebert/gureBERT/eu.gureBERT/model.ckpt.\n",
            "I0629 13:05:51.317413 139887738599296 basic_session_run_hooks.py:260] loss = 0.3301283, step = 10000 (284.257 sec)\n",
            "I0629 13:05:51.318880 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.51795\n",
            "I0629 13:05:51.319253 139887738599296 tpu_estimator.py:2160] examples/sec: 225.149\n",
            "I0629 13:05:52.334653 139887738599296 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 13:05:52.334912 139887738599296 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 13:05:52.335096 139886600554240 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 13:05:52.335194 139886600554240 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 13:05:52.335385 139887738599296 error_handling.py:96] infeed marked as finished\n",
            "I0629 13:05:52.335493 139887738599296 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 13:05:52.335581 139887738599296 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 13:05:52.335736 139886574077696 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 13:05:52.335826 139886574077696 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 13:05:52.336000 139887738599296 error_handling.py:96] outfeed marked as finished\n",
            "I0629 13:05:52.336095 139887738599296 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 13:05:54.203137 139887738599296 estimator.py:368] Loss for final step: 0.3301283.\n",
            "I0629 13:05:54.204193 139887738599296 error_handling.py:96] training_loop marked as finished\n",
            "I0629 13:05:54.204368 139887738599296 run_pretraining.py:475] ***** Running evaluation *****\n",
            "I0629 13:05:54.204454 139887738599296 run_pretraining.py:476]   Batch size = 8\n",
            "I0629 13:05:54.992729 139887738599296 estimator.py:1145] Calling model_fn.\n",
            "I0629 13:05:55.086533 139887738599296 run_pretraining.py:123] *** Features ***\n",
            "I0629 13:05:55.086806 139887738599296 run_pretraining.py:125]   name = input_ids, shape = (1, 512)\n",
            "I0629 13:05:55.086912 139887738599296 run_pretraining.py:125]   name = input_mask, shape = (1, 512)\n",
            "I0629 13:05:55.086997 139887738599296 run_pretraining.py:125]   name = masked_lm_ids, shape = (1, 20)\n",
            "I0629 13:05:55.087078 139887738599296 run_pretraining.py:125]   name = masked_lm_positions, shape = (1, 20)\n",
            "I0629 13:05:55.087162 139887738599296 run_pretraining.py:125]   name = masked_lm_weights, shape = (1, 20)\n",
            "I0629 13:05:55.087257 139887738599296 run_pretraining.py:125]   name = next_sentence_labels, shape = (1, 1)\n",
            "I0629 13:05:55.087345 139887738599296 run_pretraining.py:125]   name = segment_ids, shape = (1, 512)\n",
            "I0629 13:05:58.685091 139887738599296 run_pretraining.py:173] **** Trainable Variables ****\n",
            "I0629 13:05:58.685368 139887738599296 run_pretraining.py:179]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768)\n",
            "I0629 13:05:58.685495 139887738599296 run_pretraining.py:179]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0629 13:05:58.685587 139887738599296 run_pretraining.py:179]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0629 13:05:58.685676 139887738599296 run_pretraining.py:179]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.685760 139887738599296 run_pretraining.py:179]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.685857 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.685945 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.686023 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.686101 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.686184 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.686277 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.686352 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.686429 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.686502 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.686575 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.686647 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.686725 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.686799 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.686875 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.686948 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.687021 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.687093 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.687175 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.687262 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.687341 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.687414 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.687490 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.687563 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.687639 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.687714 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.687787 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.687859 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.687935 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.688009 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.688085 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.688166 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.688254 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.688329 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.688407 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.688481 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.688558 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.688633 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.688719 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.688802 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.688884 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.688958 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.689031 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.689105 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.689188 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.689276 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.689356 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.689429 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.689501 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.689575 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.689652 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.689726 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.689803 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.689877 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.689954 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.690029 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.690104 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.690183 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.690268 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.690341 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.690418 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.690491 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.690567 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.690640 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.690713 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.690787 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.690863 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.690936 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.691012 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.691085 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.691165 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.691251 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.691330 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.691402 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.691493 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.691568 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.691646 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.691719 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.691796 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.691870 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.691943 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.692017 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.692094 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.692174 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.692264 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.692339 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.692416 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.692488 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.692564 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.692636 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.692709 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.692781 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.692858 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.692931 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.693007 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.693081 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.693159 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.693247 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.693325 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.693397 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.693474 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.693549 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.693624 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.693699 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.693775 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.693847 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.693919 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.693991 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.694066 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.694144 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.694222 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.694311 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.694385 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.694456 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.694532 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.694605 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.694682 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.694755 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.694831 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.716998 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.717361 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.717492 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.717593 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.717690 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.717789 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.717888 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.717998 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.718101 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.718213 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.718336 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.718449 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.718555 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.718664 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.718768 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.718877 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.718980 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.719089 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.719201 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.719326 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.719455 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.719572 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.719678 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.719787 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.719892 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.719994 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.720093 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.720207 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.720335 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.720445 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.720547 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.720664 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.720773 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.720880 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.720983 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.721083 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.721196 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.721326 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.721433 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.721549 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.721667 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.721777 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.721881 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.721991 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.722093 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.722213 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.722341 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.722450 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.722549 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.722659 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.722764 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.722864 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.722963 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.723069 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.723181 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.723308 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.723413 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.723515 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.723617 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.723723 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.723824 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.723933 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.724034 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.724148 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.724271 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.724387 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.724491 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.724592 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.724695 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.724803 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.724905 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.725011 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.725116 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.725227 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.725349 139887738599296 run_pretraining.py:179]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.725459 139887738599296 run_pretraining.py:179]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.725564 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.725668 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.725769 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.725869 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.725970 139887738599296 run_pretraining.py:179]   name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0629 13:05:58.726069 139887738599296 run_pretraining.py:179]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0629 13:05:58.726184 139887738599296 run_pretraining.py:179]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0629 13:05:59.283804 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:204: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
            "\n",
            "W0629 13:05:59.300651 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:208: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "I0629 13:06:00.356783 139887738599296 estimator.py:1147] Done calling model_fn.\n",
            "I0629 13:06:00.375307 139887738599296 evaluation.py:255] Starting evaluation at 2019-06-29T13:06:00Z\n",
            "I0629 13:06:00.375581 139887738599296 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 13:06:00.901163 139887738599296 monitored_session.py:240] Graph was finalized.\n",
            "I0629 13:06:01.065716 139887738599296 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/eu.gureBERT/model.ckpt-10000\n",
            "I0629 13:06:24.608201 139887738599296 session_manager.py:500] Running local_init_op.\n",
            "I0629 13:06:24.831046 139887738599296 session_manager.py:502] Done running local_init_op.\n",
            "I0629 13:06:25.307175 139887738599296 tpu_estimator.py:557] Init TPU system\n",
            "I0629 13:06:33.942093 139887738599296 tpu_estimator.py:566] Initialized TPU in 8 seconds\n",
            "I0629 13:06:33.942871 139886756013824 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 13:06:33.943264 139886747621120 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 13:06:34.174344 139887738599296 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0629 13:06:34.364402 139887738599296 tpu_estimator.py:590] Enqueue next (100) batch(es) of data to infeed.\n",
            "I0629 13:06:34.364741 139887738599296 tpu_estimator.py:594] Dequeue next (100) batch(es) of data from outfeed.\n",
            "I0629 13:06:41.150196 139886747621120 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 13:06:43.320188 139887738599296 evaluation.py:167] Evaluation [100/100]\n",
            "I0629 13:06:43.320549 139887738599296 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 13:06:43.320647 139887738599296 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 13:06:43.320840 139886756013824 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 13:06:43.320957 139886756013824 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 13:06:43.321102 139887738599296 error_handling.py:96] infeed marked as finished\n",
            "I0629 13:06:43.321229 139887738599296 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 13:06:43.321324 139887738599296 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 13:06:44.063313 139886747621120 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 13:06:44.063584 139886747621120 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 13:06:44.063806 139887738599296 error_handling.py:96] outfeed marked as finished\n",
            "I0629 13:06:44.064006 139887738599296 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 13:06:44.664921 139887738599296 evaluation.py:275] Finished evaluation at 2019-06-29-13:06:44\n",
            "I0629 13:06:44.665283 139887738599296 estimator.py:2039] Saving dict for global step 10000: global_step = 10000, loss = 0.64757514, masked_lm_accuracy = 0.933996, masked_lm_loss = 0.41447976, next_sentence_accuracy = 1.0, next_sentence_loss = 0.0019768863\n",
            "I0629 13:06:49.262079 139887738599296 estimator.py:2099] Saving 'checkpoint_path' summary for global step 10000: gs://gurebert/gureBERT/eu.gureBERT/model.ckpt-10000\n",
            "I0629 13:06:50.198595 139887738599296 error_handling.py:96] evaluation_loop marked as finished\n",
            "I0629 13:06:50.198965 139887738599296 run_pretraining.py:489] ***** Eval results *****\n",
            "I0629 13:06:50.199102 139887738599296 run_pretraining.py:491]   global_step = 10000\n",
            "I0629 13:06:50.199516 139887738599296 run_pretraining.py:491]   loss = 0.64757514\n",
            "I0629 13:06:50.199642 139887738599296 run_pretraining.py:491]   masked_lm_accuracy = 0.933996\n",
            "I0629 13:06:50.199761 139887738599296 run_pretraining.py:491]   masked_lm_loss = 0.41447976\n",
            "I0629 13:06:50.199855 139887738599296 run_pretraining.py:491]   next_sentence_accuracy = 1.0\n",
            "I0629 13:06:50.199947 139887738599296 run_pretraining.py:491]   next_sentence_loss = 0.0019768863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAdbEYKv4P1m",
        "colab_type": "text"
      },
      "source": [
        "## EN-EU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbkJONUH77-h",
        "colab_type": "code",
        "outputId": "b3e2d71d-92aa-48e2-be2a-629f9bb896d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "FILES = \"./corpus/en-eu/2014wiki.eu.sent_splited,./corpus/en-eu/2019wiki-10k.en.sent_splited\"\n",
        "\n",
        "!python3 src/create_pretraining_data.py \\\n",
        "    --input_file={FILES} \\\n",
        "    --output_file=$GS/pretraining-en_eu.tf.data \\\n",
        "    --model_file=spModels/en-eu.model \\\n",
        "    --vocab_file=spModels/en-eu.vocab \\\n",
        "    --do_lower_case=True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 21:03:28.687825 140456185767808 deprecation_wrapper.py:119] From src/create_pretraining_data.py:458: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 21:03:28.688661 140456185767808 deprecation_wrapper.py:119] From src/create_pretraining_data.py:424: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 21:03:28.688842 140456185767808 deprecation_wrapper.py:119] From src/create_pretraining_data.py:424: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "Loaded a trained SentencePiece model.\n",
            "W0629 21:03:28.792603 140456185767808 deprecation_wrapper.py:119] From /content/gureBERT/src/tokenization_sentencepiece.py:115: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 21:03:29.007529 140456185767808 deprecation_wrapper.py:119] From src/create_pretraining_data.py:432: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 21:03:29.009263 140456185767808 deprecation_wrapper.py:119] From src/create_pretraining_data.py:434: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 21:03:29.009510 140456185767808 create_pretraining_data.py:434] *** Reading from input files ***\n",
            "I0629 21:03:29.009636 140456185767808 create_pretraining_data.py:436]   ./corpus/en-eu/2014wiki.eu.sent_splited\n",
            "I0629 21:03:29.010359 140456185767808 create_pretraining_data.py:436]   ./corpus/en-eu/2019wiki-10k.en.sent_splited\n",
            "I0629 21:04:33.381537 140456185767808 create_pretraining_data.py:445] *** Writing to output files ***\n",
            "I0629 21:04:33.381842 140456185767808 create_pretraining_data.py:447]   gs://gurebert/gureBERT/pretraining-en_eu.tf.data\n",
            "W0629 21:04:33.382053 140456185767808 deprecation_wrapper.py:119] From src/create_pretraining_data.py:91: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0629 21:04:33.383759 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.384185 140456185767808 create_pretraining_data.py:141] tokens: [CLS] [MASK] [MASK] ▁ethnic ▁fijian ▁dominat ion ▁of ▁the ▁political ▁system . ▁the ▁group ▁ against ▁racial ▁discrimination ▁( gard ) ▁was ▁formed ▁to ▁oppose ▁the ▁unilateral ly ▁imposed ▁constitution ▁and ▁to ▁restore ▁the ▁1970 ▁constitution . ▁in ▁1992 ▁ sitive ni ▁rabuka , ▁the ▁lieutenant ▁colonel ▁who ▁had ▁carried ▁out ▁the [MASK] ▁coup , ▁became ▁prime ▁minister ▁following ▁elections ▁held ▁under ▁the ▁new ▁constitution . ▁three ▁years ▁later , ▁rabuka ▁established ▁the ▁constitutional ▁review ▁commission , ▁which ▁in ▁1997 ▁wrote ▁a ▁new ▁constitution ▁which ▁was ▁supported ▁by ▁most ▁leaders ▁of ▁the ▁indigenous ▁fijian ▁and ▁indo - fijian ▁communities . ▁fiji ▁was ▁re - admitted ▁to [MASK] ▁commonwealth ▁of ▁nations . ▁the ▁year ▁2000 ▁brought ▁along ▁another ▁coup , ▁in stigated ▁by ▁george ▁speight , ▁which ▁effectively ▁topple d ▁the ▁government ▁of ▁mah endra [MASK] ▁charts ▁who ▁in ▁1997 ▁had ▁become ▁the ▁country ' s ▁first ▁indo - fijian ▁prime ▁minister ▁following ▁the ▁adoption ▁of ▁the ▁new ▁constitution . ▁commod ore ▁frank ▁bainimarama ▁assumed ▁executive ▁power ▁after ▁the ▁resignation , ▁ possibly ▁forced , ▁of ▁president ▁ ratu ▁sir ▁kami s ese ▁mara . ▁later ▁in ▁2000 , ▁fiji ▁was ▁rock ed ▁by ▁two ▁mutinie s ▁when ▁rebel ▁soldiers ▁went ▁on ▁a ▁ram page ▁at ▁suva ' s ▁queen ▁elizabeth ▁barracks . ▁the ▁high ▁court ▁ordered ▁the ▁reinstatement ▁of [MASK] [MASK] [MASK] ▁and ▁in ▁september ▁2001 , ▁to ▁restore ▁democracy , ▁a ▁general ▁election ▁was ▁held ▁which ▁was ▁won ▁by ▁interim ▁prime ▁minister ▁laise nia [MASK] [MASK] [MASK] ▁ soqo soqo ▁du avata ▁ni ▁lew en [SEP] ▁its ▁perpetrators . ▁however , [MASK] ▁military , ▁especially ▁the ▁nation ' s ▁top ▁military ▁commander , ▁frank ▁bainimarama , ▁strongly ▁oppose efa ▁this ▁bill . ▁bainimarama ▁agreed ▁with ▁detractors ▁who ▁said ▁that ▁to ▁grant ▁amnesty ▁to ▁supporters ▁of ▁the ▁present ▁government ▁who ▁had ▁played ▁a ▁role ▁in ▁the ▁violent ▁coup ▁was ▁a ▁sham . ▁his ▁attack ▁on ▁the ▁legislation , ▁which ▁continued ▁unrem ittingly ▁throughout ▁may ▁and ▁into ▁june ▁and ▁july , ▁further ▁strained ▁his ▁already ▁tense [MASK] ▁with ▁the ▁government . ▁in ▁late ▁november ▁and ▁early ▁december ▁2006 , ▁bainimarama ▁was ▁instrumental ▁in ▁the ▁2006 ▁fijian [MASK] ▁d ' état . ▁bainimarama ▁handed ▁down ▁a ▁list ▁of ▁demands ▁to ▁qarase ▁after ▁a ▁bill ▁was ▁put ▁forward ▁to ▁parliament , ▁part ▁of ▁which ▁would ▁have ▁ offered ▁pardon s ▁to ▁participants ▁in ▁the ▁2000 mur ▁attempt . ▁he ▁gave ▁qarase ▁an ▁ultimatum ▁date ▁of ▁4 ▁december ▁to ▁acce de ▁to ▁these ▁demands ▁or ▁to ▁resign ▁from ▁his ▁post . ▁qarase ▁adam antly ▁refused ▁either ▁to ▁concede ▁or ▁resign , ▁and ▁on ▁5 ▁december ▁the ▁president , ▁ ratu ▁josef a ▁iloilo , ▁was ▁said ▁to ▁have ▁signed ▁a ▁legal ▁order ▁dissolv ing ▁the ▁parliament ▁after ▁meeting ▁with ▁bainimarama . ▁in ▁april ▁2009 , ▁the ▁fiji ▁court ▁of ▁appeal ▁rule d ▁that ▁the ▁2006 ▁coup ▁had ▁been ▁illegal . ▁this ▁began ▁the ▁2009 ▁fijian ▁constitutional ▁crisis . ▁president ▁iloilo ▁abrogated ▁the ▁constitution , ▁removed ▁all ▁office ▁holders ▁under ▁the ▁constitution ▁including ▁all ▁judges ▁and ▁the ▁governor ▁of ▁the ▁central ▁bank . ▁he [MASK] ▁re [SEP]\n",
            "I0629 21:04:33.384580 140456185767808 create_pretraining_data.py:151] input_ids: 4 6 6 1538 1574 5860 579 10 7 289 115 9 7 169 16 214 3983 7543 21 24868 41 22 659 13 2916 7 9538 52 3631 1169 11 13 4600 7 689 1169 9 12 1875 16 34195 3705 27628 8 7 3684 8027 70 57 1225 144 7 6 2847 8 159 681 657 226 1583 510 124 7 84 1169 9 180 120 135 8 27628 420 7 2363 2339 1826 8 43 12 1830 360 14 84 1169 43 22 1455 27 74 2375 10 7 2183 1574 11 3427 19 5291 1403 9 277 22 270 19 18666 13 6 3617 10 910 9 7 179 834 787 350 238 2847 8 12 13599 27 753 21393 8 43 2904 15229 44 7 121 10 24582 19684 6 10620 70 12 1830 57 357 7 183 26 15 69 3427 19 5291 681 657 226 7 5678 10 7 84 1169 9 19100 7766 1232 3604 2968 1706 259 81 7 6235 8 16 2644 1397 8 10 428 16 2593 1674 19381 15 5011 9687 9 135 12 834 8 277 22 485 31 27 93 17137 15 90 10969 1970 1085 30 14 16169 2684 46 5251 26 15 1205 4764 8641 9 7 153 356 1959 7 21262 10 6 6 6 11 12 621 1051 8 13 4600 3536 8 14 293 947 22 510 43 22 574 27 5434 681 657 17970 9888 6 6 6 16 28745 28745 297 24406 5121 24877 99 5 63 18100 9 111 8 6 209 8 536 7 525 26 15 916 209 2837 8 1232 3604 8 2097 2916 2781 47 1409 9 3604 1769 29 17307 70 421 25 13 2231 10425 13 4472 10 7 524 121 70 57 611 14 384 12 7 3442 2847 22 14 19045 9 40 784 30 7 2373 8 43 613 40640 36626 701 83 11 92 772 11 960 8 439 9517 40 1287 13659 6 29 7 121 9 12 333 1179 11 148 793 686 8 3604 22 2493 12 7 686 1574 6 206 26 11382 9 3604 6430 413 14 1053 10 4902 13 7546 81 14 1409 22 894 804 13 547 8 140 10 43 103 51 16 2415 9302 15 13 2384 12 7 834 35664 1004 9 50 766 7546 37 28569 1174 10 497 793 13 34815 1011 13 87 4902 39 13 5433 33 40 453 9 7546 5864 42986 1507 466 13 13619 39 5433 8 11 30 470 793 7 428 8 16 2593 7250 42 8008 8 22 421 13 51 1919 14 895 316 16827 24 7 547 81 2075 29 3604 9 12 758 756 8 7 277 356 10 1425 495 44 25 7 686 2847 57 61 1925 9 47 282 7 756 1574 2363 1758 9 428 8008 14828 7 1169 8 1967 80 1008 15329 124 7 1169 158 80 5103 11 7 1412 10 7 517 1005 9 50 6 270 5\n",
            "I0629 21:04:33.384836 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.385074 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.385171 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 1 2 52 106 134 135 217 219 220 221 245 246 247 262 278 279 335 355 392 509\n",
            "I0629 21:04:33.385261 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 7688 5311 2421 7 27128 8 21262 7 1169 8 7546 26 15 7 2916 44 1471 2847 2847 207\n",
            "I0629 21:04:33.385357 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.385442 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 21:04:33.386520 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.386898 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁gertaerak ▁arte ▁eta ▁kultura [SEP] ▁and [MASK] [MASK] ▁the ▁resignation ▁of [MASK] ▁san ter ▁commission . ▁meanwhile , ▁the ▁parties ▁in ▁the ▁european ▁democrats ▁subgroup ▁were ▁grow ing ▁restless ▁and ▁finally ▁left ▁following ▁the ▁2009 [MASK] [MASK] ▁when ▁the ▁czech ▁civi c ▁democratic ▁party ▁and ▁british ▁conservative ▁party ▁formed ▁the ir ▁own ▁right - wing ▁european ▁conservatives ▁and ▁reformists ▁( ecr ) ▁group ▁on ▁22 ▁june ▁2009 , ▁abolish ing ▁the ▁european ▁democrats ▁subgroup ▁from ▁that ▁date . ▁the ▁epp - ed ▁group ▁reverted ▁to ▁its [MASK] ▁name ▁– ▁the [MASK] ▁group ▁– ▁immediately . ▁in ▁the ▁7 th ▁european ▁parliament ▁the ▁epp ▁group ▁remains ▁the ▁largest ▁parliament ary ▁group ▁with ▁275 ▁meps . ▁it ▁is ▁currently ▁the ▁only ▁political ▁group ▁in ▁the ▁european ▁parliament ▁to ▁ [MASK] ▁represent ▁its ▁corresponding ▁european ▁political ▁party , ▁i . e . ▁the ▁european [MASK] [MASK] s [MASK] ▁predominan ▁the ▁united ▁kingdom ▁was ▁the ▁only ▁member ▁to ▁not ▁be ▁represented ▁in ▁the ▁group ▁until ▁28 ▁february ▁2018, ▁when ▁two ▁meps ▁suspended ▁from ▁the ▁conservative ▁party ▁left ▁the ▁european ▁conservatives ▁and ▁reformists ▁and ▁joined ▁the ▁epp . ▁the ▁two ▁meps ▁later ▁joined ▁a ▁break away ▁political ▁party ▁in [MASK] ▁uk , ▁the ▁independent ▁group . ▁after ▁12 ▁member ▁parties ▁in ▁the ▁epp ▁called ▁for ▁fide s z ' s ▁exp ulsion ▁or ▁suspension , ▁fide s z ' s ▁membership ▁was ▁suspended , ▁leaving ▁the ▁epp ▁group ▁without ▁representation ▁from ▁hungary . ▁the ▁38 ▁members ▁in ▁the ▁group ▁on ▁11 ▁september ▁1952 ▁were ▁as ▁follows : ▁the ▁epp ▁group ▁is ▁govern ed ▁by ▁a ▁collective ▁( referred ▁to ▁as ▁the ▁\" presidency \") ▁that ▁allocate s ▁tasks . ▁the ▁presidency ▁consist s ▁of ▁the ▁group ▁chair ▁and ▁a ▁maximum ▁of ▁ten ▁vice - chairs , ▁including ▁the ▁treasurer . ▁the ▁day - to - day ▁running ▁of ▁the ▁epp ▁group ▁is ▁performed ▁by ▁its ▁secretariat ▁in ▁the ▁european ▁parliament , ▁led ▁by ▁its ▁secretary - general . ▁the ▁group ▁runs ▁its ▁own ▁think - tank , ▁the ▁european ▁ideas ▁network , ▁which ▁bring s [MASK] ▁opinion - formers ▁from ▁a cross ▁europe ▁to ▁discuss ▁issues ▁facing ▁the ▁european ▁union ▁from ▁a ▁centre - right ▁perspective . ▁the ▁epp ▁group ▁presidency ▁includes : ▁the ▁chair s ▁of ▁the ▁group ▁and ▁its ▁predecessors ▁from [MASK] ▁to [MASK] ▁september ▁2008 ▁are ▁as ▁follows : ▁the ▁national ▁parties ▁that ▁have ▁members ▁of ▁the ▁epp ▁group ▁are ▁as ▁follows : ▁the ▁national ▁parties ▁that ▁are ▁currently ▁suspended ▁from ▁the ▁epp ▁group ▁are ▁as ▁follows : ▁activities ▁performed ▁by ▁the ▁group ▁in ▁the ▁period ▁between ▁june ▁2004 ▁and ▁june ▁2008 ▁include ▁ monitoring ▁elections ▁in ▁palestine ▁and ▁the ▁ukraine ; ▁encourag ing ▁trans european ▁rail ▁travel , ▁telecom s ▁de regulation , ▁energy ▁security , ▁a ▁common ▁energy ▁policy , ▁the ▁accession ▁of ▁bulgaria ▁and ▁romania ▁to ▁the ▁union , ▁partial ▁reform ▁of ▁the ▁cap [MASK] ▁attempts ▁to ▁tackle ▁illegal ▁immigration ; ▁denounc ing ▁russian ▁involvement ▁in ▁south ▁ossetia ; ▁support ing ▁the ▁constitution ▁treaty ▁and ▁the ▁lisbon ▁treaty ; ▁debat ing ▁global isation , ▁relations ▁with ▁china , ▁and [SEP]\n",
            "I0629 21:04:33.387210 140456185767808 create_pretraining_data.py:151] input_ids: 4 502 358 36 406 5 11 6 6 7 6235 10 6 1180 3454 1826 9 3209 8 7 688 12 7 176 2543 8695 49 1388 24 29452 11 1537 304 226 7 756 6 6 90 7 10233 13448 161 1467 208 11 212 2065 208 659 7 78 265 512 19 3165 176 3480 11 10286 21 19397 41 169 30 1572 772 756 8 11227 24 7 176 2543 8695 33 25 1174 9 7 2979 19 31 169 10491 13 63 6 166 284 7 6 169 284 2131 9 12 7 997 104 176 547 7 2979 169 1025 7 407 547 794 169 29 21945 3550 9 35 17 1330 7 100 289 169 12 7 176 547 13 16 6 1236 63 3646 176 289 208 8 131 9 62 9 7 176 6 6 15 6 33407 7 157 532 22 7 100 446 13 54 38 1853 12 7 169 250 1958 923 3403 90 93 3550 3848 33 7 2065 208 304 7 176 3480 11 10286 11 1184 7 2979 9 7 93 3550 135 1184 14 2345 6836 289 208 12 6 1083 8 7 929 169 9 81 592 446 688 12 7 2979 152 23 16731 15 296 26 15 8359 12203 39 9330 8 16731 15 296 26 15 4263 22 3848 8 2044 7 2979 169 386 3319 33 9596 9 7 4571 310 12 7 169 30 926 621 3697 49 20 2077 45 7 2979 169 17 3004 31 27 14 2912 21 729 13 20 7 18 38077 156 25 21593 15 5078 9 7 7049 663 15 10 7 169 5784 11 14 1360 10 1273 3151 19 32541 8 158 7 17687 9 7 332 19 309 19 847 1779 10 7 2979 169 17 856 27 63 11353 12 7 176 547 8 325 27 63 2474 19 6100 9 7 169 3163 63 265 3168 19 16701 8 7 176 2464 805 8 43 1418 15 6 3182 19 40712 33 14 778 410 13 5805 1628 4660 7 176 359 33 14 942 19 3109 3578 9 7 2979 169 7049 767 45 7 5784 15 10 7 169 11 63 10662 33 6 13 6 621 545 32 20 2077 45 7 203 688 25 51 310 10 7 2979 169 32 20 2077 45 7 203 688 25 32 1330 3848 33 7 2979 169 32 20 2077 45 1030 856 27 7 169 12 7 232 101 772 1106 11 772 545 189 16 11122 1583 12 12964 11 7 8159 60 7848 24 2537 3146 1655 1171 8 12769 15 113 19968 8 634 782 8 14 247 634 843 8 7 8800 10 12546 11 10742 13 7 359 8 1882 3895 10 7 5324 6 2476 13 2733 1925 5662 60 13856 24 2157 3042 12 258 27663 60 288 24 7 1169 2046 11 7 6987 2046 60 42554 24 1328 4873 8 1436 29 371 8 11 5\n",
            "I0629 21:04:33.438923 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.439455 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.439674 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 7 8 12 36 37 87 91 127 128 142 143 144 145 146 195 341 379 381 438 476\n",
            "I0629 21:04:33.439815 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 787 109 7 1583 8 378 2979 16 1687 122 26 15 208 9 7 555 3697 573 7 11\n",
            "I0629 21:04:33.439963 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.440084 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.441469 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.441684 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁* [MASK] a [SEP] ▁the ▁bureau ▁is ▁completed ▁by ▁ten ▁vice - presidents : ▁ marta ▁ba inka ▁( siles ian ▁autonomy ▁movement ), ▁ol rik ▁bou [MASK] ▁( frisian ▁national ▁party ), ▁( future ▁of [MASK] [MASK] ▁casinos [MASK] [MASK] ▁evans [MASK] plaid ▁cymru ), [MASK] ▁fuente [MASK] [MASK] ▁( valencia n ▁nationalist ▁bloc ), ▁victor ▁gallo u ▁( breton ▁democratic ▁union ), ▁david ▁gros claude ▁( occ itan ▁party ), ▁w outer [MASK] ▁( new ▁flemish ▁alliance ), ▁ne lida ▁po gag c ic ▁( list ▁for ▁ri jek a ) ▁and ▁anne ▁tomas i ▁( party [MASK] ▁the ▁corsica n ▁nation ). [SEP]\n",
            "I0629 21:04:33.441982 140456185767808 create_pretraining_data.py:151] input_ids: 4 34 6 42 5 7 6214 17 1349 27 1273 3151 19 43504 45 16 34085 1434 35679 21 44417 1162 4257 411 76 8885 398 16255 6 21 37933 203 208 76 21 16438 10 6 6 18238 6 6 13029 6 38865 11266 76 6 28499 6 6 21 38725 55 3075 9917 76 5235 34768 225 21 25842 1467 359 76 771 43900 20163 21 42863 23500 208 76 530 23465 6 21 2203 2636 1604 76 2508 25774 2643 19506 161 278 21 4393 23 6656 23100 42 41 11 4343 11938 125 21 7133 6 7 14566 55 525 73 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.442226 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.442465 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.442576 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 2 3 27 28 37 38 39 40 41 43 44 47 49 50 75 100 0 0 0 0\n",
            "I0629 21:04:33.442667 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 32384 42 16255 1862 16 0 1158 76 19092 21 38865 4260 19369 4557 32232 10 0 0 0 0\n",
            "I0629 21:04:33.442763 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0\n",
            "I0629 21:04:33.442841 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.443893 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.444243 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁the ▁film ▁takes ▁some ▁libert ies [MASK] ▁the ▁events ▁at ▁the ▁1924 ▁olympics , ▁including ▁the ▁events ▁surrounding ▁liddell ' s ▁refusal ▁to ▁race [MASK] ▁a ▁sunday . ▁in ▁the ▁film , ▁he ▁doesn ' t ▁learn ▁that ▁the ▁100- metre ▁heat ▁is ▁to ▁be ▁held ▁on ▁the ▁christian ▁sabbath ▁until [MASK] ▁is ▁boarding ▁the ▁boat ▁to ▁paris . ▁in ▁fact , ▁the ▁schedule ▁was ▁made ▁public ▁several ▁months ▁in ▁advance ; ▁liddell ▁did ▁however ▁face ▁immense ▁pressure ▁to ▁run ▁on ▁that ▁sunday ▁and ▁to ▁compete ▁in ▁the ▁100 ▁metres , ▁gett ing ▁called ▁before ▁a ▁grill ing ▁by ▁the ▁british ▁olympic ▁committee , ▁the ▁prince ▁of ▁wales , ▁and ▁other ▁grande es , ▁and ▁his ▁refusal ▁to ▁run ▁made ▁headlines ▁around ▁the ▁world . [SEP] ▁to ▁change ▁races ▁was , ▁even ▁so , ▁made ▁well ▁before ▁embark ing ▁to ▁paris , ▁and ▁liddell ▁spent [MASK] ▁interven ing [MASK] ▁training ▁for ▁the ▁400 ▁metres , ▁an ▁event ▁in ▁which ▁he ▁had [MASK] ▁excell ed . ▁it ▁is ▁true , ▁nonetheless , ▁that ▁liddell ' s ▁success ▁in ▁the ▁olympic ▁400 m ▁was ▁large ly ▁unexpected . ▁the ▁film ▁depict s ▁lindsay , ▁hav ing [MASK] ▁won ▁a ▁medal ▁in ▁the ▁400 - metre ▁hurdles , ▁giving ▁up ▁his ▁place ▁in ▁the ▁400 - metre ▁race ▁for ▁liddell . ▁in ▁fact ▁burghley , ▁on ▁whom ▁lindsay ▁is ▁loose ly ▁based , ▁was ▁eliminated ▁in ▁the ▁heats ▁of ▁the ▁110 ▁hurdles ▁( he ▁would ▁go ▁on ▁to ▁win ▁a ▁gold ▁medal ▁in ▁the [MASK] ▁hurdles ▁at ▁the ▁1928 ▁olympics ), ▁and ▁was ▁not ▁ entered ▁for ▁the ▁400 ▁metres . ▁the ▁film ▁reverse s ▁the ▁order ▁of ▁abrahams ' ▁100 m ▁and ▁200 m ▁races ▁at ▁the ▁olympics . ▁in [MASK] [MASK] ▁after ▁winning ▁the ▁100 ▁metres ▁race , ▁abrahams ▁ran ▁the ▁200 ▁metres ▁but ▁finished ▁last , ▁jackson ▁scholz ▁ taking [MASK] ▁gold ▁medal . ▁in ▁the ▁film , ▁before ▁his ▁triumph ▁in [MASK] ▁100 m , ▁abrahams ▁is ▁shown ▁los ing ▁the ▁200 m ▁and ▁being ▁sc olded ▁by ▁mussabini . ▁and ▁dur ing ▁the ▁following ▁scene ▁in ▁which ▁abrahams ▁speak s ▁with ▁his ▁friend ▁montague ▁while ▁receiv ing ▁a ▁mass age [MASK] ▁mussabini , ▁there ▁is ▁a ▁french ▁newspaper ▁clipping ▁showing ▁scholz ▁and ▁charle y ▁paddock ▁with ▁a ▁headline ▁which ▁states ▁that ▁the ▁200 [MASK] ▁was [MASK] ▁triumph ▁for ▁the ▁united ▁states . ▁in ▁the ▁same ▁conversation , ▁abrahams ▁lament s ▁gett ing ▁\" beaten lley ▁of ▁sight \" ▁in ▁the ▁200 . ▁the ▁film ▁thus ▁has ▁abrahams [MASK] [MASK] ▁the ▁disappointment ▁of ▁los ing ▁the ▁200 ▁by ▁going ▁on ▁to ▁win ▁the ▁100 , ▁a ▁reversal ▁of ▁the ▁real ▁order . ▁eric ▁liddell ▁ actually ▁also ▁ran ▁in ▁the ▁200 m ▁race , ▁and ▁finished ▁third , ▁behind ▁paddock ▁and ▁scholz . [MASK] ▁was ▁the ▁only ▁time ▁in ▁reality ▁that ▁liddell ▁and ▁abrahams ▁competed ▁in ▁the ▁same ▁race . ▁while ▁the ir ▁meeting ▁in ▁the ▁1923 ▁aaa ▁championship ▁in ▁the ▁film ▁was ▁ficti tious , ▁liddell ' s ▁record ▁win ▁in ▁that ▁race ▁did ▁spur ▁abrahams ▁to ▁train [SEP]\n",
            "I0629 21:04:33.444577 140456185767808 create_pretraining_data.py:151] input_ids: 4 7 164 1368 71 15742 780 6 7 1048 46 7 2899 3771 8 158 7 1048 2394 2214 26 15 11582 13 2016 6 14 1869 9 12 7 164 8 50 6839 26 117 5154 25 7 24361 9194 853 17 13 38 510 30 7 461 9328 250 6 17 8347 7 6967 13 913 9 12 802 8 7 7921 22 149 256 191 823 12 4207 60 2214 274 111 1161 12931 850 13 751 30 25 1869 11 13 4852 12 7 1108 5486 8 10159 24 152 173 14 13926 24 27 7 212 3276 1513 8 7 2504 10 3428 8 11 65 9974 193 8 11 40 11582 13 751 149 18243 243 7 110 9 5 13 393 8442 22 8 216 137 8 149 143 173 12905 24 13 913 8 11 2214 1868 6 10131 24 6 864 23 7 2358 5486 8 37 953 12 43 50 57 6 33376 31 9 35 17 1702 8 5409 8 25 2214 26 15 1191 12 7 3276 2358 324 22 177 52 7469 9 7 164 5038 15 7040 8 399 24 6 574 14 3123 12 7 2358 19 9194 14884 8 2056 126 40 323 12 7 2358 19 9194 2016 23 2214 9 12 802 9506 8 30 1302 7040 17 4385 52 257 8 22 4647 12 7 18524 10 7 6766 14884 21 838 103 833 30 13 2408 14 1924 3123 12 7 6 14884 46 7 4065 3771 76 11 22 54 16 1933 23 7 2358 5486 9 7 164 4956 15 7 316 10 2229 26 1108 324 11 2178 324 8442 46 7 3771 9 12 6 6 81 3159 7 1108 5486 2016 8 2229 2668 7 2178 5486 58 2589 312 8 3338 10274 16 1720 6 1924 3123 9 12 7 164 8 173 40 9502 12 6 1108 324 8 2229 17 1022 1552 24 7 2178 324 11 138 9963 26630 27 11278 9 11 119 24 7 226 1784 12 43 2229 2255 15 29 40 2392 6596 116 3585 24 14 1356 2167 6 11278 8 85 17 14 136 1812 17418 2852 10274 11 16736 82 21197 29 14 13324 43 147 25 7 2178 6 22 6 9502 23 7 157 147 9 12 7 181 7940 8 2229 16604 15 10159 24 18 30705 43127 10 7220 28 12 7 2178 9 7 164 341 53 2229 6 6 7 21472 10 1552 24 7 2178 27 2694 30 13 2408 7 1108 8 14 22375 10 7 678 316 9 3017 2214 16 1697 56 2668 12 7 2178 324 2016 8 11 2589 455 8 1835 21197 11 10274 9 6 22 7 100 98 12 2885 25 2214 11 2229 8226 12 7 181 2016 9 116 7 78 2075 12 7 5391 9874 4219 12 7 164 22 40263 11740 8 2214 26 15 1033 2408 12 25 2016 274 8441 2229 13 4359 5\n",
            "I0629 21:04:33.444826 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.445058 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.445153 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 7 25 52 146 149 162 195 252 289 290 311 323 363 386 388 400 407 420 421 465\n",
            "I0629 21:04:33.445239 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 29 30 50 7 823 1533 1287 2358 2885 8 7 7 33 5486 14 2229 144 6046 24 47\n",
            "I0629 21:04:33.445332 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.445406 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 21:04:33.446384 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.446517 140456185767808 create_pretraining_data.py:141] tokens: [CLS] [MASK] ▁orrialdea [SEP] ▁user page [SEP]\n",
            "I0629 21:04:33.446795 140456185767808 create_pretraining_data.py:151] input_ids: 4 6 4515 5 1829 2684 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.447035 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.447264 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.541702 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.541977 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 4575 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.542121 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 21:04:33.542229 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.543891 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.544330 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁gertaerak ▁arte ▁eta ▁kultura ▁zientzia ▁eta ▁teknologia ▁* ▁urria ren ▁blending [MASK] ▁- ▁johannes ▁kepler ▁alemaniarra k ▁1604 ▁supernoba ▁begi ztatu ▁zuen , ▁esne ▁ bidean ▁ikusi ▁den ▁supernoba rik ▁berrien a . ▁kirolak ▁jaiotza k ▁heriotzak [SEP] [MASK] ▁u . s . ▁state ▁of ▁georgia . ▁with ▁an ▁estimated ▁2018 ▁population ▁of ▁498 , 0 44, ▁it ▁is ▁also ▁the ▁37 th ▁most - popul ous ▁city ▁in ▁the ▁united ▁states . ▁the ▁city ▁serve s ▁as ▁the [MASK] ▁and ▁economic ▁center ▁of ▁the ▁atlanta ▁metropolitan ▁area , ▁home ▁to ▁5.9 ▁million ▁people ▁and ▁the ▁ninth - largest ▁metropolitan ▁area ▁in ▁the ▁nation . ▁atlanta ▁is ▁the ▁seat ▁of ▁ful ton ▁county , ▁the ▁most ▁popul ous [MASK] ▁in ▁georgia . ▁a ▁small ▁portion ▁of ▁the ▁city ▁extend s ▁eastward ▁into [MASK] [MASK] ▁de kalb ▁county . ▁atlanta ▁was ▁originally ▁founded ▁as ▁the ▁terminat ing ▁stop ▁of ▁a ▁major ▁state - sponsor ed ▁railroad . ▁with ▁rapid ▁expansion , ▁however , ▁it ▁soon ▁became ▁the ▁convergence ▁point ▁between ▁multiple ▁railroads , ▁spurr ing ▁its ▁rapid ▁growth . ▁the ▁city ' s ▁name ▁derives ▁from ▁that ▁of ▁the ▁western ▁and ▁atlantic ▁railroad ' s ▁local ▁depot , ▁signify ing ▁the ▁town ' s ▁grow ing ▁reputation ▁as ▁a ▁transportation ▁hub . ▁dur ing ▁the [MASK] ▁civil ▁war , ▁the ▁city ▁was ▁almost ▁entirely ▁burned ▁to ▁the ▁ground ▁in ▁general ▁william ▁t . ▁sherman ' s ▁famous ▁march ▁to ▁the ▁sea . ▁however , ▁the ▁city ▁rose ▁from ▁its ▁as hes ▁and ▁quickly ▁became [MASK] ▁national ▁center ▁of ▁commerce ▁and ▁the ▁unofficial ▁capital ▁of ▁the ▁\" new ▁south \". ▁dur ing ▁the ▁1950 s ▁and ▁1960 s , ▁atlanta ▁became ▁a ▁major ▁organiz ing ▁center [MASK] ▁the ▁civil ▁rights ▁movement , ▁with ▁dr . ▁martin ▁luther ▁king ▁jr . , ▁ralph ▁david ▁aber n athy , ▁and ▁many ▁other ▁locals ▁playing ▁major [MASK] ▁in ▁the ▁movement ' s ▁leadership . ▁dur ing ▁the ▁modern ▁era , ▁atlanta ▁has ▁attain ed ▁international ▁prominence ▁as ▁a ▁major ▁air ▁transportation ▁hub , ▁with ▁hartsfield – jackson ▁atlanta ▁international ▁airport ▁being ▁the ▁world ' s ▁busiest ▁airport ▁by ▁passenger ▁traffic ▁since ▁1998. ▁atlanta ▁is ▁rate d ▁as [MASK] ▁\" beta ( + ) \" ▁world ▁city ▁that ▁exert s ▁a ▁moderate ▁impact ▁on ▁global ▁commerce , ▁finance , ▁research , ▁technology , [MASK] [MASK] ▁media , ▁art , ▁and ▁entertainment . ▁it ▁ranks ▁in ▁the ▁top ▁twenty ▁among ▁world ▁cities ▁and ▁10 th ▁in ▁the ▁nation ▁with ▁a ▁gross ▁domestic ▁product ▁( gdp ) ▁of ▁$3 85 ▁billion . ▁atlanta ' s ▁economy ▁is ▁considered ▁diverse , ▁with ▁dominant ▁sectors ▁that ▁include ▁transportation , ▁logistics , ▁professional ▁and ▁business ▁services , ▁media ▁operations , ▁medical ▁services , [MASK] ▁information [MASK] [MASK] ▁atlanta ▁has ▁topographic ▁features ▁that ▁include ▁roll ing ▁hills ▁and ▁dense ▁tree ▁coverage , ▁earning ▁it ▁the ▁nickname ▁of ▁\" the ▁city [MASK] ▁a ▁forest .\" ▁revitaliz ation [MASK] ▁atlanta ' s ▁neighborhoods , ▁initially ▁spurr ed ▁by ▁the ▁1996 ▁summer ▁olympics , ▁has ▁intensified ▁in ▁the ▁21 st ▁century , ▁alter ing ▁the [SEP]\n",
            "I0629 21:04:33.544724 140456185767808 create_pretraining_data.py:151] input_ids: 4 502 358 36 406 379 36 501 34 5295 79 6313 6 48 4801 5158 5747 68 28968 27560 15868 42490 130 8 34023 16 20055 3463 1178 27560 398 15404 42 9 505 651 68 499 5 6 263 9 15 9 168 10 1765 9 29 37 1461 3943 210 10 31140 8 1653 32612 35 17 56 7 6537 104 74 19 44489 874 118 12 7 157 147 9 7 118 1017 15 20 7 6 11 491 714 10 7 286 3071 234 8 327 13 37118 268 122 11 7 5448 19 4890 3071 234 12 7 525 9 286 17 7 2659 10 15279 1768 1463 8 7 74 5125 874 6 12 1765 9 14 275 3185 10 7 118 1961 15 21662 92 6 6 113 30624 1463 9 286 22 905 881 20 7 36020 24 3394 10 14 215 168 19 16569 31 5847 9 29 3036 2561 8 111 8 35 1155 159 7 10278 383 101 1265 15494 8 12202 24 63 3036 865 9 7 118 26 15 166 6748 33 25 10 7 529 11 892 5847 26 15 335 11596 8 13464 24 7 648 26 15 1388 24 3206 20 14 2001 5852 9 119 24 7 6 506 129 8 7 118 22 735 1903 8169 13 7 888 12 293 549 219 9 8051 26 15 809 677 13 7 240 9 111 8 7 118 2437 33 63 20 24635 11 2090 159 6 203 714 10 4277 11 7 9294 583 10 7 18 2203 258 89 119 24 7 719 15 11 764 15 8 286 159 14 215 8075 24 714 6 7 506 796 411 8 29 2154 9 1527 4068 245 3667 9 8 5419 771 40907 55 43022 8 11 94 65 13538 1712 215 6 12 7 411 26 15 2520 9 119 24 7 239 1032 8 286 53 15982 31 262 11338 20 14 215 230 2001 5852 8 29 14374 424 14126 286 262 1056 138 7 110 26 15 7454 1056 27 2067 1797 127 10949 286 17 712 44 20 6 18 30697 1185 0 41 28 110 118 25 8413 15 14 3733 1811 30 1328 4277 8 3476 8 377 8 830 8 6 6 868 8 487 8 11 3124 9 35 6857 12 7 916 2959 261 110 862 11 520 104 12 7 525 29 14 4804 1630 1680 21 20161 41 10 5327 7638 1133 9 286 26 15 600 17 287 3232 8 29 2526 5100 25 189 2001 8 8163 8 966 11 979 964 8 868 826 8 1647 964 8 6 676 6 6 286 53 36535 967 25 189 2509 24 4390 11 5442 3700 5674 8 6949 35 7 5414 10 18 91 118 6 14 1975 170 17216 827 6 286 26 15 4837 8 1292 12202 31 27 7 2015 1437 3771 8 53 12823 12 7 1279 402 155 8 5612 24 7 5\n",
            "I0629 21:04:33.544982 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.545213 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.545309 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 11 12 39 80 119 133 134 186 215 254 285 312 363 388 389 453 455 456 479 485\n",
            "I0629 21:04:33.545393 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 1061 42 7 854 1463 3944 24 25 235 14 10 3764 14 1029 8 11 830 9 12 10\n",
            "I0629 21:04:33.545493 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.545584 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 21:04:33.546682 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.547036 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁atlanta ▁hosted ▁the ▁atlanta ▁international ▁pop ▁festival , ▁with ▁the ▁1969 ▁festival ▁ taking ▁place ▁more ▁than ▁a [MASK] ▁before ▁woodstock ▁and ▁featur ing ▁many ▁of ▁the ▁same ▁bands . ▁the ▁city ▁was ▁also ▁a ▁center ▁for ▁southern ▁rock ▁dur ing ▁its ▁1970 s ▁he yday : ▁the ▁all man ▁brothers ▁band ' s ▁hit ▁instrumental ▁\" hot [MASK] [MASK] [MASK] ▁is ▁an ▁ode ▁to ▁the ▁city , ▁while ▁l ynyrd ▁sk ynyrd ' s ▁famous ▁live ▁rendition ▁of ▁\" free ▁bird \" ▁optimis ▁recorded ▁at ▁the ▁fox ▁theatre ▁in ▁1976 , ▁with ▁lead ▁singer ▁ro nnie ▁van ▁z ant 1846 ▁the ▁band ▁to ▁\" play ▁it ▁pretty ▁for ▁atlanta \". ▁dur ing ▁the ▁1980 s , ▁atlanta ▁had ▁an ▁active ▁punk ▁rock ▁scene [MASK] ▁on ▁two ▁of ▁the ▁city ' s ▁music ▁venues , ▁ 688 ▁club ▁and ▁the ▁metro plex , ▁and ▁atlanta ▁famously ▁played ▁host ▁to ▁the ▁sex ▁pistol s ▁first ▁u . s . ▁show , ▁which ▁was ▁performed ▁at ▁the ▁great ▁south eastern ▁music ▁hall . ▁the ▁1990 s ▁saw ▁the ▁city ▁produce ▁major ▁mainstream ▁acts ▁a cross ▁many ▁different ▁musical ▁genres . ▁country ▁music ▁artist ▁trav is ▁t ritt , ▁and ▁r & b ▁sensation s ▁t lc , ▁usher ▁and ▁toni ▁bra x ton , [MASK] ▁just [MASK] ▁of ▁the ▁musicians ▁proud ▁to ▁call ▁atlanta ▁home . ▁the ▁city [MASK] ▁gave ▁birth ▁to ▁atlanta ▁hip [MASK] [MASK] ▁a ▁subgenre ▁that ▁gained ▁relevan ce [MASK] ▁success ▁with ▁the ▁introduction ▁of ▁the ▁home - grown ▁at liens ▁known ▁as ▁out kast , ▁along ▁with ▁other ▁ dungeon ▁family ▁artists ▁such ▁as ▁organized ▁noize ▁and ▁good ie ▁mob ; ▁however , ▁it ▁was ▁not ▁until ▁the ▁2000 s ▁that [MASK] [MASK] ▁\" from [MASK] ▁margins ▁to ▁becoming ▁hip - hop ' s ▁center ▁of ▁gravity [MASK] ▁another ▁sub - genre ▁called ▁c runk , ▁part ▁of ▁a ▁larger ▁shift ▁in ▁hip - hop ▁innovation ▁to ▁the ▁south [SEP] ▁as ▁a ▁national ▁center ▁for ▁cinema ▁and ▁television ▁production , ▁atlanta ▁plays ▁a ▁significant ▁role ▁in ▁the ▁entertainment ▁industry . ▁it ▁double s ▁for ▁other ▁parts ▁of ▁the ▁world ▁and ▁fictional ▁settlements ▁in ▁blockbust er ▁productions , ▁among ▁them ▁the ▁newer ▁titles ▁from ▁\" the ▁fast ▁and ▁the ▁furious \" ▁franchise ▁and ▁marvel ▁features ▁such ▁as ▁\" ant - man \" ▁(2015) , ▁\"\" ▁(2016) , ▁\" black ▁panther \" ▁and ▁\"\" ▁( both ▁2018) . [MASK] ▁the ▁other ▁hand , ▁\" gone ▁with ▁the ▁wind \" ▁(1939) , ▁\" smoke y ▁and ▁the ▁band it \" ▁(1977) , ▁\" shar key ' s ▁machine \" ▁(1981) , ▁\" the ▁slugg er ' s ▁wife \" ▁(1985) , ▁\" driving ▁miss ▁da is y \" ▁(1989) , ▁\" atl \" ▁(2006) , ▁and ▁\" baby ▁driver \" ▁(2017) ▁are ▁among ▁several ▁notable ▁examples ▁of ▁films ▁ actually ▁set ▁in ▁atlanta . [MASK] ▁city ▁also ▁provides ▁the ▁back drop ▁for ▁shows ▁such ▁as ▁\" the ▁walking ▁dead \", ▁\" stranger ▁things \", ▁and ▁\" atlanta \", ▁in ▁addition ▁to ▁a ▁myriad ▁of ▁an imated ▁and ▁reality ▁television ▁programming . [SEP]\n",
            "I0629 21:04:33.547356 140456185767808 create_pretraining_data.py:151] input_ids: 4 286 2841 7 286 262 3831 1410 8 29 7 2211 1410 16 1720 323 77 88 14 6 173 15009 11 4063 24 94 10 7 181 4097 9 7 118 22 56 14 714 23 483 485 119 24 63 689 15 50 35992 45 7 80 763 3489 1052 26 15 2579 2493 18 12144 6 6 6 17 37 32179 13 7 118 8 116 840 29966 12541 29966 26 15 809 607 10315 10 18 1715 4838 28 41899 943 46 7 4834 1983 12 1743 8 29 434 2243 3167 7748 1994 2490 2356 41373 7 1052 13 18 6801 35 13165 23 286 89 119 24 7 486 15 8 286 57 37 564 9696 485 1784 6 30 93 10 7 118 26 15 163 7687 8 16 43299 909 11 7 4234 15091 8 11 286 6047 611 1245 13 7 6422 17353 15 69 263 9 15 9 339 8 43 22 856 46 7 224 258 6465 163 2109 9 7 658 15 832 7 118 661 215 3972 989 14 778 94 222 1094 6509 9 183 163 2104 33116 667 219 30888 8 11 721 2314 473 8083 15 219 23654 8 13406 11 13623 6787 957 1768 8 6 595 6 10 7 1699 17497 13 887 286 327 9 7 118 6 766 1003 13 286 3592 6 6 14 9231 25 1786 12582 1280 6 1191 29 7 2496 10 7 327 19 39143 46 40770 123 20 144 42531 8 350 29 65 16 14833 412 1087 66 20 2186 32779 11 450 1370 15830 60 111 8 35 22 54 250 7 834 15 25 6 6 18 2654 6 18432 13 1648 3592 19 8400 26 15 714 10 10525 6 238 1101 19 34978 152 202 13850 8 140 10 14 906 2835 12 3592 19 8400 8564 13 7 258 5 20 14 203 714 23 4708 11 969 415 8 286 1753 14 673 384 12 7 3124 646 9 35 2125 15 23 65 734 10 7 110 11 3968 3156 12 28189 171 6356 8 261 167 7 9955 5501 33 18 91 2873 11 7 14770 28 9242 11 17330 967 66 20 18 2356 19 763 28 11644 8 1722 10673 8 18 6869 21793 28 11 1722 21 5573 23360 9 6 7 65 731 8 18 10945 29 7 2247 28 18743 8 18 26528 82 11 7 1052 904 28 22050 8 18 44317 5874 26 15 2102 28 17264 8 18 91 29266 171 26 15 1325 28 18342 8 18 20255 8448 108 667 82 28 28245 8 18 30391 28 21944 8 11 18 12407 8590 28 6693 32 261 191 1686 1126 10 1573 16 1697 246 12 286 9 6 118 56 1249 7 342 8453 23 1187 66 20 18 91 4816 1319 67 18 38662 1844 67 11 18 7857 67 12 414 13 14 16707 10 37 18601 11 2885 969 1165 9 5\n",
            "I0629 21:04:33.547620 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.547857 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.547955 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 19 59 60 61 84 101 125 148 213 215 227 233 234 241 284 285 288 300 399 474\n",
            "I0629 21:04:33.548042 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 2809 437 41176 28 22 15869 6104 1245 49 71 56 5363 8 11 286 921 7 29 30 7\n",
            "I0629 21:04:33.548136 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.548213 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 21:04:33.648314 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.648973 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁social ▁media ▁and ▁the ▁rise ▁of ▁the ▁smart phone ▁has ▁changed ▁how ▁celebrities ▁are ▁treat ed ▁and ▁how ▁people ▁gain ▁the ▁platform ▁of ▁fame . ▁not ▁everything ▁is ▁as ▁conceal ed ▁as ▁it ▁was ▁back ▁in ▁old ▁hollywood ▁because ▁now ▁everything ▁is ▁put ▁out ▁on ▁the ▁internet [MASK] ▁fans ▁or ▁even ▁the ▁celebrity ▁themselves . ▁websites ▁atomoa ▁twitter , ▁facebook , ▁instagram ▁and ▁youtube ▁allow ▁people ▁to ▁become ▁a ▁celebrity ▁overnight . ▁for ▁example , ▁justin ▁bi eber ▁got ▁his ▁start ▁on ▁youtube ▁by ▁post ing ▁videos ▁of ▁him ▁singing ▁and ▁got ▁discovered . ▁all [MASK] ▁his ▁fans ▁got ▁direct ▁contact ▁to ▁his ▁content ▁and ▁were ▁able ▁to ▁interact ▁with ▁him ▁on ▁several ▁social ▁media ▁platforms . ▁social ▁media ▁has ▁substantial ly ▁changed ▁what ▁it ▁means ▁to ▁be ▁a ▁celebrity . ▁instagram ▁and ▁youtube ▁gives ▁regular ▁people ▁an ▁opportunity ▁to ▁become ▁rich [MASK] ▁famous ▁all ▁from ▁inside ▁the ir ▁home . ▁it ▁also ▁gives ▁fans ▁the ▁opportunity ▁to ▁connect ▁with ▁the ir ▁favorite ▁celebrity ▁without ▁ever ▁meeting ▁them ▁in ▁person . ▁everything ▁is ▁being ▁shared ▁on ▁social ▁media ▁so ▁it ▁makes ▁it ▁harder ▁for helburu ▁to ▁live ▁private ▁lives .</ ref > ▁kn ibbs , ▁kate ▁social ▁media ▁sites ▁have ▁also ▁contribut ed ▁to ▁the [MASK] ▁of ▁some ▁celebrities , ▁such ▁as ▁ tila ▁te qui la ▁who ▁became [MASK] ▁through ▁myspace . [SEP] ▁particular ▁field . ▁for ▁example , ▁the ▁kennedy ▁family ▁is ▁associat ed ▁with ▁us ▁politics ; ▁the ▁house [MASK] ▁winds or ▁with ▁royalty ; ▁the ▁hilton ▁and ▁roth s child ▁families ▁with ▁business ; ▁the ▁jackson ▁family ▁with ▁popular ▁music ; ▁and ▁the ▁os bourne , [MASK] ▁cheng [MASK] ▁kardashian , ▁baldwin , ▁and ▁barrymore ▁families ▁with ▁television ▁and ▁film . ▁access ▁to ▁celebrities ▁is ▁ strictly ▁controlled ▁by ▁the ir ▁ent ourage ▁of ▁staff ▁which ▁includes ▁managers , ▁public ists , ▁agents , [MASK] ▁assistants , ▁and ▁bodyguards . ▁even ▁journalists ▁find ▁it ▁difficult ▁to ▁access ▁celebrities ▁for ▁interviews . ▁an ▁interview ▁with ▁writer ▁and ▁actor ▁michael ▁must o ▁cites : ▁celebrities ▁often ▁hire ▁one ▁or ▁more ▁bodyguards ▁( or ▁close ▁protection ▁officer ) ▁to ▁protect ▁themselves ▁and ▁the ir ▁families ▁from ▁threats ▁rang ing ▁from ▁the ▁mund ane ▁( intrusi ve ▁papar azzi ▁photographer s ▁or ▁autograph - seeking ▁fans ) ▁to ▁serious ▁( assault , ▁kidnapping , ▁assassination , ▁or ▁stalking ). ▁the ▁bodyguard ▁travels ▁with ▁the ▁celebrity ▁dur ing ▁professional ▁activities ▁( movie ▁shoot s ▁or ▁concerts ) ▁and ▁personal ▁activities ▁such ▁as ▁recreation ▁and ▁erra nds . ▁celebrities ▁also ▁typically ▁have ▁security ▁staff ▁at [MASK] [MASK] ▁home , ▁to ▁protect ▁them ▁from ▁similar ▁threats . ▁and y ▁warhol ▁famously ▁coin ed ▁the ▁phrase ▁\" 15 ▁minutes ▁supremacy ▁fame \" ▁in ▁reference ▁to ▁a ▁short - lived ▁publicity . ▁certain ▁\" 15 ▁minutes ▁of [MASK] [MASK] ▁celebrities ▁can ▁be ▁average ▁people ▁seen ▁with ▁an ▁a - list ▁celebrity , ▁who ▁are ▁sometimes ▁noticed ▁on ▁entertainment ▁news ▁channels ▁such ▁as ▁e ! ▁news . ▁these ▁persons ▁are ▁ordinary ▁people ▁becoming ▁celebrities , ▁often ▁based ▁on ▁the ▁ridicul ous [MASK] ▁they ▁do . ▁\" [SEP]\n",
            "I0629 21:04:33.650922 140456185767808 create_pretraining_data.py:151] input_ids: 4 351 868 11 7 1861 10 7 11683 7816 53 1590 503 1690 32 1505 31 11 503 122 744 7 3931 10 1824 9 54 3985 17 20 10458 31 20 35 22 342 12 396 4457 182 254 3985 17 894 144 30 7 1039 6 3157 39 216 7 2369 745 9 9775 24694 17068 8 14723 8 14511 11 7475 785 122 13 357 14 2369 17610 9 23 174 8 11744 649 42619 3261 40 2076 30 7475 27 453 24 9989 10 160 2976 11 3261 1498 9 80 6 40 3157 3261 1026 2441 13 40 1242 11 49 815 13 7970 29 160 30 191 351 868 7740 9 351 868 53 2545 52 1590 301 35 635 13 38 14 2369 9 14511 11 7475 1867 1520 122 37 2413 13 357 2190 6 809 80 33 2558 7 78 327 9 35 56 1867 3157 7 2413 13 3570 29 7 78 7352 2369 386 954 2075 167 12 515 9 3985 17 138 2145 30 351 868 137 35 1303 35 10536 23 38380 13 607 775 3173 18697 14184 1499 12070 40338 8 10051 351 868 2107 51 56 3087 31 13 7 6 10 71 1690 8 66 20 16 25505 3192 18335 1334 70 159 6 172 28573 9 5 616 537 9 23 174 8 7 8653 412 17 1288 31 29 276 1579 60 7 364 6 5817 313 29 10358 60 7 21947 11 16353 15 11068 1492 29 979 60 7 3338 412 29 346 163 60 11 7 7700 28102 8 6 42994 6 27235 8 12697 8 11 28654 1492 29 969 11 164 9 1148 13 1690 17 16 3920 2140 27 7 78 15668 20580 10 3399 43 767 7214 8 256 5836 8 3373 8 6 23310 8 11 15047 9 216 5738 1252 35 1222 13 1148 1690 23 5543 9 37 3693 29 2117 11 3034 2466 365 95 7126 45 1690 141 24905 59 39 77 15047 21 313 467 1785 2752 41 13 1705 745 11 7 78 1492 33 5582 6499 24 33 7 31539 13544 21 35527 432 23649 32849 15089 15 39 28055 19 39048 3157 41 13 2367 21 26344 8 11411 8 6155 8 39 29444 73 7 26611 7398 29 7 2369 119 24 966 1030 21 25959 5243 15 39 4678 41 11 1122 1030 66 20 7410 11 31778 18651 9 1690 56 818 51 782 3399 46 6 6 327 8 13 1705 167 33 348 5582 9 11 82 28900 6047 4312 31 7 2200 18 5758 2784 8696 1824 28 12 1598 13 14 566 19 5648 8149 9 395 18 5758 2784 10 6 6 1690 75 38 561 122 490 29 37 14 19 4393 2369 8 70 32 438 8135 30 3124 2422 5564 66 20 280 2619 2422 9 87 3512 32 4984 122 1648 1690 8 141 257 30 7 19243 874 6 72 260 9 18 5\n",
            "I0629 21:04:33.652202 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.653706 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.654582 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 48 57 96 104 143 185 206 220 243 267 271 272 273 309 424 425 446 463 464 506\n",
            "I0629 21:04:33.655253 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 27 252 10 1242 11 1690 1824 123 10 7 3809 23342 8 1122 7 78 10 1824 28 1844\n",
            "I0629 21:04:33.655736 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.655898 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 21:04:33.659137 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.659748 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁from ▁latin ▁\" ar nus \" ▁( pliny , ▁\" natural ▁history \" ▁3.5 0). ▁the ▁philologist ▁hans ▁kra he ▁related ▁this ▁to ponym ▁on ▁a ▁paleo - european ▁basis ▁\"* ar - n - \", ▁derived [MASK] ▁the ▁proto - indo - european ▁root ▁* \" er - \", ▁\" flow , ▁move \". [SEP] ▁that [MASK] ▁is ▁consistent . ▁the ▁continuum ▁hypothesis ▁and ▁the ▁axiom ▁of ▁choice ▁were ▁among ▁the ▁first ▁mathematical ▁statements ▁shown ▁to ▁be ▁independent ▁of ▁zf ▁set [MASK] [MASK] ▁gödel ▁believed ▁that ▁ch ▁is ▁false , ▁and ▁that ▁his ▁proof [MASK] ▁ch ▁is ▁consistent ▁with ▁zfc ▁only ▁shows ▁that ▁the ▁zermelo – fra enkel ▁axioms ▁do ▁not ▁adequate ly ▁characterize ▁the ▁universe ▁of ▁sets . ▁gödel ▁was ▁a [MASK] [MASK] ▁and ▁therefore ▁had ▁no ▁problems ▁with ▁assert ing ▁the ▁truth ▁and ▁falsehood ▁of ▁statements ▁independent ▁of ▁the ir ▁ provability . ▁cohen , ▁though ▁a ▁formal ist ▁ , ▁also ▁tend ed ▁toward s ▁reject ing ▁ch . ▁historically , ▁mathematicians ▁who ▁favor ed ▁a ▁\" rich \" ▁and ▁\" large \" ▁universe ▁of ▁sets ▁were ▁ against [MASK] [MASK] ▁while ▁those ▁favor ing ▁a ▁\" n eat \" ▁and ▁\" controllable \" ▁universe ▁favor ed ▁ch . ▁parallel ▁arguments ▁were ▁made ▁for ▁and ▁ against ▁the [MASK] ▁of ▁ constructibility , ▁which ▁implies ▁ch . ▁more arlie [MASK] ▁matthew ▁for eman ▁has ▁point ed ▁out ▁that ▁ ontological ▁maximal ism ▁can ▁ actually ▁be ▁used ▁to ▁argue ▁in ▁favor ▁of ▁ch , ▁because ▁among ▁models ▁that ▁have ▁the ▁same ▁real s , ▁models ▁with ▁\" more \" ▁sets ▁of ▁real s ▁have ▁a ▁better ▁chance ▁of [MASK] [MASK] ▁ch ▁( ma ddy ▁1988 , ▁p . ▁500 ). ▁another ▁viewpoint ▁is ▁that ▁the ▁conception ▁of ▁set ▁is ▁not ▁specific ▁enough ▁to ▁determine ▁whe ther ▁ch ▁is ▁true [MASK] ▁false . ▁this ▁viewpoint ▁was ▁advanced ▁as ▁early ▁as ▁1923 ▁by ▁skolem , ▁even ▁before ▁gödel ' s ▁first ▁in completeness ▁theorem . ▁skolem ▁argued ▁on ▁the ▁basis ▁of ▁what ▁is [MASK] ▁known ▁as ▁skolem ' s ▁paradox , ▁and ▁it ▁was ▁later ▁supported ▁by ▁the ▁independence ▁of ▁ch ▁from ▁the ▁axioms ▁of ▁zfc ▁since ▁these ▁axioms ▁are ▁enough ▁to ▁establish ▁the ▁elementary ▁properties ▁of ▁sets ▁and ▁cardinal ities . ▁in ▁order ▁to ▁argue ▁ against ▁this ▁viewpoint , ▁it ▁would ▁be ▁sufficient ▁to ▁demonstrate ▁new ▁axioms ▁that ▁are ▁supported ▁by ▁intuition ▁and ▁resolve ▁ch ▁in ▁one ▁direction ▁or ▁another . ▁although ▁the ▁axiom ▁of ▁ constructibility ▁does ▁resolve ▁ch , ▁it ▁is ▁not ▁generally ▁considered ▁to ▁be ▁intuitively ▁true ▁any ▁more ▁than ▁ch ▁is ▁generally ▁considered ▁to ▁be ▁false ▁( kun en ▁1980 , ▁p . ▁171 ). ▁at ▁least ▁two ▁other ▁axioms ▁have ▁been ▁proposed ▁that ▁have ▁implications ▁for ▁the ▁continuum ▁hypothesis , ▁although [MASK] ▁axioms ▁have ▁not ▁currently ▁found ▁wide ▁acceptance ▁in ▁the ▁mathematical ▁community . ▁in ▁1986 , ▁chris ▁freiling ▁presented ▁an [MASK] ▁ against ▁ch ▁by ▁showing ▁that ▁the ▁negati on ▁of ▁ch ▁is ▁equivalent ▁to ▁freiling ' s ▁axiom ▁of ▁symmetr y , ▁a ▁statement ▁about ▁probabilities . ▁freiling ▁believe [SEP]\n",
            "I0629 21:04:33.660319 140456185767808 create_pretraining_data.py:151] input_ids: 4 33 829 18 1068 13813 28 21 7592 8 18 4527 344 28 19360 12958 7 17352 6134 24078 838 715 47 13 40706 30 14 19747 19 3146 1164 13728 1068 19 55 19 67 1446 6 7 5272 19 12276 19 3146 2455 34 28 171 19 67 18 14870 8 1307 89 5 25 6 17 2340 9 7 3524 2086 11 7 6405 10 2501 49 261 7 69 2876 3670 1022 13 38 929 10 11672 246 6 6 8634 1000 25 1878 17 4117 8 11 25 40 5159 6 1878 17 2340 29 4784 100 1187 25 7 15003 424 18676 18560 10677 260 54 5729 52 3273 7 3225 10 2445 9 8634 22 14 6 6 11 800 57 107 1063 29 4757 24 7 3266 11 17247 10 3670 929 10 7 78 16 29984 9 4112 8 319 14 2038 1201 16 8 56 1277 31 727 15 3702 24 1878 9 2331 8 7674 70 3320 31 14 18 4029 28 11 18 9131 28 3225 10 2445 49 16 214 6 6 116 220 3320 24 14 18 55 12121 28 11 18 22465 28 3225 3320 31 1878 9 2932 3098 49 149 23 11 16 214 7 6 10 16 29880 8 43 4291 1878 9 77 25326 6 9220 23 8989 53 383 31 144 25 16 30220 24055 1609 75 16 1697 38 86 13 3272 12 3320 10 1878 8 182 261 2100 25 51 7 181 678 15 8 2100 29 18 3291 28 2445 10 678 15 51 14 982 2419 10 6 6 1878 21 1862 19444 2259 8 423 9 4326 73 238 9459 17 25 7 6789 10 246 17 54 738 1637 13 2108 824 693 1878 17 1702 6 4117 9 47 9459 22 2007 20 148 20 5391 27 21144 8 216 173 8634 26 15 69 12 23486 4424 9 21144 1530 30 7 1164 10 301 17 6 123 20 21144 26 15 6913 8 11 35 22 135 1455 27 7 1199 10 1878 33 7 10677 10 4784 127 87 10677 32 1637 13 1580 7 7508 1206 10 2445 11 4574 3202 9 12 316 13 3272 16 214 47 9459 8 35 103 38 2188 13 5345 84 10677 25 32 1455 27 22687 11 7987 1878 12 59 2609 39 238 9 188 7 6405 10 16 29880 369 7987 1878 8 35 17 54 513 287 13 38 22372 1702 150 77 88 1878 17 513 287 13 38 4117 21 25468 99 486 8 423 9 15961 73 46 647 93 65 10677 51 61 1423 25 51 8863 23 7 3524 2086 8 188 6 10677 51 54 1330 184 1235 2528 12 7 2876 362 9 12 2389 8 9283 21215 1794 37 6 16 214 1878 27 2852 25 7 23811 187 10 1878 17 1907 13 21215 26 15 6405 10 26706 82 8 14 2671 109 11335 9 21215 958 5\n",
            "I0629 21:04:33.660843 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.661425 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.661614 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 38 58 83 84 96 104 124 125 184 185 213 223 224 273 274 304 336 385 461 481\n",
            "I0629 21:04:33.661724 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 33 4784 638 9 25 25 7355 1201 1878 8 6405 1270 8 5175 24 39 254 103 87 1851\n",
            "I0629 21:04:33.661836 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.661927 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.663492 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.663668 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁help page [SEP] ▁urtarrila ren [MASK] [SEP]\n",
            "I0629 21:04:33.663964 140456185767808 create_pretraining_data.py:151] input_ids: 4 562 2684 5 1622 79 6 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.664225 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.664465 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.664578 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.664675 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 2620 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.664772 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 21:04:33.664852 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.666133 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.666517 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁property ▁rights ▁were ▁strong . ▁while ▁nationalization ▁committees ▁were ▁set ▁up [MASK] ▁france ▁and ▁the [MASK] ▁kingdom , ▁finland ▁avoided ▁nationalization s . ▁after ▁failed ▁experiments [MASK] ▁protection ism , ▁finland ▁ease d ▁restrictions [MASK] ▁concluded ▁a ▁free ▁trade ▁agreement ▁with ▁the ▁european ▁community ▁in ▁1973 , ▁making ▁its ▁markets ▁more ▁competitive . ▁local ▁education ▁markets ▁expanded [MASK] ▁an ▁increasing ▁number ▁of ▁finns ▁also ▁went ▁abroad ▁to ▁study ▁in ▁the ▁united ▁states ▁or ▁western ▁europe , ▁bring ing ▁back ▁advanced ▁skills . ▁there ▁was ▁a ▁ quite [MASK] [MASK] ▁but ▁pragmatic - minded , ▁credit ▁and ▁investment ▁cooperation ▁by ▁state ▁and ▁corporations , ▁though ▁it ▁was ▁considered ▁with ▁suspicion . ▁support ▁for ▁capitalism ▁was ▁widespread . [MASK] ▁rate ▁hover ed ▁among ▁the ▁world ' s ▁highest , ▁at ▁around ▁ 8% ▁until ▁the ▁1980 s . ▁in ▁the [MASK] ▁of ▁the ▁1970 s , ▁finland ' s ▁gdp ▁per ▁capita ▁reached ▁the ▁level ▁of ▁japan ▁and ▁the ▁uk . ▁finland ' s ▁economic ▁development ▁shared ▁many ▁aspects ▁with ▁export - led ▁asian ▁countries . ▁the [MASK] ▁policy ▁of ▁neutrality ▁enabled ▁finland ▁to ▁trade ▁both ▁with ▁western ▁and ▁ comecon ▁markets . ▁significant ▁bilateral ▁trade ▁was ▁conducted ▁with ▁the ▁soviet ▁union , ▁but [MASK] ▁did ▁not ▁grow ▁into ▁a ▁dependence . [SEP] ical ▁damage . ▁a ▁deep er ▁coma ▁alone ▁does ▁not ▁investigated ▁mean ▁a ▁slim mer ▁chance ▁of ▁recovery , ▁similarly , ▁mild er ▁comas ▁do ▁not ▁ensure ▁higher ▁chance s ▁of ▁recovery ▁ . ▁the ▁most ▁common [MASK] [MASK] ▁death ▁for ▁a ▁person ▁in ▁a ▁vegetati ve ▁state ▁papa ▁secondary ▁infection ▁such ▁as ▁pneumonia , ▁which ▁can ▁occur ▁in ▁patients [MASK] ▁lie ▁still ▁for ▁extended ▁periods . [MASK] ▁may ▁emerge ▁from ▁a ▁coma ▁with ▁a ▁combination ▁of ▁physical , ▁intellectual , ▁and ▁psychological ▁difficulties ▁that ▁need ▁special ▁attention . ▁it ▁is ▁common ▁for ▁coma ▁patients ▁to [MASK] ▁in ▁a ▁profound ▁state ▁of ▁confusion ▁and ▁suffer [MASK] ▁dys arthri a , ▁the ▁in ability ▁to ▁articulate ▁any ▁speech . ▁recovery ▁usually ▁occurs ▁gradually . ▁in ▁the ▁first ▁days , ▁patients ▁may ▁only ▁awaken ▁for ▁a ▁few ▁minutes , ▁with ▁increased ▁duration ▁of ▁ wakefulness ▁as ▁the ir ▁recovery ▁progresses ▁and ▁may ▁eventually ▁recover ▁full ▁awareness . ▁that ▁said , ▁some ▁patients ▁may ▁never ▁progress ▁beyond ▁very ▁basic ▁responses . ▁there ▁are ▁report s ▁of ▁patients ▁coming ▁out ▁of ▁coma ▁after ▁long ▁periods ▁of ▁time . ▁after ▁19 ▁years ▁in ▁a ▁minimal ly ▁conscious ▁state , ▁terr y ▁wall is ▁spontaneously ▁began ▁speaking ▁and ▁regain ed ▁awareness ▁of ▁his ▁surroundings . ▁a ▁brain - damaged ▁man , [MASK] ▁in ▁a ▁coma - like ▁state ▁for ▁six ▁years , ▁was ▁brought ▁back ▁to ▁consciousness ▁in ▁2003 ▁by ▁doctors ▁who ▁plant ed ▁electrode s ▁deep ▁inside ▁his ▁brain . ▁the ▁method , ▁called ▁deep ▁brain ▁stimulat ion ▁( dbs ) ▁successfully ▁rous ed ▁communication , ▁complex ▁movement ▁and ▁eating ▁ability ▁in ▁the ▁38 - year - old ▁american ▁man ▁who ▁suffer ed ▁a ▁trauma tic ▁brain ▁injury . ▁his ▁injuries ▁left ▁him ▁in ▁a ▁minimal ly ▁conscious ▁state ▁( mcs ), ▁a ▁condition [SEP]\n",
            "I0629 21:04:33.760035 140456185767808 create_pretraining_data.py:151] input_ids: 4 1457 796 49 747 9 116 22398 9995 49 246 126 6 200 11 7 6 532 8 195 6412 22398 15 9 81 1606 4175 6 1785 1609 8 195 8954 44 3540 6 3043 14 248 468 1470 29 7 176 362 12 2505 8 587 63 2492 77 3858 9 335 1029 2492 2004 6 37 1421 154 10 2823 56 1085 4268 13 430 12 7 157 147 39 529 410 8 1418 24 342 2007 2725 9 85 22 14 16 2162 6 6 58 18191 19 16124 8 4546 11 2855 3719 27 168 11 6067 8 319 35 22 287 29 14225 9 288 23 12946 22 2083 9 6 712 24569 31 261 7 110 26 15 1079 8 46 243 16 3517 250 7 486 15 9 12 7 6 10 7 689 15 8 195 26 15 2533 300 4237 1494 7 541 10 1729 11 7 1083 9 195 26 15 491 283 2145 94 2547 29 2792 19 2059 2830 285 9 7 6 843 10 11431 5741 195 13 468 134 29 529 11 16 32112 2492 9 673 9550 468 22 2280 29 7 1272 359 8 58 6 274 54 1388 92 14 11697 9 5 1015 1651 9 14 1517 171 1752 2020 369 54 6694 2432 14 9667 4410 2419 10 2465 8 2062 8 4178 171 12250 260 54 2532 737 2419 15 10 2465 16 9 7 74 247 6 6 340 23 14 515 12 14 16225 432 168 11037 3555 9239 66 20 14982 8 43 75 1512 12 1175 6 4330 303 23 1429 2566 9 6 83 8335 33 14 1752 29 14 2022 10 1102 8 5129 8 11 4528 3964 25 896 664 1727 9 35 17 247 23 1752 1175 13 6 12 14 4782 168 10 8048 11 2818 6 13882 31130 42 8 7 12 3942 13 24075 150 1442 9 2465 314 1921 3128 9 12 7 69 743 8 1175 83 100 16723 23 14 389 2784 8 29 614 5018 10 16 17394 20 7 78 2465 18047 11 83 724 5585 576 3878 9 25 421 8 71 1175 83 593 2996 1493 264 1383 10154 9 85 32 1035 15 10 1175 2751 144 10 1752 81 255 2566 10 98 9 81 449 120 12 14 5531 52 8716 168 8 10642 82 1977 667 11508 282 3470 11 6810 31 3878 10 40 6477 9 14 1678 19 38599 374 8 6 12 14 1752 19 1336 168 23 585 120 8 22 787 342 13 4793 12 1516 27 6037 70 1801 31 18042 15 1517 2558 40 1678 9 7 1306 8 152 1517 1678 12442 579 21 44151 41 2978 44248 31 3184 8 650 411 11 5076 1629 12 7 4571 19 1327 19 1751 235 374 70 2818 31 14 8272 1320 1678 4139 9 40 5191 304 160 12 14 5531 52 8716 168 21 42815 76 14 2513 5\n",
            "I0629 21:04:33.760986 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.761632 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.761984 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 12 16 27 35 58 88 89 117 139 176 203 222 249 250 260 272 279 308 317 427\n",
            "I0629 21:04:33.762169 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 12 157 29 11 11 247 8 10613 754 730 47 3422 882 10 17 70 122 16723 33 10404\n",
            "I0629 21:04:33.762482 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.762662 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.764227 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.764434 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁prev n [SEP] ▁ editsection [SEP]\n",
            "I0629 21:04:33.764770 140456185767808 create_pretraining_data.py:151] input_ids: 4 40597 55 5 16 28864 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.765050 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.765304 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.765414 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.765542 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.765666 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 21:04:33.765751 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.766896 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.767096 140456185767808 create_pretraining_data.py:141] tokens: [CLS] [MASK] ▁* ▁espainia / portugal ▁- ▁alba ko ▁duke ak [MASK] ▁konkistatu [MASK] ▁* ▁espainia / portugal ▁- [MASK] ▁feli pe ▁ii ari ▁portugal go ▁feli pe ▁monoton ▁titulua ▁gehitze n ▁diote [MASK] ▁eta ▁kultura ▁zientzia [MASK] ▁teknologia ▁kirolak ▁jaiotza k ▁* ▁francisco ▁de [MASK] broadcast ▁ligaments ▁espainiar [MASK] ▁* ▁pierre ▁vernier : ▁frantziar ▁matematikaria , ▁vernier ▁eskala ren ▁asmatzailea ▁heriotzak ▁* ▁abuztua ren [MASK] [MASK] ▁- ▁andrea ▁di ▁pietr o [MASK] ▁gondol a , ▁andrea ▁palladi o ▁izenez ▁ezaguna , ▁pizkunde ko ▁italiar ▁arkitekto ▁ospetsua . ▁---- [SEP] ▁agintari ak [SEP]\n",
            "I0629 21:04:33.767407 140456185767808 create_pretraining_data.py:151] input_ids: 4 6 34 7890 142 20049 48 19344 64 2037 102 6 11529 6 34 7890 142 20049 48 6 9657 4935 305 1864 2030 426 9657 4935 36340 11600 23157 55 15357 6 36 406 379 6 501 505 651 68 34 3306 113 6 25912 21964 3246 6 34 2911 28171 45 3254 9177 8 28171 7945 79 9498 499 34 6262 79 6 6 48 10836 1344 11939 95 6 24498 42 8 10836 22210 95 8831 6034 8 9290 64 3497 3767 13692 9 924 5 857 102 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.767699 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.767961 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.768065 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 1 11 13 19 28 33 37 45 46 47 49 65 66 72 0 0 0 0 0 0\n",
            "I0629 21:04:33.768158 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 502 2030 130 8992 131 358 36 10844 30554 45 880 449 42 26707 0 0 0 0 0 0\n",
            "I0629 21:04:33.768262 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 21:04:33.768343 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 21:04:33.769473 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.769723 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁any ▁real ▁amplifier ▁is ▁an ▁imperfect ▁real ization ▁of [MASK] ▁ideal ▁amplifier . ▁an ▁important ▁limitation ▁of ▁a ▁real ▁amplifier ▁is ▁that ▁the [MASK] [MASK] ▁generate s ▁is ▁ultimately ▁limited ▁by ▁the ▁power [MASK] ▁from ▁the ▁power ▁supply . ▁an ▁amplifier ▁saturate s ▁and ▁clip s ▁the ▁output ▁if ▁the ▁input ▁signal ▁becomes ▁too ▁large ▁for ▁the ▁amplifier ▁to ▁reproduce ▁or ▁exceed s [MASK] ▁limits ▁for ▁the ▁device . ▁the ▁power ▁supply ▁may ▁influence ▁the ▁output , ▁so ▁must ▁be ▁considered ▁in ▁the ▁design . ▁the [MASK] ▁output ▁from ▁an ▁amplifier ▁cannot ▁exceed ▁its ▁input [MASK] [MASK] ▁the [MASK] [MASK] ▁has ▁an ▁\" open ▁loop \" ▁performance . ▁this ▁is ▁described ▁by ▁various ▁parameters ▁( gain , [MASK] [MASK] ▁rate , ▁output ▁impedance , ▁distortion , ▁bandwidth [MASK] ▁signal - to - noise ▁ratio , ▁etc . ). ▁many ▁modern ▁amplifiers ▁use [MASK] ▁feedback ▁techniques ▁to ▁hold ▁the ▁gain ▁at ▁the ▁desired ▁value ▁and [MASK] ▁distortion . ▁negative [MASK] ▁feedback ▁has ▁the ngel ▁effect ▁of ▁lower ing ▁the ▁output ▁impedance ▁and ▁thereby ▁increasing ▁electrical ▁damping ▁of ▁loudspeaker ▁motion ▁at ▁and ▁near ▁the ▁resonan ce ▁frequency ▁of ▁the [MASK] [MASK] [SEP] ▁unlockdb text [SEP]\n",
            "I0629 21:04:33.770049 140456185767808 create_pretraining_data.py:151] input_ids: 4 150 678 869 17 37 17144 678 2019 10 6 1683 869 9 37 321 14272 10 14 678 869 17 25 7 6 6 3066 15 17 2043 806 27 7 259 6 33 7 259 1645 9 37 869 33271 15 11 13854 15 7 636 151 7 1200 1055 2552 1634 177 23 7 869 13 10866 39 6177 15 6 2736 23 7 2023 9 7 259 1645 83 336 7 636 8 137 365 38 287 12 7 507 9 7 6 636 33 37 869 1407 6177 63 1200 6 6 7 6 6 53 37 18 4888 3387 28 1027 9 47 17 565 27 330 4497 21 15931 8 6 6 712 8 636 3912 8 5365 8 4779 6 1055 19 309 19 20217 3154 8 1550 9 73 94 239 902 97 6 1896 1948 13 1250 7 744 46 7 6009 976 11 6 5365 9 1070 6 1896 53 7 23247 630 10 707 24 7 636 3912 11 3643 1421 4027 21396 10 39113 4383 46 11 692 7 11088 1280 1614 10 7 6 6 5 27829 1608 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.770307 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.770576 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.770685 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 10 24 25 34 64 87 96 97 99 100 118 119 127 128 143 155 159 163 188 189\n",
            "I0629 21:04:33.770781 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 37 636 35 619 2489 259 259 9 869 1677 290 7831 4779 8 1070 2296 3387 1258 6931 9\n",
            "I0629 21:04:33.770882 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.770973 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.772129 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.772489 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁t [SEP] ▁press ▁attention ▁at ▁the ▁time , ▁exciting ▁the ▁lagoa ▁that ▁they ▁would ▁have ▁a ▁mass - produced ▁afford able ▁airplane ▁product ▁that ▁would ▁be ▁made , ▁marketed , ▁sold , ▁and ▁maintained ▁just ▁like ▁an ▁automobile . ▁the ▁airplane ▁was ▁to ▁be ▁as ▁commonplace ▁in ▁the ▁future ▁as ▁the ▁model [MASK] ▁of ▁the ▁time . ▁in ▁1940 , ▁henry ▁for [MASK] ▁famously ▁predicted : ▁\" mark ▁my ▁word : ▁a ▁combination ▁airplane ▁and ▁motor car ▁is ▁coming . ▁you ▁may ▁smile , ▁but ▁it ▁will ▁come . ” ▁in ▁1942 , ▁the ▁soviet ▁armed ▁forces ▁experiment ed ▁with ▁a ▁glid ing ▁tank , ▁the ▁anton ov ▁a -40 , ▁but ▁it ▁was ▁not ▁ capable ▁of ▁flying ckel ▁its ▁own . ▁the ▁aero car ▁designed ▁and ▁built ▁by ▁molt ▁taylor ▁made ▁a ▁successful ▁flight ▁in ▁december ▁1949 , ▁and ▁in ▁following ▁years ▁versions ▁under went ▁a ▁series ▁of ▁road ▁and ▁flying ▁tests . ▁ chuck ▁berry ▁featured ▁the ▁concept ▁in ▁his ▁1956 ▁song ▁\" you ▁can ' t ▁catch ▁me \", ▁and [MASK] ▁december ▁1956 ▁the ▁civil ▁aviation ▁authority ▁approved ▁the ▁design ▁for ▁mass ▁production , ▁but ▁de spite ▁wide ▁publicity ▁and ▁an ▁improved ▁version ▁produced ▁in ▁1989 , ▁taylor ▁did ▁not ▁succeed ▁in ▁gett ing ▁the ▁flying ▁car ▁into ▁production . ▁in ▁the ▁period ▁between ▁1956 - 1958 , ▁for d ' s ▁advanced ▁design ▁studio ▁built ▁the ▁ volant e ▁tri - atho dyne , ▁a ▁ 3/8 ▁scale ▁concept [MASK] ▁model . ▁it ▁was ▁designed ▁to ▁have ▁three ▁ducted ▁fans , ▁each ▁with [MASK] [MASK] ▁own ▁motor , ▁that ▁would ▁lift ▁it ▁off ▁the ▁ground ▁and ▁move ▁it ▁through ▁the ▁air . ▁in ▁public ▁relation ▁release , ▁for d ▁noted ▁that [MASK] the ▁day ▁where ▁there ▁will ▁be ▁an ▁aero - car ▁in ▁every ▁garage ▁is ▁still [MASK] ▁time ▁off \", ▁but ▁added ▁that ▁\" the ▁ volant e ▁indicate s ▁one ▁direction ▁that ▁the ▁styling ▁of ▁such ▁a ▁vehicle ▁would ▁take \". ▁in ▁1957 , ▁popular ▁mechanics ▁reported ▁that ▁hill er ▁helicopters 67. ▁developing ▁a ▁ducted - fan ▁aircraft ▁that ▁would ▁be ▁easier ▁to ▁fly ▁than ▁helicopters , ▁and ▁should ▁cost ▁a ▁lot ▁less . ▁hill er ▁engineers ▁expected ▁that ▁this ▁type ▁of ▁an ▁aircraft ▁would ▁become ▁the ▁basis ▁for ▁a ▁whole ▁family ▁of ▁special - purpose ▁aircraft . ▁in ▁1956 , [MASK] ▁us ▁army ' s ▁transportation ▁research ▁command ▁began ▁an ▁investigation ▁into ▁\" flying ▁jeep s \", ▁ducted - fan - based ▁aircraft ▁that [MASK] ▁envision ed ▁to ▁be ▁smaller ▁and ▁easier ▁to ▁fly ▁than ▁helicopters . ▁in ▁1957 , ▁chrys ler , ▁curtis s - wright , ▁and ▁piasecki ▁were ▁as signed ▁contracts ▁for ▁building ▁and ▁delivery ▁of ▁prototypes . ▁they ▁all ▁delivered ▁the ir ▁prototypes ; ▁however , ▁piasecki ' s ▁v z -8 ▁was ▁the ▁most ▁successful ▁of ▁the ▁three . ▁while ▁it ▁would ▁normally ▁operate ▁close ▁to ▁the ▁ground , ▁it ▁was ▁ capable ▁of ▁flying ▁to ▁several ▁thousand ▁feet , ▁ proving [MASK] ▁be ▁stable ▁in ▁flight . ▁nonetheless , ▁the ▁army ▁decided ▁that ▁the ▁\" flying [SEP]\n",
            "I0629 21:04:33.865853 140456185767808 create_pretraining_data.py:151] input_ids: 4 219 5 2155 1727 46 7 98 8 11387 7 30077 25 72 103 51 14 1356 19 7254 5265 294 10081 1680 25 103 38 149 8 9832 8 1468 8 11 3355 595 252 37 16703 9 7 10081 22 13 38 20 10354 12 7 1074 20 7 811 6 10 7 98 9 12 1787 8 781 23 6 6047 5477 45 18 20002 1312 451 45 14 2022 10081 11 2361 9066 17 2751 9 759 83 22753 8 58 35 165 684 9 0 12 2802 8 7 1272 1138 458 3193 31 29 14 19164 24 10166 8 7 3927 9044 14 33989 8 58 35 22 54 16 2066 10 1375 40796 63 265 9 7 9694 9066 945 11 558 27 15405 5964 149 14 1128 1104 12 793 2756 8 11 12 226 120 1521 124 8572 14 550 10 952 11 1375 2227 9 16 4813 15137 1741 7 928 12 40 1971 372 18 5596 75 26 117 4542 1120 67 11 6 793 1971 7 506 8017 983 3548 7 507 23 1356 415 8 58 113 1937 1235 8149 11 37 1698 480 534 12 2625 8 5964 274 54 5879 12 10159 24 7 1375 1077 92 415 9 12 7 232 101 1971 19 39324 8 23 44 26 15 2007 507 2164 558 7 16 25977 62 4523 19 34673 13686 8 14 16 40850 2047 928 6 811 9 35 22 945 13 51 180 13342 3157 8 190 29 6 6 265 2361 8 25 103 8302 35 674 7 888 11 1307 35 172 7 230 9 12 256 2794 1465 8 23 44 1456 25 6 91 332 114 85 165 38 37 9694 19 9066 12 375 14774 17 303 6 98 674 67 58 1212 25 18 91 16 25977 62 1452 15 59 2609 25 7 18006 10 66 14 3392 103 553 89 12 3293 8 346 2949 742 25 2776 171 5492 44447 2429 14 13342 19 10684 440 25 103 38 4617 13 4539 88 5492 8 11 429 1405 14 6041 370 9 2776 171 2842 1490 25 47 696 10 37 440 103 357 7 1164 23 14 1018 412 10 664 19 11086 440 9 12 1971 8 6 276 597 26 15 2001 377 1420 282 37 3052 92 18 9195 31221 15 67 13342 19 10684 19 859 440 25 6 17922 31 13 38 819 11 4617 13 4539 88 5492 9 12 3293 8 40016 8265 8 10377 15 19 25593 8 11 20784 49 20 4329 3799 23 986 11 6330 10 18804 9 72 80 2769 7 78 18804 60 111 8 20784 26 15 504 296 15708 22 7 74 1128 10 7 180 9 116 35 103 2012 1559 467 13 7 888 8 35 22 16 2066 10 1375 13 191 3008 3980 8 16 16089 6 38 5547 12 1104 9 5409 8 7 597 1289 25 7 18 9195 5\n",
            "I0629 21:04:33.866769 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.867534 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.867762 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 10 11 53 62 63 120 175 245 259 260 270 287 288 303 339 356 389 413 451 496\n",
            "I0629 21:04:33.867907 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 7 256 219 23 44 30 12 1077 7 78 888 18 91 71 22 429 7 49 80 13\n",
            "I0629 21:04:33.868036 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.868124 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.869221 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.869363 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁sare [MASK] [SEP] ▁upload text [SEP]\n",
            "I0629 21:04:33.869646 140456185767808 create_pretraining_data.py:151] input_ids: 4 2034 6 5 10175 1608 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.869888 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.870123 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.870216 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.870301 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 7806 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.870395 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 21:04:33.870474 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.871503 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.871660 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁hona ▁hemen ▁$1( e ) tik ▁gordeta ko [MASK] [MASK] [SEP] ▁google search [SEP]\n",
            "I0629 21:04:33.871937 140456185767808 create_pretraining_data.py:151] input_ids: 4 10488 7680 12948 62 41 476 15258 64 6 6 5 5208 36299 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.872182 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.872416 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.872513 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 9 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.872617 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 9484 45 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.872715 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 21:04:33.872792 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.873930 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.874288 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁internationally , ▁the ▁blackpool ▁dance ▁festival , [MASK] ▁annually ▁at ▁blackpool , ▁england ▁is ▁considered ▁the ▁most ▁prestigio us ▁event ▁a ▁dancesport ▁competitor ▁can ▁attend . [SEP] ▁dance ▁is ▁another ▁style ▁of ▁competitive ▁dance ▁recognized ▁by ▁the ▁wdsf . ▁in ▁this ▁style , ▁multiple ▁dancers ▁( usually ▁in ▁couples ▁and ▁typically ▁up ▁to ▁16 ▁dancers [MASK] ▁one ▁time ) ▁compete ▁on ▁the ▁same ▁team , ▁ moving ▁in ▁and ▁out ▁of ▁various ▁formations ▁while ▁dancing . ▁in ▁competitive ▁ballroom , [MASK] ▁are ▁judge d ▁by ▁diverse ▁criteria ▁such [MASK] ▁poise , ▁the ▁hold ▁or ▁frame , ▁posture , ▁musical ity ▁and ▁expression , ▁timing , ▁body ▁alignment ▁and ▁shape , ▁floor ▁craft , ▁foot ▁and ▁leg ▁action , ▁and ▁presentation . ▁judg ing ▁in ▁a ▁performance - oriented ▁sport ▁is ▁inevitabl y ▁subjective ▁in ▁nature , ▁and [MASK] ▁and ▁complaints ▁by ▁competitor s ▁over ▁judg ing ▁placement s ▁are ▁not ▁uncommon . ▁the ▁scorekeeper s — called ▁scrutineers — will ▁t ally ▁the ▁total ▁number ▁recalls ▁accumulate d ▁by ▁each ▁couple ▁through ▁each ▁round ▁until ▁the ▁finals ▁when ▁the ▁skat ing ▁system ▁is ▁used ▁to ▁place ▁each ▁couple ▁by ▁ordinals , ▁typically ▁1 –6 , ▁though ▁the [MASK] [MASK] ▁couples ▁in ▁the ▁final ▁may ▁vary . ▁sometimes , ▁up ▁to ▁8 ▁couples ▁may ▁be ▁present ▁on ▁the ▁floor ▁dur ing ▁the ▁finals . ▁competitor s ▁dance ▁at ▁different ▁levels ▁based ▁on ▁the ir ▁ability ▁and ▁experience . ▁the ▁levels ▁are ▁split ▁into ▁two ▁categories , ▁syllabus ▁and ▁open . ▁the ▁syllabus ▁levels ▁are ▁newcomer / pre - bronze , ▁bronze , ▁silver , ▁and ▁gold — with ▁gold ▁the ▁highest ▁syllabus ▁level ▁and ▁newcomer ▁the ▁lowest . [MASK] ▁these ▁levels , ▁moves ▁are ▁restricted ▁to ▁those ▁written ▁in ▁a [MASK] , ▁and ▁illegal ▁moves ▁can ▁lead ▁to ▁dis qualification . ▁each ▁level , ▁bronze , ▁silver , ▁and [MASK] , ▁has ▁different ▁moves ▁on ▁the ir ▁syllabus , ▁increasing ▁in ▁difficulty . ▁there ▁are ▁three ▁levels ▁in ▁the ▁open ▁category ; ▁novice , ▁pre - champ , ▁and ▁champ ▁in ▁increasing ▁order ▁of ▁skill . [MASK] ▁those ▁levels , ▁dancers ▁no ▁longer ▁have ▁restrictions ▁on ▁the ir ▁moves , ▁so ▁complex ▁routines ▁are ▁more ▁common . ▁medal ▁evaluations [MASK] ▁amateur s ▁enable ▁dancers ' ▁individual ▁abilities ▁to ▁be ▁recognized ▁according ▁to ▁conventional ▁standards . ▁in ▁medal ▁evaluations , ▁which ▁are ▁run ▁by ▁bodies ▁such ▁as ▁the ▁imperial ▁society ▁of ▁teachers ▁of ▁dancing ▁( ist d ) ▁and ▁the ▁united ▁kingdom ▁alliance ▁( uka ), ▁each ▁dancer ▁performs ▁two ▁or ▁more ▁dances ▁in ▁a ▁certain ▁genre ▁in ▁front ▁of ▁a ▁judge . ▁genres ▁such ▁as [MASK] ▁ballroom ▁or ▁latin ▁are ▁the ▁most ▁popular . ▁societies ▁such ▁as ▁the ▁is t d ▁and ▁uk a ▁also ▁offer ▁medal ▁tests ▁on ▁other ▁dance ▁styles ▁( such ▁as ▁country ▁& ▁western , ▁rock ▁' n ▁roll ▁or ▁tap ). ▁in ▁some ▁north ▁american ▁examinations , ▁levels ▁include ▁newcomer , ▁bronze , ▁silver , ▁gold , ▁novice , [MASK] [MASK] [MASK] , ▁and ▁championship ; ▁each ▁level ▁may ▁be ▁further ▁subdivide d ▁into [MASK] ▁two [SEP]\n",
            "I0629 21:04:33.874630 140456185767808 create_pretraining_data.py:151] input_ids: 4 5807 8 7 28315 920 1410 8 6 3335 46 28315 8 419 17 287 7 74 17691 302 953 14 12831 7023 75 5863 9 5 920 17 238 479 10 3858 920 1610 27 7 22674 9 12 47 479 8 1265 6703 21 6443 12 5794 11 818 126 13 709 6703 6 59 98 41 4852 30 7 181 355 8 16 2692 12 11 144 10 330 9939 116 4280 9 12 3858 2327 8 6 32 2377 44 27 3232 5910 66 6 35437 8 7 1250 39 7384 8 13275 8 1094 1181 11 2491 8 11775 8 675 14265 11 2746 8 6031 5734 8 5234 11 3559 762 8 11 6340 9 9840 24 12 14 1027 19 3244 1503 17 16790 82 8756 12 792 8 11 6 11 15125 27 7023 15 106 9840 24 10793 15 32 54 7018 9 7 29081 15 249 2310 29713 249 8994 219 1172 7 363 154 22771 9368 44 27 190 3089 172 190 2163 250 7 7211 90 7 24034 24 115 17 86 13 323 190 3089 27 18236 8 818 273 33280 8 319 7 6 6 5794 12 7 542 83 1809 9 438 8 126 13 795 5794 83 38 524 30 7 6031 119 24 7 7211 9 7023 15 920 46 222 1007 257 30 7 78 1629 11 1233 9 7 1007 32 2294 92 93 3523 8 12870 11 519 9 7 12870 1007 32 20401 142 9965 19 36779 8 2529 8 3859 8 11 1924 249 2029 1924 7 1079 12870 541 11 20401 7 3867 9 6 87 1007 8 5170 32 4145 13 220 308 12 14 6 8 11 1925 5170 75 434 13 2395 38060 9 190 541 8 2529 8 3859 8 11 6 8 53 222 5170 30 7 78 12870 8 1421 12 5237 9 85 32 180 1007 12 7 519 3745 60 10115 8 755 19 16585 8 11 24439 12 1421 316 10 3271 9 6 220 1007 8 6703 107 970 51 3540 30 7 78 5170 8 137 650 9882 32 77 247 9 3123 18584 6 4670 15 4553 6703 26 605 5980 13 38 1610 295 13 2721 1725 9 12 3123 18584 8 43 32 751 27 3958 66 20 7 2435 654 10 4503 10 4280 21 1201 44 41 11 7 157 532 1604 21 33712 76 190 15298 6515 93 39 77 2722 12 14 395 1815 12 1346 10 14 2377 9 6509 66 20 6 2327 39 829 32 7 74 346 9 5983 66 20 7 17 117 44 11 1083 42 56 2254 3123 2227 30 65 920 2576 21 1854 20 183 1361 529 8 485 437 55 2509 39 13471 73 12 71 272 235 11834 8 1007 189 20401 8 2529 8 3859 8 1924 8 10115 8 6 6 6 8 11 4219 60 190 541 83 38 439 10658 44 92 6 93 5\n",
            "I0629 21:04:33.969215 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.970193 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 21:04:33.970577 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 8 56 81 89 138 198 199 278 290 291 309 310 346 369 435 494 495 496 497 509\n",
            "I0629 21:04:33.970862 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 2841 46 6703 20 3834 154 10 12 12870 8 1924 8 46 23 239 755 19 37597 8 466\n",
            "I0629 21:04:33.971116 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.971386 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 21:04:33.972855 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.973065 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁sources [SEP] ▁imap ▁edo ▁internet ▁message ▁access ▁protocol ▁internet eko ▁mezuak ▁atzitze ko ▁protokoloa ▁da . ▁posta ▁elektronikoa n ▁posta kutxak ▁kudeatz eko ▁eta ▁hau en ▁mezuak ▁atzitze ko [MASK] [MASK] [MASK] [MASK] ▁ezaugarri ▁nagusi enetakoa ▁mezu ▁karp etak ▁zerbitzaria n ▁gordetze ko ▁aukera ▁ematen ▁duela ▁da . ▁pop ▁protokoloa ren ▁alternatiba ▁bat ▁da . ▁adibidez , ▁bezeroa ri , ▁postontzi ak ▁bere [MASK] ▁propioa n ▁baile uden ▁manipulat zen ▁ uzten ▁dio . ▁hala , ▁bezeroa k ▁edozein ▁tokitatik ▁ikusi ▁ahal ▁izan go ▁ditu magnitude [MASK] ▁gordeta ko [MASK] [MASK] ▁kopia rik ▁egin ▁gabe . ▁lehen , ▁etxean ▁jasotako [MASK] ▁ezin ▁ziren ▁bulegoa n ▁irakurri , ▁esate ▁baterako . [MASK] [MASK] ▁norber e ▁helbidera ▁kopia ▁bat ▁bidali ▁beharra ▁izaten ▁zen . [MASK] [MASK] ▁protokolo ▁honekin , ▁postontzi ak ▁bai [MASK] [MASK] ▁kazakhstan ▁ etxetik , ▁bai ▁beste ▁edozein ▁tokitatik [MASK] [MASK] [MASK] [SEP]\n",
            "I0629 21:04:33.973357 140456185767808 create_pretraining_data.py:151] input_ids: 4 1177 5 22294 463 1039 2071 1148 9232 1039 533 9486 22088 64 9856 108 9 12336 7874 55 12336 38462 34121 533 36 918 99 9486 22088 64 6 6 6 6 5161 2806 31526 7251 31558 19722 13584 55 12060 64 5290 2954 6307 108 9 3831 9856 79 27488 205 108 9 4258 8 25952 629 8 21361 102 400 6 21968 55 42399 31420 39229 2230 16 15648 6688 9 1502 8 25952 68 4608 27865 3463 6457 194 426 1304 37893 6 15258 64 6 6 9484 398 441 2757 9 511 8 14975 14690 6 3077 345 18128 55 12901 8 10816 9571 9 6 6 25648 62 29809 9484 205 3962 12305 6257 105 9 6 6 7806 13128 8 21361 102 3393 6 6 16920 16 42952 8 3393 456 4608 27865 6 6 6 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.973647 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.973892 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.973997 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 30 31 32 33 64 86 87 90 91 101 111 112 123 124 131 132 133 141 142 143\n",
            "I0629 21:04:33.974085 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 9856 9 22294 99 9896 21361 990 9486 8 9486 7003 8 6084 8 16646 476 3393 2079 4246 9\n",
            "I0629 21:04:33.974180 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.974255 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:04:33.975309 140456185767808 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 21:04:33.975535 140456185767808 create_pretraining_data.py:141] tokens: [CLS] ▁on ▁25 ▁july ▁1137 , [MASK] ▁and ▁louis [MASK] ▁married ▁in ▁the ▁cathedral ▁of ▁saint - andré ▁in ▁(330–390 ▁by ▁the ▁archbishop ▁of [MASK] [MASK] ▁immediately ▁after ▁the ▁wedding , ▁the [MASK] ▁were ▁enth ron ed ▁as [MASK] [MASK] ▁duchess ▁of ▁aquitaine . [MASK] [MASK] [MASK] ▁was ▁a ▁catch : ▁the ▁land ▁would ▁remain ▁independent ▁of ▁france ▁until ▁eleanor ' s ▁oldest ▁son ▁became ▁both [MASK] ▁of ▁france ▁and ▁duke ▁of ▁aquitaine . ▁thus , ▁her ▁holdings ▁would ▁not ▁be ▁ merged ▁with ▁france ▁until [MASK] ▁next ▁generation . ▁as ▁a ▁wedding ▁present ▁she ▁gave ▁louis ▁a ▁rock ▁crystal ▁vas e ▁ , ▁currently ▁on ▁display ▁at [MASK] ▁louvre . ▁louis ▁gave ▁the ▁vas e ▁to ▁the ▁basilica ▁of ▁st ▁denis . ▁this ▁vas e ▁is ▁the ▁only ▁object ▁connected ▁with ▁eleanor ▁of ▁aquitaine ▁that ▁still ▁survives . ▁louis ' s ▁tenure [MASK] ▁count ▁of ▁poitou ▁and ▁duke ▁of ▁aquitaine ▁and ▁gas cony ▁last ed ▁only [MASK] ▁few ▁days . ▁although ▁he ▁had ▁been ▁invested ▁as ▁such ▁on ▁8 ▁august ▁1137 , ▁a ▁messenger ▁gave ▁him ▁the ▁news ▁that ▁louis ▁vi ▁had ▁died [MASK] ▁dysentery ▁on ▁1 ▁august ▁while ▁he ▁and ▁eleanor ▁were [MASK] ▁a ▁tour [MASK] ▁the ▁provinces . ▁he ▁and ▁eleanor ▁were ▁an ointed ▁and ▁crown ed ▁king ▁and ▁queen ▁of ▁france ▁on ▁christmas ▁day ▁of ▁the ▁same ▁year . [SEP] ▁redirected from [SEP]\n",
            "I0629 21:04:33.975827 140456185767808 create_pretraining_data.py:151] input_ids: 4 30 1002 960 14784 8 6 11 602 6 1532 12 7 5613 10 789 19 37448 12 27040 27 7 4767 10 6 6 2131 81 7 8046 8 7 6 49 16309 8356 31 20 6 6 8844 10 4115 9 6 6 6 22 14 4542 45 7 337 103 1595 929 10 200 250 531 26 15 2767 538 159 134 6 10 200 11 2037 10 4115 9 341 8 139 15725 103 54 38 16 6503 29 200 250 6 716 2670 9 20 14 8046 524 228 766 602 14 485 13685 13533 62 16 8 1330 30 3418 46 6 28711 9 602 766 7 13533 62 13 7 10253 10 443 15760 9 47 13533 62 17 7 100 1176 1534 29 531 10 4115 25 303 10694 9 602 26 15 8825 6 1800 10 14486 11 2037 10 4115 11 601 43234 312 31 100 6 389 743 9 188 50 57 61 10698 20 66 30 795 879 14784 8 14 16718 766 160 7 2422 25 602 1774 57 932 6 11268 30 273 879 116 50 11 531 49 6 14 2680 6 7 2207 9 50 11 531 49 37 11233 11 2930 31 245 11 1205 10 200 30 5454 332 10 7 181 179 9 5 29350 2654 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.976067 140456185767808 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.976296 140456185767808 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 21:04:33.976391 140456185767808 create_pretraining_data.py:151] masked_lm_positions: 6 9 19 24 25 32 38 39 44 45 46 66 86 108 119 143 157 184 194 197\n",
            "I0629 21:04:33.976480 140456185767808 create_pretraining_data.py:151] masked_lm_ids: 531 49 5898 5898 9 3089 2037 11 111 8 85 245 7 7 10 20 14 10 587 10\n",
            "I0629 21:04:33.976583 140456185767808 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 21:04:33.976662 140456185767808 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 21:05:10.505954 140456185767808 create_pretraining_data.py:156] Wrote 44747 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p8b1EVZ79Ub",
        "colab_type": "code",
        "outputId": "58e3763f-fcc2-42dc-a547-326ebab63ee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python src/run_pretraining.py \\\n",
        "  --config_file en-eu.config.ini \\\n",
        "  --input_file=$GS/pretraining-en_eu.tf.data \\\n",
        "  --output_dir=$GS/en-eu.gureBERT \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=20000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  --init_checkpoint=$GS/en-eu.gureBERT/ \\\n",
        "  #--num_train_steps=1000000 \\\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 21:44:43.035492 140659881912192 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 21:44:43.038059 140659881912192 deprecation_wrapper.py:119] From src/run_pretraining.py:498: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 21:44:43.038740 140659881912192 deprecation_wrapper.py:119] From src/run_pretraining.py:413: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 21:44:43.038905 140659881912192 deprecation_wrapper.py:119] From src/run_pretraining.py:413: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 21:44:43.039040 140659881912192 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 21:44:43.039786 140659881912192 deprecation_wrapper.py:119] From src/run_pretraining.py:420: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0629 21:44:44.391881 140659881912192 deprecation_wrapper.py:119] From src/run_pretraining.py:424: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 21:44:44.557700 140659881912192 deprecation_wrapper.py:119] From src/run_pretraining.py:426: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 21:44:44.557963 140659881912192 run_pretraining.py:426] *** Input Files ***\n",
            "I0629 21:44:44.558057 140659881912192 run_pretraining.py:428]   gs://gurebert/gureBERT/pretraining-en_eu.tf.data\n",
            "W0629 21:44:45.674227 140659881912192 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 21:44:46.681165 140659881912192 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fedb82fad08>) includes params argument, but params are not passed to Estimator.\n",
            "I0629 21:44:46.682844 140659881912192 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/en-eu.gureBERT', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.2.81.2:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fedc47835f8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.2.81.2:8470', '_evaluation_master': 'grpc://10.2.81.2:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fedc477ab70>}\n",
            "I0629 21:44:46.683385 140659881912192 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0629 21:44:46.684198 140659881912192 run_pretraining.py:465] ***** Running training *****\n",
            "I0629 21:44:46.684307 140659881912192 run_pretraining.py:466]   Batch size = 64\n",
            "I0629 21:44:46.999361 140659881912192 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.2.81.2:8470) for TPU system metadata.\n",
            "2019-06-29 21:44:47.000899: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 21:44:47.015257 140659881912192 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0629 21:44:47.015493 140659881912192 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0629 21:44:47.015652 140659881912192 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0629 21:44:47.015725 140659881912192 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0629 21:44:47.015800 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5201560055092340784)\n",
            "I0629 21:44:47.016601 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15235922008977912019)\n",
            "I0629 21:44:47.016680 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16587921834020422802)\n",
            "I0629 21:44:47.016747 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 18249457006028921626)\n",
            "I0629 21:44:47.016820 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9769923856279949422)\n",
            "I0629 21:44:47.016886 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14707561106018069366)\n",
            "I0629 21:44:47.016950 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11546342222793515035)\n",
            "I0629 21:44:47.017010 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15225485262720068302)\n",
            "I0629 21:44:47.017070 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 18107303912184056565)\n",
            "I0629 21:44:47.017129 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12015888753985987776)\n",
            "I0629 21:44:47.017191 140659881912192 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6671482654539601268)\n",
            "W0629 21:44:47.024765 140659881912192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0629 21:44:47.041332 140659881912192 estimator.py:1145] Calling model_fn.\n",
            "W0629 21:44:47.042061 140659881912192 deprecation_wrapper.py:119] From src/run_pretraining.py:343: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0629 21:44:47.049173 140659881912192 deprecation.py:323] From src/run_pretraining.py:374: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0629 21:44:47.049446 140659881912192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0629 21:44:47.080957 140659881912192 deprecation.py:323] From src/run_pretraining.py:391: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0629 21:44:47.081236 140659881912192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0629 21:44:47.083138 140659881912192 deprecation_wrapper.py:119] From src/run_pretraining.py:399: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0629 21:44:47.090007 140659881912192 deprecation.py:323] From src/run_pretraining.py:406: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0629 21:44:47.185767 140659881912192 run_pretraining.py:123] *** Features ***\n",
            "I0629 21:44:47.186055 140659881912192 run_pretraining.py:125]   name = input_ids, shape = (8, 512)\n",
            "I0629 21:44:47.186162 140659881912192 run_pretraining.py:125]   name = input_mask, shape = (8, 512)\n",
            "I0629 21:44:47.186248 140659881912192 run_pretraining.py:125]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0629 21:44:47.186331 140659881912192 run_pretraining.py:125]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0629 21:44:47.186415 140659881912192 run_pretraining.py:125]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0629 21:44:47.186496 140659881912192 run_pretraining.py:125]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0629 21:44:47.186591 140659881912192 run_pretraining.py:125]   name = segment_ids, shape = (8, 512)\n",
            "W0629 21:44:47.186859 140659881912192 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0629 21:44:47.189514 140659881912192 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0629 21:44:47.231752 140659881912192 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0629 21:44:47.437777 140659881912192 deprecation.py:506] From /content/gureBERT/src/../bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0629 21:44:47.461791 140659881912192 deprecation.py:323] From /content/gureBERT/src/../bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0629 21:44:51.759648 140659881912192 run_pretraining.py:173] **** Trainable Variables ****\n",
            "I0629 21:44:51.759918 140659881912192 run_pretraining.py:179]   name = bert/embeddings/word_embeddings:0, shape = (45000, 768)\n",
            "I0629 21:44:51.760065 140659881912192 run_pretraining.py:179]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0629 21:44:51.760159 140659881912192 run_pretraining.py:179]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0629 21:44:51.760246 140659881912192 run_pretraining.py:179]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.760340 140659881912192 run_pretraining.py:179]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.760423 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.760527 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.760628 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.760710 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.760787 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.760866 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.760941 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.761018 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.761092 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.761165 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.761237 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.761322 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.761397 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.761474 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.761548 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.761638 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.761712 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.761792 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.761866 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.761944 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.762018 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.762096 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.762169 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.762245 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.762326 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.762399 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.762472 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.762561 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.762638 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.762717 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.762792 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.762866 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.762939 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.763016 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.763089 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.763169 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.763242 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.763341 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.763419 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.763498 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.763586 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.763664 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.763737 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.763817 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.763894 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.763971 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.764044 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.764117 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.764189 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.764275 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.764352 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.764430 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.764504 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.764595 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.764670 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.764747 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.764822 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.764896 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.764969 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.765048 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.765122 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.765199 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.765280 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.765356 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.765448 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.765525 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.765613 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.765690 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.765765 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.765844 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.765918 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.765995 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.766068 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.766140 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.766213 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.766298 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.766374 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.766471 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.766547 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.766637 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.766712 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.766791 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.766865 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.766944 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.767018 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.767096 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.767169 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.767278 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.767354 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.767427 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.767500 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.767590 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.767666 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.767742 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.767818 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.767894 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.767967 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.768044 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.768119 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.768195 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.768274 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.768353 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.768427 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.768504 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.768592 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.768667 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.768741 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.768828 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.768904 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.768980 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.769055 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.769129 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.769201 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.769283 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.769356 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.769434 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.769509 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.829059 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.829411 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.829579 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.829702 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.829810 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.829917 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.830026 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.830129 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.830232 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.830355 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.830461 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.830581 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.830687 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.830789 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.830903 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.831008 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.831115 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.831218 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.831340 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.831445 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.831548 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.831685 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.831799 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.831902 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.832009 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.832112 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.832217 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.832329 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.832439 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.832542 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.832673 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.832777 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.832888 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.832994 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.833100 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.833202 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.833314 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.833421 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.833528 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.833661 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.833773 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.833881 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.833983 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.834085 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.834195 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.834309 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.834419 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.834523 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.834653 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.834755 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.834865 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.834969 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.835071 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.835170 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.835285 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.835385 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.835493 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.835616 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.835718 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.835821 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.835932 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 21:44:51.836034 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.836140 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 21:44:51.836244 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.836371 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 21:44:51.836475 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.836603 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.836713 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.836818 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.836919 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 21:44:51.837027 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 21:44:51.837131 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 21:44:51.837238 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.837357 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.837460 140659881912192 run_pretraining.py:179]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.837580 140659881912192 run_pretraining.py:179]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.837712 140659881912192 run_pretraining.py:179]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.837818 140659881912192 run_pretraining.py:179]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0629 21:44:51.837927 140659881912192 run_pretraining.py:179]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0629 21:44:51.838034 140659881912192 run_pretraining.py:179]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 21:44:51.838136 140659881912192 run_pretraining.py:179]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 21:44:51.838236 140659881912192 run_pretraining.py:179]   name = cls/predictions/output_bias:0, shape = (45000,)\n",
            "I0629 21:44:51.838350 140659881912192 run_pretraining.py:179]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0629 21:44:51.838460 140659881912192 run_pretraining.py:179]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0629 21:44:51.838682 140659881912192 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0629 21:44:51.840614 140659881912192 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0629 21:44:51.849527 140659881912192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0629 21:44:52.169674 140659881912192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0629 21:45:06.976102 140659881912192 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0629 21:45:07.672245 140659881912192 estimator.py:1147] Done calling model_fn.\n",
            "I0629 21:45:11.239114 140659881912192 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 21:45:12.703834 140659881912192 monitored_session.py:240] Graph was finalized.\n",
            "I0629 21:45:25.319812 140659881912192 session_manager.py:500] Running local_init_op.\n",
            "I0629 21:45:26.470335 140659881912192 session_manager.py:502] Done running local_init_op.\n",
            "I0629 21:45:39.141649 140659881912192 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://gurebert/gureBERT/en-eu.gureBERT/model.ckpt.\n",
            "W0629 21:46:11.694537 140659881912192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0629 21:46:14.025953 140659881912192 util.py:98] Initialized dataset iterators in 1 seconds\n",
            "I0629 21:46:14.027035 140659881912192 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-06-29 21:46:14.027386: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 21:46:14.032509 140659881912192 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0629 21:46:14.034853 140659881912192 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0629 21:46:14.038425 140659881912192 tpu_estimator.py:557] Init TPU system\n",
            "I0629 21:46:21.772088 140659881912192 tpu_estimator.py:566] Initialized TPU in 7 seconds\n",
            "I0629 21:46:21.774013 140658731775744 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 21:46:21.775388 140658705037056 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 21:46:22.930474 140659881912192 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 21:46:22.931644 140659881912192 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 21:46:58.849603 140658705037056 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 21:47:59.043489 140658705037056 tpu_estimator.py:275] Outfeed finished for iteration (0, 239)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne5xtoXYgOvM",
        "colab_type": "text"
      },
      "source": [
        "## SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxSQy-Gu47Bq",
        "colab_type": "code",
        "outputId": "4c140dce-315b-4ad3-b4ec-9217b62f301c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "# pre training SQuAD\n",
        "\n",
        "# https://github.com/google-research/bert#squad-20\n",
        "\n",
        "#!git clone https://github.com/zmwebdev/bert.git\n",
        "#%cd bert\n",
        "\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
        "# evaluation script: download file from a url that returns a save dialog box : https://superuser.com/questions/795265/download-file-from-a-url-that-returns-a-save-dialog-box#795269\n",
        "!wget --content-disposition https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-30 08:14:35--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.108.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "\rtrain-v2.0.json       0%[                    ]       0  --.-KB/s               \rtrain-v2.0.json      68%[============>       ]  27.56M   138MB/s               \rtrain-v2.0.json     100%[===================>]  40.17M   155MB/s    in 0.3s    \n",
            "\n",
            "2019-06-30 08:14:36 (155 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n",
            "--2019-06-30 08:14:37--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.108.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "dev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-06-30 08:14:37 (49.8 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n",
            "--2019-06-30 08:14:38--  https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
            "Resolving worksheets.codalab.org (worksheets.codalab.org)... 40.71.231.153\n",
            "Connecting to worksheets.codalab.org (worksheets.codalab.org)|40.71.231.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/x-python]\n",
            "Saving to: ‘evaluate-v2.0.py’\n",
            "\n",
            "evaluate-v2.0.py        [ <=>                ]  10.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-06-30 08:14:38 (194 MB/s) - ‘evaluate-v2.0.py’ saved [10547]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA0oUJsbBTaW",
        "colab_type": "code",
        "outputId": "d78574f8-5536-4aa4-8aa9-123bad78fd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        }
      },
      "source": [
        "# sentenpiece tokenizer erabili behar da!!: src/run_squad.py\n",
        "\n",
        "!gsutil cp -r gs://gurebert/gureBERT/spModels .\n",
        "\n",
        " \n",
        "!python src/run_squad.py \\\n",
        "  --vocab_file=./spModels/en-eu.vocab \\\n",
        "  --model_file=./spModels/en-eu.model \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --do_lower_case=True \\\n",
        "  --do_train=True \\\n",
        "  --train_file=train-v2.0.json \\\n",
        "  --do_predict=True \\\n",
        "  --predict_file=dev-v2.0.json \\\n",
        "  --train_batch_size=24 \\\n",
        "  --learning_rate=3e-5 \\\n",
        "  --num_train_epochs=0.1 \\\n",
        "  --max_seq_length=384 \\\n",
        "  --doc_stride=128 \\\n",
        "  --output_dir=gs://gurebert/gureBERT/squad/ \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name=$TPU_NAME \\\n",
        "  --version_2_with_negative=True \\\n",
        "  --init_checkpoint=gs://gurebert/gureBERT/squad/ \\\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0630 08:21:13.292181 140305218697088 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0630 08:21:13.294131 140305218697088 deprecation_wrapper.py:119] From src/run_squad.py:1293: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0630 08:21:13.294800 140305218697088 deprecation_wrapper.py:119] From src/run_squad.py:1137: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0630 08:21:13.294953 140305218697088 deprecation_wrapper.py:119] From src/run_squad.py:1137: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0630 08:21:13.295083 140305218697088 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0630 08:21:13.296034 140305218697088 deprecation_wrapper.py:119] From src/run_squad.py:1143: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "Loaded a trained SentencePiece model.\n",
            "W0630 08:21:15.794102 140305218697088 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0630 08:21:16.800096 140305218697088 deprecation_wrapper.py:119] From src/run_squad.py:239: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/run_squad.py\", line 1293, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"src/run_squad.py\", line 1169, in main\n",
            "    input_file=FLAGS.train_file, is_training=True)\n",
            "  File \"src/run_squad.py\", line 296, in read_squad_examples\n",
            "    tokenization.whitespace_tokenize(orig_answer_text))\n",
            "AttributeError: module 'tokenization_sentencepiece' has no attribute 'whitespace_tokenize'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lablZa6fsh51",
        "colab_type": "text"
      },
      "source": [
        "## wordpiece erabiliz\n",
        "\n",
        "- https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379\n",
        "- https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e5XE43Q7D9i",
        "colab_type": "text"
      },
      "source": [
        "### EU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2XwxN-Z4a3m",
        "colab_type": "code",
        "outputId": "b708fe4d-623a-41e9-b591-a66b85bdeeb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format('./spModels/eu'))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 14705\n",
            "Sample tokens: ['emakumezko', '▁erla', 'olof', '▁40', 'ulara', '▁abiarazle', '▁uploadlogpage', '▁belle', '▁arazoen', '▁aipamena']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-YOATBGsg02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hau errepasatu. adibidez @ ez dut ondo egiten... [UNK] agertzen da...\n",
        "# adibidez, \n",
        "\n",
        "#import string\n",
        "#def parse_sentencepiece_token(token):\n",
        "#    if token.startswith(\"▁\"):\n",
        "#        return token[1:]\n",
        "#    else:\n",
        "#        if token in string.punctuation:\n",
        "#            return token\n",
        "#        else:\n",
        "#            return \"##\" + token\n",
        "          \n",
        "\n",
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYEtWr0D39Rf",
        "colab_type": "code",
        "outputId": "2dec5177-3053-4434-e716-fb56c2886ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import string\n",
        "\n",
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "# puntuazio sinboloak gehitu \n",
        "ctrl_symbols_end = list(string.punctuation)\n",
        "#ctrl_symbols = [\"[UNK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab + ctrl_symbols_end\n",
        "\n",
        "#bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(32000 - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiGAWsMw4CLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\"\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLLaP8_04uY4",
        "colab_type": "code",
        "outputId": "8b2c297d-1016-41a0-835a-540e6c1b71cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "from bert import tokenization\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(\"Nere kotxea aitonaren etxe alboan dago!, eta bere kolorea gorria da. ni@ni.eus erabili \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ne',\n",
              " '##re',\n",
              " 'ko',\n",
              " '##txea',\n",
              " 'ait',\n",
              " '##ona',\n",
              " '##ren',\n",
              " 'etxe',\n",
              " 'alb',\n",
              " '##oan',\n",
              " 'dago',\n",
              " '!',\n",
              " ',',\n",
              " 'eta',\n",
              " 'bere',\n",
              " 'kolore',\n",
              " '##a',\n",
              " 'gorria',\n",
              " 'da',\n",
              " '.',\n",
              " 'ni',\n",
              " '@',\n",
              " 'ni',\n",
              " '.',\n",
              " 'eus',\n",
              " 'erabili']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn54GJq95Uf7",
        "colab_type": "code",
        "outputId": "1ef864b3-7b3b-4c77-cff7-d9b52e5ac679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python bert/create_pretraining_data.py \\\n",
        "  --input_file=$GS/corpus/eu/2014wiki.eu.sent_splited \\\n",
        "  --output_file=$GS/wordpiece/pretraining.tf.data \\\n",
        "  --vocab_file=vocab.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=666 \\\n",
        "  --do_whole_word_mask=True \\\n",
        "  #--dupe_factor=5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 14:27:51.331921 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 14:27:51.332679 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 14:27:51.332850 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 14:27:51.332988 140698090002304 deprecation_wrapper.py:119] From /content/gureBERT/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 14:27:51.387063 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 14:27:52.573521 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 14:27:52.573760 140698090002304 create_pretraining_data.py:446] *** Reading from input files ***\n",
            "I0629 14:27:52.573846 140698090002304 create_pretraining_data.py:448]   gs://gurebert/gureBERT/corpus/eu/2014wiki.eu.sent_splited\n",
            "I0629 14:28:04.601530 140698090002304 create_pretraining_data.py:457] *** Writing to output files ***\n",
            "I0629 14:28:04.601782 140698090002304 create_pretraining_data.py:459]   gs://gurebert/gureBERT/wordpiece/pretraining.tf.data\n",
            "W0629 14:28:04.601961 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0629 14:28:04.603364 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.603535 140698090002304 create_pretraining_data.py:151] tokens: [CLS] [MASK] [SEP] disambiguations [SEP]\n",
            "I0629 14:28:04.603769 140698090002304 create_pretraining_data.py:161] input_ids: 2 4 3 4887 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.603982 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.604194 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.604281 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.604363 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 496 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.604471 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.604547 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.605411 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.605550 140698090002304 create_pretraining_data.py:151] tokens: [CLS] bai , datu - basea blokeatu nahi dut . [SEP] kirolak [MASK] [MASK] heriotzak * jamaika [MASK] 1 ##a - mart ##zel ##o ii . a aita santua [MASK] [MASK] [MASK] hilabete baino gutxiago eman ostean . * maiatz ##aren 25 [MASK] enrike ii . [MASK] , nafarroako erregea . - - - - agintari ##ak [SEP]\n",
            "I0629 14:28:04.605771 140698090002304 create_pretraining_data.py:161] input_ids: 2 431 14721 340 14722 9946 952 135 324 14723 3 74 4 4 70 14719 7652 4 218 16 14722 6308 3233 39 271 14723 77 274 940 4 4 4 1233 192 1666 139 804 14723 14719 318 27 356 4 5495 271 14723 4 14721 585 531 14723 14722 14722 14722 14722 94 23 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.605977 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.606178 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.606264 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 12 13 16 17 29 30 31 42 46 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.606343 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 86 20 318 27 14721 3204 22 234 77 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.606447 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.606524 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.607523 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.607639 140698090002304 create_pretraining_data.py:151] tokens: [CLS] colum ##ns [SEP] ezin izan zaio \" [MASK] 1 \" fitxategia ##ri \" $ 2 \" [MASK] berria [MASK] . [SEP]\n",
            "I0629 14:28:04.607862 140698090002304 create_pretraining_data.py:161] input_ids: 2 3772 2827 3 367 35 664 14711 4 218 14711 1212 83 14711 14713 422 14711 4 408 4 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.608070 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.608278 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.608363 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 8 17 19 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.608461 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 14713 655 139 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.608549 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.608622 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.609474 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.609719 140698090002304 create_pretraining_data.py:151] tokens: [CLS] musika , bitarteko ##tzat soinua eta isiltasuna darabilt ##zan artea [MASK] . musika sortze ##n duen pertsona ##ri musikagile deitze ##n zaio ; musika jotze ##n edo abest ##en duenari , musikari ; eta musika aztertzen duenari , musik ##ologo . historia musika soinu - konbina ##zio edo - [MASK] da ; soinua ##k giza organo fon ##atzaileek ( ah ##ot ##s musika ) edota horretarako diseinatu ##riko musika tresne ##k sortua ##k ( musika instrumentala ) dira , zibilizazio batetik bestera aldatzen diren printzipio teknikoe ##n arabera landu ##ak [MASK] antolatua ##k . herri musika ##ren jatorria historiaurrea ##n du ##go . segur aski , hasiera batean erlijioa ##ri [MASK] fax lotur ##ik izan ##go zen , eta egin ##kizun estetikoa baino [MASK] [MASK] ##ko - sin ##boli ##koa izan ##go zuen . musika [MASK] hori talde etniko [MASK] kultura ##l guztiek dute , gehienbat ahozko tradizio ##z transmitit ##zen da , eta [MASK] garapena ##k eta eboluzioa ##k bide desberdina ##k jarraitu dituzte zibilizazio bakoitzean , baina , betiere , teknikoki nahi ##z instrumental ##ki sinplea izaten da , eta gai jakin baten inguruan bar ##ia ##zioak inp ##ro ##bis ##atzeko aukera ematen [MASK] . musika horren barruan sartze ##n dira , hasi tribu primiti ##bo ##en abesti ##etatik eta [MASK] egung ##o jazz ##eraino [MASK] herrialde desberdineta ##ko folklor ##ea [MASK] [MASK] duten manifestazio musika ##l guztieta ##tik igarot ##a . mendebaldean , musika jaso ##aren garapena grezian eta erroman hasi zen , eta handik europa osora zabaldu zen , bi bide erabiliz : musika prof ##anoa eta erlijiosoa . azken hori , kristautasuna ##ren heda ##pena ##ri zuzenean lotua . gregoriar kantua vi [MASK] mendean sortu zen . jarraian etorri zen , vii . mendean , polifoni ##a ( grekot ##ik dator hitza , eta « ah ##ot ##s asko [UNK] esan nahi du ) . pentag ##rama edo paper pau ##tatuan musika idazteko modua xi . mendean asmatu zen . [SEP] martxoa ##ren 2 [SEP]\n",
            "I0629 14:28:04.658286 140698090002304 create_pretraining_data.py:161] input_ids: 2 162 14721 5387 368 11044 14 7069 9173 3956 6427 4 14723 162 1190 22 134 267 83 1423 1451 22 664 14726 162 10470 22 66 13817 25 5601 14721 849 14726 14 162 487 5601 14721 5923 5562 14723 146 162 3200 14722 12807 937 66 14722 4 28 14726 11044 20 1076 5933 5874 12469 14717 14074 1535 47 162 14718 874 1130 5474 994 162 4731 20 9133 20 14717 162 8188 14718 41 14721 1620 1513 4289 1864 156 3972 13449 22 212 1647 23 4 10920 20 14723 243 162 21 1532 5814 22 69 60 14723 10924 5485 14721 610 127 3310 83 4 11140 14310 190 35 60 24 14721 14 63 1762 7548 192 4 4 17 14722 1275 11769 786 35 60 30 14723 162 4 128 237 4790 4 58 205 924 120 14721 2525 2826 3436 45 4995 220 28 14721 14 4 3332 20 14 1964 20 837 3363 20 968 282 1620 3823 14721 92 14721 3763 14721 7725 135 45 11968 440 11138 983 28 14721 14 283 670 177 675 887 186 6741 6391 745 5808 2082 967 360 4 14723 162 428 1239 5728 22 41 14721 96 1352 7335 2650 25 6995 420 14 4 6557 39 4059 12600 4 355 10609 17 7504 947 4 4 236 1851 162 205 1613 59 10155 16 14723 1068 14721 162 1147 27 3332 4069 14 2189 96 24 14721 14 2687 429 9688 462 24 14721 99 837 818 14725 162 10013 14118 14 3354 14723 160 128 14721 2202 21 4791 10883 83 1884 4047 14723 9607 4336 573 4 329 98 24 14723 1242 900 24 14721 825 14723 329 14721 6807 16 14717 14145 190 2433 1085 14721 14 737 14074 1535 47 284 1 270 135 69 14718 14723 8763 10346 66 3922 3467 13283 162 5428 6771 1490 14723 329 991 24 14723 3 514 21 422 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.658822 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.659170 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.659320 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 11 50 91 111 112 124 125 126 136 140 145 155 196 213 218 224 225 278 283 287\n",
            "I0629 14:28:04.659469 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 28 5356 14 164 10817 6342 5963 17 489 14 14721 53 69 257 14721 681 104 14723 1242 825\n",
            "I0629 14:28:04.659601 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:28:04.659718 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.660927 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.661057 140698090002304 create_pretraining_data.py:151] tokens: [CLS] erakutsi azken $ [MASK] aldaketak [MASK] 2 egun ##etan . [SEP] image ##page [SEP]\n",
            "I0629 14:28:04.661310 140698090002304 create_pretraining_data.py:161] input_ids: 2 701 160 14713 4 612 4 422 56 87 14723 3 2554 562 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.661542 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.661763 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.661859 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 4 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.661940 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 218 14713 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.662024 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.662094 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.662963 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.663069 140698090002304 create_pretraining_data.py:151] tokens: [CLS] image ##revert ##ed [SEP] delete ##comment [SEP]\n",
            "I0629 14:28:04.663285 140698090002304 create_pretraining_data.py:161] input_ids: 2 2554 11857 661 3 3358 4718 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.663508 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.663717 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.663802 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.663882 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.663965 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.664037 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.664889 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.664994 140698090002304 create_pretraining_data.py:151] tokens: [CLS] head ##line [MASK] [SEP] text ##match ##es [SEP]\n",
            "I0629 14:28:04.665207 140698090002304 create_pretraining_data.py:161] input_ids: 2 4011 2311 4 3 8907 2538 199 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.665446 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.665664 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.665754 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.665832 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 1277 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.665914 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.665985 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.666849 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.666951 140698090002304 create_pretraining_data.py:151] tokens: [CLS] ipb ##lock ##list [SEP] ara ##katu [SEP]\n",
            "I0629 14:28:04.667162 140698090002304 create_pretraining_data.py:161] input_ids: 2 4135 11740 2344 3 11549 6458 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.667375 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.667593 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.761057 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.761304 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.761462 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.761572 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.763029 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.763186 140698090002304 create_pretraining_data.py:151] tokens: [CLS] old ##password [SEP] [MASK] helbide ##rik ez [SEP]\n",
            "I0629 14:28:04.763431 140698090002304 create_pretraining_data.py:161] input_ids: 2 9686 2380 3 4 1274 52 36 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.763645 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.763862 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.763949 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.764030 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 2263 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.764119 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.764193 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.765066 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.765181 140698090002304 create_pretraining_data.py:151] tokens: [CLS] next ##n [SEP] kirolak [MASK] [MASK] * uztaila ##ren [MASK] ##a - giorg ##io vas ##ari italiar margolari eta arkitektoa . heriotzak - - - - agintari ##ak [SEP]\n",
            "I0629 14:28:04.765411 140698090002304 create_pretraining_data.py:161] input_ids: 2 8068 22 3 74 4 4 14719 773 21 4 16 14722 5124 526 6551 153 438 605 14 2668 14723 70 14722 14722 14722 14722 94 23 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.765640 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.765848 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.765938 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 5 6 10 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.766017 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 86 20 430 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.766101 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.766174 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.767030 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.767133 140698090002304 create_pretraining_data.py:151] tokens: [CLS] gertaerak [SEP] [MASK] [SEP]\n",
            "I0629 14:28:04.767348 140698090002304 create_pretraining_data.py:161] input_ids: 2 72 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.767568 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.767780 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.767867 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.767947 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 14719 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.768030 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.768105 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 14:28:04.768952 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.769061 140698090002304 create_pretraining_data.py:151] tokens: [CLS] orrialde bereziak [SEP] izen - [MASK] hauetan [MASK] : $ 1 $ 2 birzuzenkete ##n zerrenda $ 3 $ 9 1690) [SEP]\n",
            "I0629 14:28:04.769275 140698090002304 create_pretraining_data.py:161] input_ids: 2 189 3137 3 655 14722 4 1678 4 14725 14713 218 14713 422 14304 22 178 14713 187 14713 686 8687 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.769495 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.769707 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.864479 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 6 8 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.864718 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 5994 1645 1645 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.864897 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.865170 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.869458 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.869652 140698090002304 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] by title [SEP] heriotzak - - - - agintari ##ak [SEP]\n",
            "I0629 14:28:04.870024 140698090002304 create_pretraining_data.py:161] input_ids: 2 4 4 1539 9625 3 70 14722 14722 14722 14722 94 23 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.870371 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.870724 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.870842 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.870944 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 2272 9080 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.871060 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.871155 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.872426 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.872573 140698090002304 create_pretraining_data.py:151] tokens: [CLS] unblockip [SEP] zure hobespenak gorde dira [MASK] [SEP]\n",
            "I0629 14:28:04.872892 140698090002304 create_pretraining_data.py:161] input_ids: 2 7403 3 474 2102 883 41 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.873401 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.874418 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.874642 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.874773 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 14723 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.874881 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.874965 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.876312 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.876444 140698090002304 create_pretraining_data.py:151] tokens: [CLS] bol ##d tip [SEP] [MASK] [SEP]\n",
            "I0629 14:28:04.876710 140698090002304 create_pretraining_data.py:161] input_ids: 2 2860 232 1277 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.877016 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.877262 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.877355 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.877458 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 1500 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.877539 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.877607 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.878518 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.878636 140698090002304 create_pretraining_data.py:151] tokens: [CLS] watchnologin ##text [SEP] aldaketa [MASK] [SEP]\n",
            "I0629 14:28:04.878882 140698090002304 create_pretraining_data.py:161] input_ids: 2 7339 191 3 393 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.879196 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.879544 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.970041 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.970332 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 535 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.970508 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.970775 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.972299 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.972450 140698090002304 create_pretraining_data.py:151] tokens: [CLS] ext ##link samp ##le [SEP] notargett ##itle [SEP]\n",
            "I0629 14:28:04.972678 140698090002304 create_pretraining_data.py:161] input_ids: 2 11291 1596 2341 286 3 7699 2024 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.972888 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.973093 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.973177 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.973278 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.973367 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.973464 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.974359 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.974545 140698090002304 create_pretraining_data.py:151] tokens: [CLS] [MASK] eta teknologia kirolak jaiotza ##k euskal [MASK] mundua [MASK] 1820 - william te ##cum ##seh sher ##man , sistematiko militarra , [MASK] [MASK] eta idazlea ( h . [SEP] 1891 ) . * 1828 - [MASK] [MASK] verne [MASK] idazle frantziarra * 1828 - antoni ##o can ##ova ##s del cast ##illo espainiar politikaria . * 1925 - jack lem ##mon , aktore [MASK] . [MASK] 1931 - james dean , [MASK] estatubatuarra . heriotzak euskal herria * 2012 - jose lui ##s aka ##rre ##gi , euskal pilotaria ( j . 1923 [MASK] . ##sada * 1921 - pio ##tr kro ##po ##t ##kin , anarkista errusiarra ( j [MASK] [MASK] ) . jaiak eta urteurrena ##k * se ##haska egutegi ##ko [MASK] [MASK] : x ##ant ##ian ##a eta ustaritz / ustaritz ##e . [SEP]\n",
            "I0629 14:28:04.974777 140698090002304 create_pretraining_data.py:161] input_ids: 2 4 14 71 74 86 20 38 4 57 4 4615 14722 417 966 4357 8704 9982 503 14721 11231 768 14721 4 4 14 97 14717 151 14723 3 1267 14718 14723 14719 6649 14722 4 4 6073 4 89 695 14719 6649 14722 813 39 4736 14291 47 1428 9954 11505 399 265 14723 14719 1563 14722 771 6385 4686 14721 110 4 14723 4 693 14722 262 4850 14721 4 114 14723 70 38 55 14719 312 14722 196 662 47 14593 10405 1118 14721 38 3062 14717 195 14723 4505 4 14723 10663 14719 1553 14722 1918 4220 3375 1971 118 982 14721 7098 863 14717 195 4 4 14718 14723 172 14 167 20 14719 112 170 90 17 4 4 14725 484 2800 1747 16 14 8286 14724 8286 44 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.974992 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.975207 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.975294 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 1 2 8 10 20 23 24 37 38 40 60 65 67 73 95 97 112 113 125 126\n",
            "I0629 14:28:04.975395 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 54 14 55 14719 103 4381 2224 6714 47 14721 771 114 14719 110 14718 57 14723 11397 176 161\n",
            "I0629 14:28:04.975488 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:28:04.975559 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 14:28:04.976414 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.976535 140698090002304 create_pretraining_data.py:151] tokens: [CLS] ezin [MASK] $ 2 [MASK] eztabaida ) wikilaria ##k [MASK] $ 1 [UNK] orrian egindako [MASK] aldaketa desegin [MASK] geroztik , beste ##ren batek editatu [MASK] edo jada desegin du [MASK] [SEP] azken aldaketa $ 3 ( eztabaida ) wikilaria ##k egin du . [SEP]\n",
            "I0629 14:28:04.976761 140698090002304 create_pretraining_data.py:161] input_ids: 2 367 4 14713 422 4 379 14718 13790 20 4 14713 218 1 1705 567 4 393 1354 4 1353 14721 61 21 140 3002 4 66 2543 1354 69 4 3 160 393 14713 187 14717 379 14718 13790 20 63 69 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.976993 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.977210 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.977298 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 2 5 10 16 19 26 31 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.977392 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 28 14717 737 160 14726 69 14723 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.977479 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.977550 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 14:28:04.978444 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.978586 140698090002304 create_pretraining_data.py:151] tokens: [CLS] up ##dat ##ed [SEP] xvii . [MASK] priest taula [SEP]\n",
            "I0629 14:28:04.978808 140698090002304 create_pretraining_data.py:161] input_ids: 2 13127 9627 661 3 980 14723 4 7575 672 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.979019 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.979233 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:05.076257 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:05.076543 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 415 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:05.076719 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:05.076859 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:20.369910 140698090002304 create_pretraining_data.py:166] Wrote 20467 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvMUE9sT6P1Y",
        "colab_type": "code",
        "outputId": "48fef8d0-0387-4e03-cccb-1cd1736410b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "!python src/run_pretraining.py \\\n",
        "  --config_file eu.congif.ini \\\n",
        "  --input_file=$GS/pretraining.tf.data \\\n",
        "  --output_dir=$GS/eu.gureBERT \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=1000000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "'''\n",
        "\n",
        "!python bert/run_pretraining.py \\\n",
        "  --input_file=$GS/wordpiece/pretraining.tf.data \\ \\\n",
        "  --output_dir=$GS/wordpiece/model \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=10000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  #--num_train_steps=1000000 \\\n",
        "  #--init_checkpoint=$GS/wordpiece/model \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 14:43:18.416723 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 14:43:18.417868 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:493: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 14:43:18.418495 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:407: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 14:43:18.418647 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:407: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 14:43:18.418800 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 14:43:18.419515 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:414: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0629 14:43:19.794317 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:418: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 14:43:19.957866 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:420: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 14:43:19.958102 139621729412992 run_pretraining.py:420] *** Input Files ***\n",
            "I0629 14:43:19.958192 139621729412992 run_pretraining.py:422]   gs://gurebert/gureBERT/wordpiece/pretraining.tf.data\n",
            "W0629 14:43:20.898190 139621729412992 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 14:43:21.903313 139621729412992 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7efc017958c8>) includes params argument, but params are not passed to Estimator.\n",
            "I0629 14:43:21.904874 139621729412992 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/wordpiece/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.36.212.66:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7efc0dbfc5f8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.36.212.66:8470', '_evaluation_master': 'grpc://10.36.212.66:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7efc0dc06320>}\n",
            "I0629 14:43:21.905195 139621729412992 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0629 14:43:21.905850 139621729412992 run_pretraining.py:459] ***** Running training *****\n",
            "I0629 14:43:21.905941 139621729412992 run_pretraining.py:460]   Batch size = 64\n",
            "I0629 14:43:22.253876 139621729412992 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.36.212.66:8470) for TPU system metadata.\n",
            "2019-06-29 14:43:22.255221: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 14:43:22.268328 139621729412992 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0629 14:43:22.268509 139621729412992 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0629 14:43:22.268592 139621729412992 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0629 14:43:22.268658 139621729412992 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0629 14:43:22.268725 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10173880149761554381)\n",
            "I0629 14:43:22.269498 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13648402031402572782)\n",
            "I0629 14:43:22.269571 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16511697586746770188)\n",
            "I0629 14:43:22.269636 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 4131540690459295465)\n",
            "I0629 14:43:22.269721 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15236588464056244225)\n",
            "I0629 14:43:22.269787 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 11456561882886992321)\n",
            "I0629 14:43:22.269848 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2519461556312082510)\n",
            "I0629 14:43:22.269910 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 11930862125451840624)\n",
            "I0629 14:43:22.269969 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10570494583918704491)\n",
            "I0629 14:43:22.270029 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1546865553820533360)\n",
            "I0629 14:43:22.270090 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1239985580371320602)\n",
            "W0629 14:43:22.276013 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0629 14:43:22.288680 139621729412992 estimator.py:1145] Calling model_fn.\n",
            "W0629 14:43:22.289170 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0629 14:43:22.295006 139621729412992 deprecation.py:323] From bert/run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0629 14:43:22.295165 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0629 14:43:22.319609 139621729412992 deprecation.py:323] From bert/run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0629 14:43:22.319752 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0629 14:43:22.321086 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:393: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0629 14:43:22.326452 139621729412992 deprecation.py:323] From bert/run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0629 14:43:22.403764 139621729412992 run_pretraining.py:117] *** Features ***\n",
            "I0629 14:43:22.403985 139621729412992 run_pretraining.py:119]   name = input_ids, shape = (8, 512)\n",
            "I0629 14:43:22.404090 139621729412992 run_pretraining.py:119]   name = input_mask, shape = (8, 512)\n",
            "I0629 14:43:22.404177 139621729412992 run_pretraining.py:119]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0629 14:43:22.404266 139621729412992 run_pretraining.py:119]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0629 14:43:22.404348 139621729412992 run_pretraining.py:119]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0629 14:43:22.404448 139621729412992 run_pretraining.py:119]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0629 14:43:22.404531 139621729412992 run_pretraining.py:119]   name = segment_ids, shape = (8, 512)\n",
            "W0629 14:43:22.404757 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0629 14:43:22.406776 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0629 14:43:22.440623 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0629 14:43:22.595678 139621729412992 deprecation.py:506] From /content/gureBERT/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0629 14:43:22.616803 139621729412992 deprecation.py:323] From /content/gureBERT/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0629 14:43:26.349247 139621729412992 run_pretraining.py:167] **** Trainable Variables ****\n",
            "I0629 14:43:26.349500 139621729412992 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768)\n",
            "I0629 14:43:26.349629 139621729412992 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0629 14:43:26.349723 139621729412992 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0629 14:43:26.349814 139621729412992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.349900 139621729412992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.349981 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.350063 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.350141 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.350228 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.350304 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.350397 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.350474 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.350554 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.350629 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.350703 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.350777 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.350857 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.350930 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.351008 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.351083 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.351157 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.351237 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.351315 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.351402 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.351481 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.351556 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.351633 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.351707 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.351786 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.351861 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.351933 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.352007 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.352084 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.352159 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.352245 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.352322 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.352409 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.352487 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.352566 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.352641 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.352720 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.352796 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.352873 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.352947 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.353025 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.353099 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.353173 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.353253 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.353330 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.353418 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.353498 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.353572 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.353647 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.353722 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.353802 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.353878 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.353958 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.354033 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.354112 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.354188 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.354304 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.354398 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.354475 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.354550 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.354630 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.354706 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.354785 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.354862 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.354936 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.355012 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.355091 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.355167 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.355252 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.355329 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.355420 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.355497 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.355575 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.355649 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.355722 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.355798 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.355876 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.355950 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.356029 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.356103 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.356176 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.356256 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.356334 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.356437 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.356519 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.356592 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.356670 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.356746 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.356825 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.356901 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.356975 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.357048 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.357126 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.357201 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.357285 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.357358 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.357446 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.357520 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.357598 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.357673 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.357752 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.357827 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.357905 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.357980 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.358057 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.358131 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.358210 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.358285 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.358362 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.358451 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.358530 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.358607 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.358682 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.358756 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.358835 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.358910 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.358987 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.359061 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.431471 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.431774 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.431921 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.432029 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.432135 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.432262 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.432397 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.432513 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.432628 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.432738 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.432848 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.432953 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.433065 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.433170 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.433297 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.433422 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.433540 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.433648 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.433757 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.433862 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.433968 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.434073 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.434182 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.434305 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.434436 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.434545 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.434650 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.434756 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.434868 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.434973 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.435085 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.435191 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.435316 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.435438 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.435553 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.435657 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.435759 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.435863 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.435973 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.436076 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.436186 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.436304 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.436426 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.436533 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.436646 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.436751 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.436861 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.436962 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.437073 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.437179 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.437299 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.437424 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.437534 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.437637 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.437747 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.437855 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.437963 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.438069 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.438172 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.438289 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.438414 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.438525 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.438637 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.438742 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.438853 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.438961 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.439071 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.439174 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.439291 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.439412 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.439526 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.439632 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.439741 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.439848 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.439951 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.440054 139621729412992 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.440166 139621729412992 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.440282 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.440411 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.440522 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.440629 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.440733 139621729412992 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0629 14:43:26.440838 139621729412992 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0629 14:43:26.440950 139621729412992 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0629 14:43:26.441147 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0629 14:43:26.442971 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0629 14:43:26.451014 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0629 14:43:26.723497 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0629 14:43:40.094371 139621729412992 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0629 14:43:40.735457 139621729412992 estimator.py:1147] Done calling model_fn.\n",
            "I0629 14:43:44.249897 139621729412992 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 14:43:45.621609 139621729412992 monitored_session.py:240] Graph was finalized.\n",
            "I0629 14:43:56.204694 139621729412992 session_manager.py:500] Running local_init_op.\n",
            "I0629 14:43:57.315330 139621729412992 session_manager.py:502] Done running local_init_op.\n",
            "I0629 14:44:09.169803 139621729412992 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://gurebert/gureBERT/wordpiece/model/model.ckpt.\n",
            "W0629 14:44:38.667258 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0629 14:44:40.952161 139621729412992 util.py:98] Initialized dataset iterators in 1 seconds\n",
            "I0629 14:44:40.953270 139621729412992 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-06-29 14:44:40.953657: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 14:44:40.957974 139621729412992 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0629 14:44:40.959899 139621729412992 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0629 14:44:40.963664 139621729412992 tpu_estimator.py:557] Init TPU system\n",
            "I0629 14:44:45.529625 139621729412992 tpu_estimator.py:566] Initialized TPU in 4 seconds\n",
            "I0629 14:44:45.530617 139620579305216 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 14:44:45.531130 139620552566528 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 14:44:46.636756 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 14:44:46.637878 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 14:45:19.172789 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 14:46:19.242447 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (0, 252)\n",
            "I0629 14:47:19.313048 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (0, 504)\n",
            "I0629 14:48:19.384603 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (0, 756)\n",
            "I0629 14:49:18.960929 139621729412992 basic_session_run_hooks.py:262] loss = 8.012552, step = 1000\n",
            "I0629 14:49:18.963466 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 14:49:18.963687 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 14:49:28.226758 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (1, 0)\n",
            "I0629 14:50:28.293938 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (1, 252)\n",
            "I0629 14:51:28.363535 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (1, 504)\n",
            "I0629 14:52:28.432737 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (1, 756)\n",
            "I0629 14:53:27.216873 139621729412992 basic_session_run_hooks.py:260] loss = 6.979651, step = 2000 (248.256 sec)\n",
            "I0629 14:53:27.218307 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.0281\n",
            "I0629 14:53:27.219255 139621729412992 tpu_estimator.py:2160] examples/sec: 257.799\n",
            "I0629 14:53:28.114691 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 14:53:28.115189 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 14:53:29.534754 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (2, 0)\n",
            "I0629 14:54:29.601197 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (2, 252)\n",
            "I0629 14:55:29.668438 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (2, 504)\n",
            "I0629 14:56:29.734620 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (2, 756)\n",
            "I0629 14:57:28.486635 139621729412992 basic_session_run_hooks.py:260] loss = 5.267299, step = 3000 (241.270 sec)\n",
            "I0629 14:57:28.488214 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.14474\n",
            "I0629 14:57:29.366555 139621729412992 tpu_estimator.py:2160] examples/sec: 265.263\n",
            "I0629 14:57:29.368444 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 14:57:29.368669 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 14:57:30.672204 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (3, 0)\n",
            "I0629 14:58:30.737096 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (3, 252)\n",
            "I0629 14:59:30.802705 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (3, 504)\n",
            "I0629 15:00:30.868494 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (3, 756)\n",
            "I0629 15:01:30.695654 139621729412992 basic_session_run_hooks.py:260] loss = 5.717806, step = 4000 (242.209 sec)\n",
            "I0629 15:01:30.697094 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.12867\n",
            "I0629 15:01:30.697449 139621729412992 tpu_estimator.py:2160] examples/sec: 264.235\n",
            "I0629 15:01:30.698603 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:01:30.698791 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:01:32.090096 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (4, 0)\n",
            "I0629 15:02:32.159316 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (4, 252)\n",
            "I0629 15:03:32.229675 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (4, 504)\n",
            "I0629 15:04:32.299462 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (4, 756)\n",
            "I0629 15:05:31.083400 139621729412992 basic_session_run_hooks.py:260] loss = 4.143191, step = 5000 (240.388 sec)\n",
            "I0629 15:05:31.085220 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.15994\n",
            "I0629 15:05:31.085409 139621729412992 tpu_estimator.py:2160] examples/sec: 266.236\n",
            "I0629 15:05:32.114043 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:05:32.114781 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:05:33.523420 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (5, 0)\n",
            "I0629 15:06:33.591883 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (5, 252)\n",
            "I0629 15:07:33.661691 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (5, 504)\n",
            "I0629 15:08:33.731530 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (5, 756)\n",
            "I0629 15:09:32.515052 139621729412992 basic_session_run_hooks.py:260] loss = 4.916213, step = 6000 (241.432 sec)\n",
            "I0629 15:09:32.516788 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.14196\n",
            "I0629 15:09:32.516986 139621729412992 tpu_estimator.py:2160] examples/sec: 265.086\n",
            "I0629 15:09:33.464939 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:09:33.465581 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:09:34.852605 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (6, 0)\n",
            "I0629 15:10:34.919304 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (6, 252)\n",
            "I0629 15:11:34.987476 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (6, 504)\n",
            "I0629 15:12:35.053251 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (6, 756)\n",
            "I0629 15:13:34.809849 139621729412992 basic_session_run_hooks.py:260] loss = 1.476404, step = 7000 (242.295 sec)\n",
            "I0629 15:13:34.811696 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.1272\n",
            "I0629 15:13:34.811857 139621729412992 tpu_estimator.py:2160] examples/sec: 264.141\n",
            "I0629 15:13:34.813434 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:13:34.813604 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:13:36.227687 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (7, 0)\n",
            "I0629 15:14:36.292591 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (7, 252)\n",
            "I0629 15:15:36.358829 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (7, 504)\n",
            "I0629 15:16:36.425096 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (7, 756)\n",
            "I0629 15:17:35.195486 139621729412992 basic_session_run_hooks.py:260] loss = 2.120125, step = 8000 (240.386 sec)\n",
            "I0629 15:17:35.197097 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.15999\n",
            "I0629 15:17:35.197261 139621729412992 tpu_estimator.py:2160] examples/sec: 266.239\n",
            "I0629 15:17:36.081555 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:17:36.082070 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:17:37.515753 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (8, 0)\n",
            "I0629 15:18:37.581182 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (8, 252)\n",
            "I0629 15:19:37.647189 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (8, 504)\n",
            "I0629 15:20:37.713159 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (8, 756)\n",
            "I0629 15:21:36.440223 139621729412992 basic_session_run_hooks.py:260] loss = 0.74855816, step = 9000 (241.245 sec)\n",
            "I0629 15:21:36.441756 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.14517\n",
            "I0629 15:21:37.415345 139621729412992 tpu_estimator.py:2160] examples/sec: 265.291\n",
            "I0629 15:21:37.417267 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:21:37.417520 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:21:38.757766 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (9, 0)\n",
            "I0629 15:22:38.823191 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (9, 252)\n",
            "I0629 15:23:38.889420 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (9, 504)\n",
            "I0629 15:24:38.957069 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (9, 756)\n",
            "I0629 15:25:37.698538 139621729412992 basic_session_run_hooks.py:606] Saving checkpoints for 10000 into gs://gurebert/gureBERT/wordpiece/model/model.ckpt.\n",
            "I0629 15:26:05.393211 139621729412992 basic_session_run_hooks.py:260] loss = 0.70103675, step = 10000 (268.953 sec)\n",
            "I0629 15:26:05.395117 139621729412992 tpu_estimator.py:2159] global_step/sec: 3.71812\n",
            "I0629 15:26:05.395523 139621729412992 tpu_estimator.py:2160] examples/sec: 237.96\n",
            "I0629 15:26:06.430169 139621729412992 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 15:26:06.430505 139621729412992 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 15:26:06.430712 139620579305216 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 15:26:06.430819 139620579305216 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 15:26:06.431037 139621729412992 error_handling.py:96] infeed marked as finished\n",
            "I0629 15:26:06.431169 139621729412992 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 15:26:06.431255 139621729412992 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 15:26:06.431420 139620552566528 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 15:26:06.431507 139620552566528 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 15:26:06.431634 139621729412992 error_handling.py:96] outfeed marked as finished\n",
            "I0629 15:26:06.431729 139621729412992 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 15:26:08.297458 139621729412992 estimator.py:368] Loss for final step: 0.70103675.\n",
            "I0629 15:26:08.298751 139621729412992 error_handling.py:96] training_loop marked as finished\n",
            "I0629 15:26:08.298901 139621729412992 run_pretraining.py:469] ***** Running evaluation *****\n",
            "I0629 15:26:08.298988 139621729412992 run_pretraining.py:470]   Batch size = 8\n",
            "I0629 15:26:09.132520 139621729412992 estimator.py:1145] Calling model_fn.\n",
            "I0629 15:26:09.230249 139621729412992 run_pretraining.py:117] *** Features ***\n",
            "I0629 15:26:09.230603 139621729412992 run_pretraining.py:119]   name = input_ids, shape = (1, 512)\n",
            "I0629 15:26:09.230720 139621729412992 run_pretraining.py:119]   name = input_mask, shape = (1, 512)\n",
            "I0629 15:26:09.230808 139621729412992 run_pretraining.py:119]   name = masked_lm_ids, shape = (1, 20)\n",
            "I0629 15:26:09.230891 139621729412992 run_pretraining.py:119]   name = masked_lm_positions, shape = (1, 20)\n",
            "I0629 15:26:09.230974 139621729412992 run_pretraining.py:119]   name = masked_lm_weights, shape = (1, 20)\n",
            "I0629 15:26:09.231053 139621729412992 run_pretraining.py:119]   name = next_sentence_labels, shape = (1, 1)\n",
            "I0629 15:26:09.231130 139621729412992 run_pretraining.py:119]   name = segment_ids, shape = (1, 512)\n",
            "I0629 15:26:13.235622 139621729412992 run_pretraining.py:167] **** Trainable Variables ****\n",
            "I0629 15:26:13.235882 139621729412992 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768)\n",
            "I0629 15:26:13.236008 139621729412992 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0629 15:26:13.236102 139621729412992 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0629 15:26:13.236192 139621729412992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.236278 139621729412992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.236360 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.236464 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.236546 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.236627 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.236711 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.236791 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.236867 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.236946 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.237021 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.237096 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.237170 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.237250 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.237326 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.237419 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.237496 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.237570 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.237647 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.237728 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.237804 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.237883 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.237957 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.238035 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.238109 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.238187 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.238262 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.238336 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.238423 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.238502 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.238577 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.238655 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.238736 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.238812 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.238885 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.238964 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.239038 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.239117 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.239191 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.239270 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.239346 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.239438 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.239513 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.239588 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.239660 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.239747 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.239823 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.239902 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.239978 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.240052 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.240126 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.240203 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.240278 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.240356 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.240445 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.240524 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.240599 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.240678 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.240759 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.240834 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.240908 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.240986 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.241061 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.241140 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.241215 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.241289 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.241364 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.241456 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.241532 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.241612 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.241688 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.241775 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.241851 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.241931 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.242007 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.242080 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.242155 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.242232 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.242308 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.242400 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.242478 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.242554 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.242628 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.242712 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.242788 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.242868 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.242942 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.243020 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.243095 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.243173 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.243247 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.243321 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.243408 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.243489 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.243564 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.243642 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.243722 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.243797 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.243869 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.243947 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.244020 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.244098 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.244172 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.244249 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.244323 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.244413 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.244488 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.244561 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.244635 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.244718 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.244794 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.244873 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.244947 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.245020 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.245093 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.245171 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.245246 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.245324 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.245411 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.245509 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.245584 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.314746 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.315032 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.315150 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.315252 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.315360 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.315508 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.315624 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.315745 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.315851 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.315956 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.316069 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.316177 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.316287 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.316413 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.316533 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.316652 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.316772 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.316881 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.316987 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.317091 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.317204 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.317313 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.317446 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.317555 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.317661 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.317780 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.317892 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.317998 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.318111 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.318216 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.318327 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.318455 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.318570 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.318675 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.318795 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.318900 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.319009 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.319113 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.319222 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.319327 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.319450 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.319555 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.319666 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.319781 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.319900 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.320007 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.320116 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.320221 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.320330 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.320455 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.320560 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.320661 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.320782 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.320888 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.320998 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.321103 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.321208 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.321312 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.321439 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.321550 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.321659 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.321774 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.321887 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.321993 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.322100 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.322204 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.322308 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.322425 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.322535 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.322643 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.322763 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.322867 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.322970 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.323072 139621729412992 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.323181 139621729412992 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.323286 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.323412 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.323522 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.323627 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.323740 139621729412992 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0629 15:26:13.323847 139621729412992 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0629 15:26:13.323957 139621729412992 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0629 15:26:13.718744 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:198: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
            "\n",
            "W0629 15:26:13.735288 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:202: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "I0629 15:26:14.790006 139621729412992 estimator.py:1147] Done calling model_fn.\n",
            "I0629 15:26:14.807806 139621729412992 evaluation.py:255] Starting evaluation at 2019-06-29T15:26:14Z\n",
            "I0629 15:26:14.808050 139621729412992 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 15:26:15.327312 139621729412992 monitored_session.py:240] Graph was finalized.\n",
            "W0629 15:26:15.328129 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0629 15:26:15.489625 139621729412992 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/wordpiece/model/model.ckpt-10000\n",
            "I0629 15:26:38.726282 139621729412992 session_manager.py:500] Running local_init_op.\n",
            "I0629 15:26:38.924824 139621729412992 session_manager.py:502] Done running local_init_op.\n",
            "I0629 15:26:39.397541 139621729412992 tpu_estimator.py:557] Init TPU system\n",
            "I0629 15:26:47.804910 139621729412992 tpu_estimator.py:566] Initialized TPU in 8 seconds\n",
            "I0629 15:26:47.805719 139620775925504 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 15:26:47.806060 139620745778944 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 15:26:48.016711 139621729412992 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0629 15:26:48.184749 139621729412992 tpu_estimator.py:590] Enqueue next (100) batch(es) of data to infeed.\n",
            "I0629 15:26:48.185075 139621729412992 tpu_estimator.py:594] Dequeue next (100) batch(es) of data from outfeed.\n",
            "I0629 15:26:53.507681 139620745778944 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 15:26:55.895613 139621729412992 evaluation.py:167] Evaluation [100/100]\n",
            "I0629 15:26:55.895987 139621729412992 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 15:26:55.896078 139621729412992 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 15:26:55.896346 139620775925504 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 15:26:55.896678 139620775925504 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 15:26:55.896860 139621729412992 error_handling.py:96] infeed marked as finished\n",
            "I0629 15:26:55.897046 139621729412992 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 15:26:55.897107 139621729412992 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 15:26:56.829536 139620745778944 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 15:26:56.829809 139620745778944 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 15:26:56.830069 139621729412992 error_handling.py:96] outfeed marked as finished\n",
            "I0629 15:26:56.830365 139621729412992 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 15:26:57.449885 139621729412992 evaluation.py:275] Finished evaluation at 2019-06-29-15:26:57\n",
            "I0629 15:26:57.450251 139621729412992 estimator.py:2039] Saving dict for global step 10000: global_step = 10000, loss = 0.366147, masked_lm_accuracy = 0.91963214, masked_lm_loss = 0.45129755, next_sentence_accuracy = 0.99875, next_sentence_loss = 0.002986016\n",
            "I0629 15:27:02.516640 139621729412992 estimator.py:2099] Saving 'checkpoint_path' summary for global step 10000: gs://gurebert/gureBERT/wordpiece/model/model.ckpt-10000\n",
            "I0629 15:27:03.299861 139621729412992 error_handling.py:96] evaluation_loop marked as finished\n",
            "I0629 15:27:03.300286 139621729412992 run_pretraining.py:483] ***** Eval results *****\n",
            "I0629 15:27:03.300433 139621729412992 run_pretraining.py:485]   global_step = 10000\n",
            "I0629 15:27:03.300760 139621729412992 run_pretraining.py:485]   loss = 0.366147\n",
            "I0629 15:27:03.300865 139621729412992 run_pretraining.py:485]   masked_lm_accuracy = 0.91963214\n",
            "I0629 15:27:03.300944 139621729412992 run_pretraining.py:485]   masked_lm_loss = 0.45129755\n",
            "I0629 15:27:03.301016 139621729412992 run_pretraining.py:485]   next_sentence_accuracy = 0.99875\n",
            "I0629 15:27:03.301085 139621729412992 run_pretraining.py:485]   next_sentence_loss = 0.002986016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNV42_z-O92_",
        "colab_type": "text"
      },
      "source": [
        "### EN-EU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "506576e5-5382-4ec8-f5fc-6c8b8abc4a2e",
        "id": "k-kNc1NmPg55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format('./spModels/en-eu'))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 44999\n",
            "Sample tokens: ['▁styling', 'lainn', '▁mayan', 'ned', 'spots', 'spectre', 'leninis', '▁diaspora', 'commenced', '▁subsidise']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IMHIP2dDPg7M",
        "colab": {}
      },
      "source": [
        "# hau errepasatu. adibidez @ ez dut ondo egiten... [UNK] agertzen da...\n",
        "# adibidez, \n",
        "\n",
        "#import string\n",
        "#def parse_sentencepiece_token(token):\n",
        "#    if token.startswith(\"▁\"):\n",
        "#        return token[1:]\n",
        "#    else:\n",
        "#        if token in string.punctuation:\n",
        "#            return token\n",
        "#        else:\n",
        "#            return \"##\" + token\n",
        "          \n",
        "\n",
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "10fb40df-fdd1-4780-8abc-8d313ef544fb",
        "id": "TYrllb6wPg7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import string\n",
        "\n",
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "# puntuazio sinboloak gehitu \n",
        "ctrl_symbols_end = list(string.punctuation)\n",
        "#ctrl_symbols = [\"[UNK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab + ctrl_symbols_end\n",
        "\n",
        "#bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(32000 - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RQVCJ3izPg72",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab-en_eu.txt\"\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2a9f7004-a6f9-4399-b439-b7b828cdb278",
        "id": "Lm-A4Er3Pg8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "from bert import tokenization\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(\"Nere kotxea aitonaren etxe alboan dago!, eta bere kolorea gorria da. ni@ni.eus erabili \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ne',\n",
              " '##re',\n",
              " 'ko',\n",
              " '##txea',\n",
              " 'ai',\n",
              " '##ton',\n",
              " '##aren',\n",
              " 'etxe',\n",
              " 'alb',\n",
              " '##oa',\n",
              " '##n',\n",
              " 'dago',\n",
              " '!',\n",
              " ',',\n",
              " 'eta',\n",
              " 'bere',\n",
              " 'kolore',\n",
              " '##a',\n",
              " 'gorria',\n",
              " 'da',\n",
              " '.',\n",
              " 'ni',\n",
              " '@',\n",
              " 'ni',\n",
              " '.',\n",
              " 'eu',\n",
              " '##s',\n",
              " 'erabili']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2989a392-f2cd-49e1-b2d4-c3d4ee5bfc80",
        "id": "q0P25Z_LPg8W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "FILES = \"./corpus/en-eu/2014wiki.eu.sent_splited,./corpus/en-eu/2019wiki-10k.en.sent_splited\"\n",
        "\n",
        "!python bert/create_pretraining_data.py \\\n",
        "  --input_file={FILES} \\\n",
        "  --output_file=$GS/wordpiece/pretraining-en_eu.tf.data \\\n",
        "  --vocab_file=vocab-en_eu.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=666 \\\n",
        "  --do_whole_word_mask=True \\\n",
        "  #--dupe_factor=5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 17:36:15.284230 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 17:36:15.285029 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 17:36:15.285196 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 17:36:15.285347 140555365861248 deprecation_wrapper.py:119] From /content/gureBERT/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 17:36:15.468693 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 17:36:15.470420 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 17:36:15.470667 140555365861248 create_pretraining_data.py:446] *** Reading from input files ***\n",
            "I0629 17:36:15.470755 140555365861248 create_pretraining_data.py:448]   ./corpus/en-eu/2014wiki.eu.sent_splited\n",
            "I0629 17:36:15.471500 140555365861248 create_pretraining_data.py:448]   ./corpus/en-eu/2019wiki-10k.en.sent_splited\n",
            "I0629 17:37:37.557271 140555365861248 create_pretraining_data.py:457] *** Writing to output files ***\n",
            "I0629 17:37:37.557595 140555365861248 create_pretraining_data.py:459]   gs://gurebert/gureBERT/wordpiece/pretraining-en_eu.tf.data\n",
            "W0629 17:37:37.557804 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0629 17:37:37.559309 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.559506 140555365861248 create_pretraining_data.py:151] tokens: [CLS] [MASK] geroztik , data ##k zehatzago ##ak [MASK] : [SEP] ! dead - [MASK] pages [SEP]\n",
            "I0629 17:37:37.559770 140555365861248 create_pretraining_data.py:161] input_ids: 2 4 8010 45015 752 72 29987 106 4 45019 3 45004 1323 45016 4 4913 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.559995 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.560211 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.560298 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 1 8 14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.560381 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 35309 241 248 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.560500 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.560594 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.561495 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.561639 140555365861248 create_pretraining_data.py:151] tokens: [CLS] post ##comment [SEP] irudiak igo [SEP]\n",
            "I0629 17:37:37.561862 140555365861248 create_pretraining_data.py:161] input_ids: 2 457 14140 3 12947 7423 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.562078 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.562287 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.562369 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.562457 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 7423 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.562544 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.562637 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.563494 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.563615 140555365861248 create_pretraining_data.py:151] tokens: [CLS] mainpage ##text [SEP] imgde ##lete [SEP]\n",
            "I0629 17:37:37.563832 140555365861248 create_pretraining_data.py:161] input_ids: 2 29974 1612 3 34403 34342 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.564043 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.564251 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.564334 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.564419 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.564503 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.564591 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.565460 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.565613 140555365861248 create_pretraining_data.py:151] tokens: [CLS] the eusko ##bar ##ome ##tro , the survey carried out by the universi ##dad del [MASK] ##s vasco ( university [MASK] the [MASK] country ) [MASK] aski ##ng about the [MASK] of eta within the basque population , obtained these results in may 2009 [MASK] 64 [MASK] rejected eta totally , [MASK] % identified themselves as former eta sympathise ##rs who no longer support the group . another [MASK] % agreed with [MASK] ' s ends [MASK] but not thei ##r means [MASK] [SEP] upload [SEP]\n",
            "I0629 17:37:37.565837 140555365861248 create_pretraining_data.py:161] input_ids: 2 11 8154 3174 20240 13863 45015 11 3905 1229 148 31 11 42869 23148 2315 4 19 13976 45011 365 4 11 4 187 45012 4 24259 389 113 11 4 14 40 237 11 1319 214 45015 1972 91 832 16 87 760 4 6193 4 2424 40 7227 45015 4 45008 1588 749 24 567 40 22527 2328 74 111 974 292 11 173 45017 242 4 45008 1773 33 4 45010 294 4601 4 62 58 10026 179 639 4 3 10179 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.566059 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.566271 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.647868 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 16 17 21 23 26 31 45 47 52 69 73 77 83 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.648111 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 34017 19 14 1319 45015 2303 45019 45008 874 524 40 45015 45017 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.648246 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.648340 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.649856 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.650359 140555365861248 create_pretraining_data.py:151] tokens: [CLS] although the grammars of the spoken varieties share many [MASK] , they do possess differences . the entire chinese character corpus since antiquity comprise ##s [MASK] over 20 , 0 ##00 characters , [MASK] which only roughly 10 , 0 ##00 are now commonly in use . however chinese characters should not be confused with chinese words . because most chinese words are made up of two or more characters , there are many more chinese words than characters . a more accurate equivalent for a chinese character is the [MASK] , as characters represent the smallest grammatical units with individual meanings in the chinese language . estimates of the total number of chinese words and lexical ##ized phrases vary greatly [MASK] the \" han ##yu da zi ##dian \" , a compen ##dium of chinese [MASK] , includes 54 , 67 ##8 head entries for characters , including bone oracle versions . the \" zho ##nghu ##a zi ##hai \" ( 1994 ) contains [MASK] , [MASK] [MASK] head entries for character definitions , [MASK] is the largest reference work based pure ##ly on character and its literary variants . the c ##c - ced ##ic ##t project ( 2010 ) contains 97 , 40 ##4 contemporar ##y entries including idiom ##s , technology terms and names of political figures , businesses and products . the 2009 version of the [MASK] [MASK] ' s digital chinese dictionary ( w ##dc ##d ) , based on c ##c - ced ##ic ##t , contains over 84 , 0 ##00 entries . the most comprehensive pure linguistic chinese - language [MASK] , the 12 - volume \" han ##yu da ci ##dian \" , records more than 23 , 0 ##00 head chinese [MASK] and gives over 370 , 0 ##00 definitions . the 1999 revise ##d \" ci ##hai \" , a multi - volume encyclopedi ##c dictionary reference work , gives 122 , 83 ##6 vocabulary entry definitions under [MASK] , [MASK] [MASK] chinese characters , including proper names , phrases and common zoological , geographical , sociological , scientific and technical terms . the 7 ##th [MASK] 2016 ) edition of \" xian ##da ##i han ##yu ci ##dian \" , an authoritative one - volume dictionary on modern standard chinese language as used in mainland china , has 13 , 0 ##00 head characters and define ##s [MASK] , 0 ##00 words . like any other language , [MASK] has absorbed a siz ##able number of loanwords from other cultures . most chinese words are formed out of native chinese morphemes , including words describ ##ing imported objects and ideas . however , direct phonetic borrow ##ing of foreign words has gone on since ancient times . [SEP] urtarrila ##ren 18 [SEP]\n",
            "I0629 17:37:37.650736 140555365861248 create_pretraining_data.py:161] input_ids: 2 192 11 23180 14 11 1846 2319 1956 98 4 45015 76 264 10807 1829 45017 11 1117 257 862 5941 131 10245 3773 19 4 110 330 45015 3884 3683 724 45015 4 47 104 2013 524 45015 3884 3683 36 258 935 16 101 45017 115 257 724 433 58 42 4793 33 257 518 45017 186 78 257 518 36 153 130 14 97 43 81 724 45015 89 36 98 81 257 518 92 724 45017 18 81 3606 1911 27 18 257 862 21 11 4 45015 24 724 1240 11 4450 7060 1527 33 609 6647 16 11 257 190 45017 5774 14 11 367 158 14 257 518 15 12728 1144 3907 1813 2442 4 11 45005 1820 32776 112 5334 10786 45005 45015 18 42663 33414 14 257 4 45015 771 5939 45015 11957 1298 730 6240 27 724 45015 162 13332 7092 1525 45017 11 45005 43931 24769 46 5334 9138 45005 45011 1615 45012 1002 4 45015 4 4 730 6240 27 862 5739 45015 4 21 11 411 1602 150 261 2185 56 34 862 15 67 2052 4859 45017 11 206 165 45016 31590 282 121 840 45011 632 45012 1002 15139 45015 1662 1421 1674 86 6240 162 23766 19 45015 834 590 15 1161 14 293 2127 45015 3999 15 1207 45017 11 760 484 14 11 4 4 45010 294 2230 257 5039 45011 534 13474 48 45012 45015 261 34 206 165 45016 31590 282 121 45015 1002 110 18909 45015 3884 3683 6240 45017 11 78 4640 2185 4037 257 45016 190 4 45015 11 596 45016 1504 45005 1820 32776 112 9052 10786 45005 45015 1038 81 92 1546 45015 3884 3683 730 257 4 15 1871 110 14037 45015 3884 3683 5739 45017 11 1395 24437 48 45005 9052 9138 45005 45015 18 1408 45016 1504 26791 165 5039 1602 150 45015 1871 13898 45015 15927 1572 6988 2894 5739 128 4 45015 4 4 257 724 45015 162 1978 1161 45015 3907 15 251 21232 45015 4769 45015 22262 45015 1493 15 2482 590 45017 11 1001 108 4 1562 45012 2028 14 45005 24523 2311 129 1820 32776 9052 10786 45005 45015 41 14711 63 45016 1504 5039 34 243 482 257 190 24 90 16 1734 375 45015 57 874 45015 3884 3683 730 724 15 2291 19 4 45015 3884 3683 518 45017 256 154 69 190 45015 4 57 5208 18 12356 298 158 14 14587 37 69 2193 45017 78 257 518 36 663 148 14 1196 257 9567 45015 162 518 3417 28 4829 2700 15 2468 45017 115 45015 1030 6373 10256 28 14 616 518 57 5517 34 131 929 493 45017 3 1626 83 577 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.650989 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.651205 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.651293 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 10 26 34 91 122 137 166 168 169 176 232 233 270 293 331 333 334 359 401 412\n",
            "I0629 17:37:37.651376 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 6284 147 14 16743 45017 724 8479 4362 1298 15 1129 5836 5039 724 453 5801 1419 45011 3829 257\n",
            "I0629 17:37:37.651474 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.651565 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.652521 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.652853 140555365861248 create_pretraining_data.py:151] tokens: [CLS] schr ##odi ##nger published in \" anna ##len der physik \" the paper \" \" ( quanti ##zation as an eigenvalue problem ) on wave mechanics and presented what is now known as the schr ##odi ##nger equation . in this paper , he gave a \" derivation [MASK] of the wave equation for time - independent systems and showe ##d that it gave the correct energy eigenvalue ##s for a hydrogen - like atom . this paper has been universal ##ly celebrated as one of the most important achievements of the twentieth century and created a revolution in most areas of quantum mechanics and indeed of all physics and chemistry . a second paper was submitted just four weeks later that solved [MASK] quantum harmonic oscillator , rigid rotor , and dia ##tomic molecule problems and gave a new derivation of the schr ##odi ##nger equation . a third paper , published in may , showe ##d the equivalen ##ce of [MASK] [MASK] to that of heisenberg and gave the treatment of the stark effect . a [MASK] paper in this series showe ##d how to treat problems in which the colony changes with time , as in scatter ##ing problems . in this paper he introduced [MASK] complex solution to the wave equation in order to prevent the occurrence of fourth and sixth order differential equations . ( this was arguably the moment when quantum mechanics switche ##d from real to complex numbers . ) when [MASK] introduced complex numbers in order to lower the order of the differential equations , something magical happened , and all of wave mechanics was at his feet . ( he eventually reduced the order to one . ) these papers were his central achievement and [MASK] at once recognized as havi ##ng great significance [SEP] schr ##odi ##nger was not entirely comfortable with the implications of quantum theory . he wrote about the probability interpretation of quantum mechanics , saying : \" i don ' t like it , and i ' m sor ##ry i ever had [MASK] to do with it . \" ( just in order to ridicule the copenhagen interpretation of quantum mechanics [MASK] cont ##rived the famous thought - experiment called schr ##odi ##nger ' s cat paradox . [MASK] following his work on quantum mechanics , schr ##odi ##nger devote ##d considerable effort to working on a unified field theory that would unite gravity , electromagneti ##sm , and nuclear forces within the basic framework of [MASK] relativ ##ity skip doing the work with an extended correspondence with albert einstein . in 1947 , he announced a result , \" af ##fin ##e field theory , \" [MASK] a talk at the royal irish academy , but the announcement was criticize ##d by einstein as \" pre ##lim ##inary ##methyl and [MASK] to lead to the desired unified theory [MASK] following the failure of his attempt at unification , schr ##odi ##nger gave up his work on unification and turn ##ed to other topics . [SEP]\n",
            "I0629 17:37:37.653126 140555365861248 create_pretraining_data.py:161] input_ids: 2 36468 35027 25036 356 16 45005 5567 13874 2653 22962 45005 11 1792 45005 45005 45011 42344 16194 24 41 27798 948 45012 34 1797 2953 15 1798 305 21 258 127 24 11 36468 35027 25036 1102 45017 16 51 1792 45015 54 770 18 45005 16755 4 14 11 1797 1102 27 102 45016 933 420 15 24772 48 29 39 770 11 3283 638 27798 19 27 18 3224 45016 256 2734 45017 51 1792 57 65 1595 56 3988 24 63 14 11 78 325 8271 14 11 4124 159 15 544 18 2021 16 78 384 14 2137 2953 15 3049 14 84 1771 15 3547 45017 18 215 1792 26 8075 599 271 2210 139 29 9594 4 2137 6914 2801 45015 8086 11203 45015 15 24047 38882 6562 1067 15 770 18 88 16755 14 11 36468 35027 25036 1102 45017 18 459 1792 45015 356 16 87 45015 24772 48 11 20605 1284 14 4 4 17 29 14 14628 15 770 11 709 14 11 22947 634 45017 18 4 1792 16 51 554 24772 48 507 17 1509 1067 16 47 11 1837 614 33 102 45015 24 16 26793 28 1067 45017 16 51 1792 54 600 4 654 2503 17 11 1797 1102 16 320 17 825 11 10271 14 1589 15 4167 320 8549 4789 45017 45011 51 26 8118 11 5019 94 2137 2953 25436 48 37 682 17 654 466 45017 45012 94 4 600 654 466 16 320 17 711 11 320 14 11 8549 4789 45015 1557 15745 4811 45015 15 84 14 1797 2953 26 50 44 3984 45017 45011 54 728 1248 11 320 17 63 45017 45012 91 5603 53 44 521 7372 15 4 50 621 1614 24 23762 389 228 4776 3 36468 35027 25036 26 58 1907 9431 33 11 8867 14 2137 642 45017 54 364 113 11 2705 2932 14 2137 2953 45015 1888 45019 45005 135 2602 45010 223 256 39 45015 15 135 45010 319 8945 1303 135 958 61 4 17 264 33 39 45017 45005 45011 599 16 320 17 10896 11 6976 2932 14 2137 2953 4 16568 35721 11 813 824 45016 3197 156 36468 35027 25036 45010 294 6475 6917 45017 4 230 44 150 34 2137 2953 45015 36468 35027 25036 6037 48 2824 1711 17 1187 34 18 3746 541 642 29 107 18677 10529 45015 32723 9069 45015 15 1368 462 237 11 1387 2886 14 4 12108 1185 10140 3470 11 150 33 41 1433 5453 33 1451 7056 45017 16 2014 45015 54 1094 18 295 45015 45005 9888 44587 66 541 642 45015 45005 4 18 3331 50 11 783 2474 3042 45015 62 11 4452 26 4898 48 31 7056 24 45005 759 32506 35948 28361 15 4 17 438 17 11 6013 3746 642 4 230 11 2392 14 44 1008 50 5681 45015 36468 35027 25036 770 130 44 150 34 5681 15 1032 35 17 69 4867 45017 3\n",
            "I0629 17:37:37.653346 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.653572 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.653660 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 49 124 163 164 179 193 209 249 295 348 367 384 415 422 425 434 453 475 477 485\n",
            "I0629 17:37:37.653741 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45005 11 44 1280 1589 119 18 54 53 4113 54 45012 1368 297 45015 1451 16 45005 1610 45017\n",
            "I0629 17:37:37.653825 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.653895 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 17:37:37.654838 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.655040 140555365861248 create_pretraining_data.py:151] tokens: [CLS] by 1956 , the zappa family had moved to lancaster , a small aerospace and farming town in [MASK] ante ##lope valley of the mo ##ja ##ve desert close to edwards [MASK] force base ; he would later refer [MASK] sun village ( a town close to lancaster ) in the 1973 [MASK] \" village [MASK] the sun \" . zappa ' s mother encouraged him in his [MASK] interests . although she dislike ##d var ##ese ' s music , she was indulge ##nt enough to give her son a ##burnt distance call to the composer as a [MASK] [MASK] birthday present . unfortunately , var ##ese was in europe at [MASK] time , so zappa [MASK] to the composer ' s wife and she suggested he call back later . in a letter var ##ese thank ##ed him for his interest , [MASK] told him about a composition he was working on called \" deserts \" . living in the desert town of lancaster , zappa found this very [MASK] . [MASK] [MASK] [MASK] him to [MASK] if he ever came to new york . the meeting never took place ( var ##ese died in 1965 ) , but [MASK] frame ##d the letter and kep ##t it on display for the rest of his life . [SEP] title ##matches [SEP]\n",
            "I0629 17:37:37.751277 140555365861248 create_pretraining_data.py:161] input_ids: 2 31 1975 45015 11 296 416 61 925 17 14441 45015 18 279 9357 15 2383 652 16 4 25478 34531 2933 14 11 1760 5172 436 3291 471 17 19331 4 333 960 45020 54 107 139 649 4 1692 2119 45011 18 652 471 17 14441 45012 16 11 2509 4 45005 2119 4 11 1692 45005 45017 296 45010 294 1080 2902 164 16 44 4 2759 45017 192 232 9456 48 6488 5015 45010 294 167 45015 232 26 21291 3168 1641 17 1134 143 542 18 26498 2076 891 17 11 1861 24 18 4 4 8764 528 45017 14780 45015 6488 5015 26 16 414 50 4 102 45015 141 296 4 17 11 1861 45010 294 1329 15 232 1273 54 891 346 139 45017 16 18 396 6488 5015 37350 35 164 27 44 1252 45015 4 1444 164 113 18 2117 54 26 1187 34 156 45005 24033 45005 45017 761 16 11 3291 652 14 14441 45015 296 188 51 268 4 45017 4 4 4 164 17 4 155 54 958 461 17 88 710 45017 11 2079 597 448 327 45011 6488 5015 936 16 3003 45012 45015 62 4 7388 48 11 396 15 19517 121 39 34 3422 27 11 1488 14 44 273 45017 3 918 14152 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.751838 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.752216 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.752371 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 19 32 40 44 53 56 69 92 100 101 113 118 145 172 174 175 176 179 190 202\n",
            "I0629 17:37:37.752507 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 11 234 17 18 1821 14 1098 259 622 108 11 4046 15 11391 6488 5015 3632 1973 597 296\n",
            "I0629 17:37:37.752660 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.752769 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.754054 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.754399 140555365861248 create_pretraining_data.py:151] tokens: [CLS] mono ##na terrace , originally designed in 1937 as municipal offices for madison , wisconsin , was completed in 1997 on the original site , us ##ing [MASK] variation of wright ' s final design for the exterior , with the interior design altered by its new purpose as a convention center . the \" as - built \" design was carried out [MASK] wright ' s apprentice ton ##y puttnam . mono ##na terrace was accompanie ##d by controversy throughout the 60 years between [MASK] original design and the completi ##on of the [MASK] . florida southern college , located in lake ##land , florida , constructed 12 ( out of 18 planned ) frank lloyd [MASK] [MASK] between 1941 and 1958 as part of the child of the sun project . it is the world ' s largest single - site collection of frank lloyd wright architecture . [SEP] made innovati ##ve use of new building materials such as pre ##cast concrete blocks , glass bricks , and zinc came ##s ( instead of the traditional lead ) for his lead ##light windows , and he [MASK] used pyr ##ex glass tubing as a major element in the johnson wax headquarters . wright was also one of the first architects to design and install custom - made electric light fittings , including some of the first electric floor lamps , and his very early use of the then - novel spher ##ical glass lamps ##had ##e ( a design previously not possible due to the physical restrictions of gas lighting ) . in 1897 , wright received a patent for \" pri ##sm glass tiles \" that were used in [MASK] [MASK] ##s to direct light toward the interior . wright full ##y embrace ##d glass hamad his designs and found that it fit well into his philosophy of organic architecture . according to wright ' s organic theory , all components of the building should appear unified , as though they belong together . nothing should be attache ##d [MASK] it without consider ##ing the effect on the whole . to unify the house to its site , wright often used large expanse ##s of glass to blur the boundary between the indoor ##s and outdoors . glass allowed for interaction and viewing of [MASK] outdoors while still protecti ##ng from the elements . in 1928 , wright wrote an essay on glass in which [MASK] compared it to the mirror ##s of nature : lakes , rivers and pon ##ds . one of wright ' s earliest uses of glass in his works was to string pan ##es of glass along whole administration in an attempt to create light screens [MASK] join together solid walls . by us ##ing this large amount of glass , wright sought to achieve a balance between the lightness and air ##iness of the glass and the solid , hard walls . arguably , wright ' [MASK] best - known art glass is that of the prairie style . the simple [SEP]\n",
            "I0629 17:37:37.754716 140555365861248 create_pretraining_data.py:161] input_ids: 2 9208 1994 11181 45015 909 949 16 4224 24 2282 4840 27 8213 45015 5180 45015 26 1353 16 1834 34 11 382 769 45015 280 28 4 4568 14 347 45010 294 546 511 27 11 27959 45015 33 11 2266 511 5727 31 67 88 1570 24 18 2196 718 45017 11 45005 24 45016 562 45005 511 26 1229 148 4 347 45010 294 16641 33522 86 7453 45017 9208 1994 11181 26 5541 48 31 3838 705 11 2830 124 105 4 382 511 15 11 20466 191 14 11 4 45017 3433 487 871 45015 750 16 2358 1162 45015 3433 45015 1904 596 45011 148 14 577 2138 45012 1236 2708 4 4 105 3225 15 2622 24 144 14 11 1814 14 11 1692 840 45017 39 21 11 114 45010 294 411 409 45016 769 2245 14 1236 2708 347 1985 45017 3 153 6458 436 101 14 88 990 1983 70 24 759 15704 6597 4907 45015 3334 22916 45015 15 1707 461 19 45011 598 14 11 422 438 45012 27 44 438 5797 4666 45015 15 54 4 90 36533 6157 3334 21702 24 18 219 2337 16 11 3980 10581 3226 45017 347 26 60 63 14 11 73 5549 17 511 15 4484 5467 45016 153 1728 1143 13233 45015 162 75 14 11 73 1728 6035 17997 45015 15 44 268 152 101 14 11 211 45016 1946 13642 1019 3334 17997 7346 66 45011 18 511 1537 58 408 255 17 11 1106 3544 14 605 13635 45012 45017 16 5646 45015 347 498 18 5366 27 45005 18454 9069 3334 30176 45005 29 53 90 16 4 4 19 17 1030 1143 731 11 2266 45017 347 580 86 7915 48 3334 33082 44 3039 15 188 29 39 2720 147 96 44 1896 14 2645 1985 45017 299 17 347 45010 294 2645 642 45015 84 1921 14 11 990 433 991 3746 45015 24 323 76 1705 559 45017 2909 433 42 8500 48 4 39 390 1116 28 11 634 34 11 1022 45017 17 17754 11 368 17 67 769 45015 347 145 90 181 15568 19 14 3334 17 18623 11 5010 105 11 13665 19 15 32010 45017 3334 701 27 5482 15 9096 14 4 32010 120 307 35711 389 37 11 911 45017 16 4069 45015 347 364 41 4085 34 3334 16 47 4 1289 39 17 11 5005 19 14 796 45019 4195 45015 2746 15 32296 11607 45017 63 14 347 45010 294 1405 1404 14 3334 16 44 500 26 17 1722 4655 197 14 3334 354 1022 1732 16 41 1008 17 1157 1143 31132 4 1864 559 2793 3267 45017 31 280 28 51 181 1070 14 3334 45015 347 3223 17 2186 18 2951 105 11 29672 15 234 10635 14 11 3334 15 11 2793 45015 1457 3267 45017 8118 45015 347 45010 4 588 45016 127 491 3334 21 29 14 11 5985 483 45017 11 982 3\n",
            "I0629 17:37:37.754959 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.755170 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.755254 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 28 64 65 86 95 118 119 189 281 283 284 285 299 343 388 409 417 447 455 496\n",
            "I0629 17:37:37.755334 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 18 31 347 11 905 347 1468 6051 90 3411 14183 19 16 17 11 54 796 3267 17 294\n",
            "I0629 17:37:37.755427 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.755497 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 17:37:37.756480 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.756698 140555365861248 create_pretraining_data.py:151] tokens: [CLS] [MASK] translations of the epistle to titus : [SEP] * 1870 - miguel primo de rivera militar eta diktadore espainiarra . ( h . 1930 ) . * 1933 - [MASK] mars ##e idazle kataluniarra [MASK] * 1934 - jacques an ##quet ##il txirrindulari frantziarra , [MASK] frantziako tour [MASK] zuen lehenengo ##a . ( [MASK] . 1987 ) . * 1935 - elvis presley rock abeslari estatubatuarra . ( h . 1977 ) . [MASK] 1941 - gra ##ham chapman umorista [MASK] idazle britainiarra . ecosystems [MASK] . 1989 ) . * 1942 - stephen hawking fisikari britainiarra . * 1942 - junichi ##ro koizumi , japoniak ##o lehen ministroa . * 1947 - david bowie [MASK] [MASK] . heriotzak euskal herria * 2001 - marian ##o ize ##ta idazle nafarra [MASK] ( j . 1915 ) . * 2007 - patx ##i be ##itia aurre ##sk ##ular ##i gipuzkoarra wrest ( j . 1925 ) . mundua * 132 ##4 - marco polo esploratzaile italiarra . [MASK] j . 125 ##4 ) . * 1642 - galileo galilei , italiar filosofo , fisikari eta merchandis . ( j . 1564 [MASK] . jaiak eta urteurrena ##k * se ##haska egutegi ##ko izend ##egia : diag ##ur eta [MASK] [MASK] . jaieguna , ondo ##ko herrian : * bizkaian : muskiz . [SEP]\n",
            "I0629 17:37:37.756947 140555365861248 create_pretraining_data.py:161] input_ids: 2 4 4915 14 11 972 17 6725 45019 3 45013 4686 45016 2415 22493 117 25041 6033 40 24299 6653 45017 45011 504 45017 1592 45012 45017 45013 4889 45016 4 7199 66 818 8061 4 45013 4766 45016 4454 41 19593 2387 9132 4964 45015 4 2317 2684 4 134 2519 46 45017 45011 4 45017 2425 45012 45017 45013 5598 45016 10627 8680 489 3477 1158 45017 45011 504 45017 2818 45012 45017 4 3225 45016 12030 5749 8655 14588 4 818 6642 45017 15372 4 45017 2629 45012 45017 45013 2806 45016 6548 12998 5650 6642 45017 45013 2806 45016 21054 2502 27249 45015 10901 99 515 13823 45017 45013 2014 45016 775 28618 4 4 45017 503 202 385 45013 1055 45016 16090 99 34768 753 818 7175 4 45011 595 45017 3592 45012 45017 45013 732 45016 18859 129 42 18557 5879 6126 10010 129 4827 22738 45011 595 45017 4598 45012 45017 401 45013 13770 1421 45016 8138 11966 8357 4044 45017 4 595 45017 9177 1421 45012 45017 45013 13586 45016 5069 8114 45015 3501 2491 45015 5650 40 39429 45017 45011 595 45017 6815 4 45017 1646 40 1643 72 45013 581 1639 887 68 1689 1616 45019 32228 1738 40 4 4 45017 8931 45015 5552 68 4205 45019 45013 10426 45019 28093 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.757171 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.757382 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.757479 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 1 31 36 47 50 56 76 83 87 88 118 119 133 152 167 169 187 193 210 211\n",
            "I0629 17:37:37.757580 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 2061 2242 45017 474 2269 504 45013 40 45011 504 3477 6642 45017 45017 4044 45011 11629 45012 5819 59\n",
            "I0629 17:37:37.757680 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.757753 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.758699 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.758918 140555365861248 create_pretraining_data.py:151] tokens: [CLS] the largest airport is the gran canaria airport . tenerife has two airports , tenerife north airport and tenerife south airport . the island [MASK] tenerife gather ##s the highest passenger movement of all the canary islands through its two airports . [MASK] two main islands ( [MASK] and gran canaria ) receive the greatest number of rö . tenerife 6 , 20 ##4 , 49 ##9 passengers and gran canaria 5 , 0 ##11 , 176 passengers . [SEP] the port of las palmas is first in freight traffic in the islands , [MASK] the port of santa cruz de tenerife is the [MASK] fishing port with approximate ##ly 7 ##emp 500 tons of fish caught , according to the spanish government publication statistical year ##book of state ports . similarly , it is [MASK] second port in spain as regards ship traffic , only [MASK] ##checked by the port of alge ##cir ##as bay . the port ' s facilities include a border inspection post [MASK] [MASK] ) approved by the european union , which is responsible for inspect ##ing all types of imports from third countries or exports to countries outside the european economic area . the port of los [MASK] [MASK] ( tenerife ) has the greatest number of passengers [MASK] in the canary islands , followed by the port of santa cruz de tenerife . the port of las [MASK] is the [MASK] port in [MASK] [MASK] in passengers and first in number of vehicles transported . [SEP]\n",
            "I0629 17:37:37.854686 140555365861248 create_pretraining_data.py:161] input_ids: 2 11 411 1060 21 11 2074 2130 1060 45017 1650 57 97 2970 45015 1650 276 1060 15 1650 262 1060 45017 11 225 4 1650 5792 19 11 1083 2071 415 14 84 11 966 100 176 67 97 2970 45017 4 97 245 100 45011 4 15 2074 2130 45012 2102 11 2140 158 14 42417 45017 1650 520 45015 330 1421 45015 6442 1617 2930 15 2074 2130 474 45015 3884 6089 45015 23683 2930 45017 3 11 683 14 3412 4329 21 73 16 7474 1801 16 11 100 45015 4 11 683 14 1547 3232 117 1650 21 11 4 1752 683 33 1051 56 1001 44416 4330 6106 14 989 7064 45015 299 17 11 476 125 2446 2410 183 5290 14 172 3767 45017 2066 45015 39 21 4 215 683 16 853 24 11727 1193 1801 45015 104 4 9865 31 11 683 14 43069 43283 449 1397 45017 11 683 45010 294 2320 193 18 1420 11508 457 4 4 45012 3552 31 11 180 363 45015 47 21 1259 27 22910 28 84 954 14 3758 37 459 289 43 2732 17 289 801 11 180 495 238 45017 11 683 14 1556 4 4 45011 1650 45012 57 11 2140 158 14 2930 4 16 11 966 100 45015 778 31 11 683 14 1547 3232 117 1650 45017 11 683 14 3412 4 21 11 4 683 16 4 4 16 2930 15 73 16 158 14 3473 6799 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.855138 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.855450 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.855582 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 25 43 48 58 95 105 112 136 147 148 162 168 169 204 205 215 235 238 241 242\n",
            "I0629 17:37:37.856912 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 14 11 1650 2930 120 73 45015 11 32541 48 2320 45011 26854 7937 18867 947 4329 459 11 100\n",
            "I0629 17:37:37.857119 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.857234 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 17:37:37.858990 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.859275 140555365861248 create_pretraining_data.py:151] tokens: [CLS] atlanta is [MASK] [MASK] by a mayor and [MASK] atlanta city council . the [MASK] council consist ##s of 15 representatives — [MASK] from each of the city ' s 12 districts and three at - large positions . the [MASK] may veto a [MASK] passed by the council , but the council can overrid ##e [MASK] veto with a two - thirds majority . [MASK] mayor ##princip atlanta is kei ##sha lance bottoms [MASK] a democrat elected on a nonpartisan ballot whos ##e first term in office began on january 2 , 2018 ##60.9 [SEP] oharra ezikusi eta [MASK] [MASK] . [SEP]\n",
            "I0629 17:37:37.859703 140555365861248 create_pretraining_data.py:161] input_ids: 2 290 21 4 4 31 18 3107 15 4 290 122 426 45017 11 4 426 667 19 14 622 4888 850 4 37 194 14 11 122 45010 294 596 4080 15 184 50 45016 181 2498 45017 11 4 87 11560 18 4 1827 31 11 426 45015 62 11 426 79 35858 66 4 11560 33 18 97 45016 19856 745 45017 4 3107 36973 290 21 39985 24919 10665 33544 4 18 6180 1141 34 18 21368 12478 2741 66 73 246 16 1012 286 34 608 386 45015 3947 41538 3 11400 28092 40 4 4 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.860047 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.860368 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.860519 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 3 4 9 15 23 41 45 57 66 68 75 79 80 95 100 101 0 0 0 0\n",
            "I0629 17:37:37.860669 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 3008 35 11 122 63 3107 1413 11 11 14 45015 34 18 45017 7209 5954 0 0 0 0\n",
            "I0629 17:37:37.860787 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.860890 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.862283 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.862538 140555365861248 create_pretraining_data.py:151] tokens: [CLS] text ##matches [SEP] heriotzak euskal herria * 1833 - faust ##o el ##huy ##ar , euskal kimikaria , wolfram ##aren aurkitzailea ( j . [MASK] ) . ##descending * 1804 - joseph priest ##ley , kimikari ingelesa , oxi ##gen ##oaren aurkitzailea [MASK] 1916 - ruben dario , poeta nikaragua ##rra sausages j . [MASK] ) . [MASK] 1929 - maria [MASK] [MASK] austriako ##a , espainiako [MASK] [MASK] ( j . 1858 ) . * [MASK] - ju ##r ##gi vi . a , erresuma batuko erregea ( j . 1895 ) . * 1994 - joseph co ##tten , estatubatuar aktorea ( j . ##fame ) . * [MASK] - [MASK] moore , irlandar [MASK] . ( j . 1952 [MASK] . * 2012 - [MASK] tap ##ies , kataluniar margolari eta eskultorea [MASK] [MASK] . 1923 ) . jaiak eta urteurrena ##k * se ##haska egutegi ##ko izend ##egia : ob ##eko / ob ##ek ##a eta gaston . [SEP]\n",
            "I0629 17:37:37.862906 140555365861248 create_pretraining_data.py:161] input_ids: 2 1382 14152 3 503 202 385 45013 6638 45016 14829 99 1462 43349 1072 45015 202 18869 45015 21280 136 27900 45011 595 45017 4 45012 45017 39087 45013 11445 45016 1605 3051 2559 45015 9202 5035 45015 43495 8426 6031 27900 4 5105 45016 15707 22102 45015 4934 8639 4700 22360 595 45017 4 45012 45017 4 3461 45016 1544 4 4 13333 46 45015 2516 4 4 45011 595 45017 10854 45012 45017 45013 4 45016 7990 179 5333 1778 45017 18 45015 2293 7599 3940 45011 595 45017 5614 45012 45017 45013 1615 45016 1605 452 15928 45015 952 1150 45011 595 45017 24414 45012 45017 45013 4 45016 4 6204 45015 4702 4 45017 45011 595 45017 3701 4 45017 45013 702 45016 4 13475 784 45015 9313 4529 40 9789 4 4 45017 5395 45012 45017 1646 40 1643 72 45013 581 1639 887 68 1689 1616 45019 9684 537 45018 9684 302 46 40 5707 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.868937 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.869499 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.869647 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 22 25 28 43 52 55 58 62 63 68 69 77 107 111 113 117 123 128 136 137\n",
            "I0629 17:37:37.869757 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45011 25954 401 45013 45011 6813 45013 30635 46 3100 18566 3701 5024 689 7124 20951 45012 15485 45011 595\n",
            "I0629 17:37:37.869859 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.869937 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.872419 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.872767 140555365861248 create_pretraining_data.py:151] tokens: [CLS] kirkj ##a has a very mild climate , similar to akraberg . it [MASK] the lowest frequency of frosts out [MASK] all weather stations included in [MASK] records [MASK] the [MASK] meteorologica ##l institute , with 36 days of euskaltzale in an average [MASK] . snow ##fall is uncommon demon due to mild temperatures [MASK] relative ##ly low precipitation [MASK] the weather station [MASK] somewh ##at high at 53 meters above sea level , [MASK] could possibl ##y [MASK] [MASK] data , but not as much as the previous stations [MASK] [SEP] nb ##yt ##es [SEP]\n",
            "I0629 17:37:37.873178 140555365861248 create_pretraining_data.py:161] input_ids: 2 19326 46 57 18 268 4182 579 45015 352 17 27060 45017 39 4 11 3871 1618 14 15410 148 4 84 2763 2264 405 16 4 1038 4 11 4 15915 233 1163 45015 33 5606 747 14 15619 16 41 565 4 45017 3019 4170 21 7022 21890 255 17 4182 2447 4 816 56 488 3848 4 11 2763 1092 4 18611 907 157 50 5572 4843 552 244 545 45015 4 208 44401 86 4 4 752 45015 62 58 24 221 24 11 1266 2264 4 3 19415 33371 197 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.873453 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.873722 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.874896 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 14 21 27 29 31 40 44 50 55 60 64 75 79 80 91 0 0 0 0 0\n",
            "I0629 17:37:37.875089 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 57 14 11 31 1675 6937 183 45015 15 45017 21 47 1748 11 45017 0 0 0 0 0\n",
            "I0629 17:37:37.875206 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.875293 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.876377 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.876682 140555365861248 create_pretraining_data.py:151] tokens: [CLS] reactivated in 2016 , the national guard serve ##s as the official primary military and police reserve service of the armed forces . it also double ##s as a force multiplier for law enforcement personnel duri ##ng [MASK] [MASK] and [MASK] reinforce military personnel [MASK] associates being deployed within france and abroad . [SEP] edith and otto ' s children were : both buried in st . alban ' s abbey , mainz . initially buried in the st mauri ##ce monastery , edith ' s tomb since the 16 ##th century has been located in magd ##eburg cathedral . long regarded as a cenotaph , [MASK] lead coff ##in inside [MASK] stone sarc ##ophag ##us with her name on it was found and opened in 2008 by archaeologists duri ##ng work on the building . an inscription recorded that it was [MASK] body of eadgyth , reb ##uri ##ed in 1510 . the fragmented and incomplete bones were examined in 2009 , then brought to bristol , england , for tests in 2010 . the investigation ##s at bristol , apply ##ing isotope tests on tooth enamel , check ##ed [MASK] [MASK] she was born and [MASK] up in wessex and mercia , [MASK] written history indicated . testing on the bones revealed that they are the remains of eadgyth , from study made of the enamel of the teeth in her upper jaw . testing of the enamel [MASK] that [MASK] individual entom ##bed [MASK] magd ##eburg had spent time as a [MASK] in the chalk ##y upland ##s of wessex . the bones are the oldest found of a member of english royalty . following the tests the bones were re - inter ##red in a new titani ##um coff ##in in her tomb at magd ##eburg cathedral ##ligand 22 [MASK] 2010 . [SEP]\n",
            "I0629 17:37:37.876953 140555365861248 create_pretraining_data.py:161] input_ids: 2 14771 16 1562 45015 11 207 2253 1021 19 24 11 734 1267 213 15 1295 2557 240 14 11 1142 462 45017 39 60 2129 19 24 18 333 12872 27 201 4300 2592 940 389 4 4 15 4 4544 213 2592 4 8937 142 3235 237 204 15 4272 45017 3 8102 15 4818 45010 294 560 53 45019 138 3329 16 447 45017 19879 45010 294 8033 45015 31828 45017 1296 3329 16 11 447 7703 1284 9404 45015 8102 45010 294 4904 131 11 713 108 159 57 65 750 16 13099 12329 5617 45017 259 1590 24 18 27128 45015 4 438 16311 189 2562 4 3442 29329 25246 306 33 143 170 34 39 26 188 15 1959 16 549 31 11679 940 389 150 34 11 990 45017 41 12516 947 29 39 26 4 679 14 11302 45015 34475 13730 35 16 17968 45017 11 11634 15 6343 6724 53 5720 16 760 45015 211 791 17 10200 45015 423 45015 27 2231 16 632 45017 11 3056 19 50 10200 45015 2718 28 23550 2231 34 19006 17055 45015 6564 35 4 4 232 26 992 15 4 130 16 7472 15 8617 45015 4 312 348 4879 45017 1918 34 11 6724 2901 29 76 36 11 1029 14 11302 45015 37 434 153 14 11 17055 14 11 7083 16 143 2092 14208 45017 1918 14 11 17055 4 29 4 609 22312 7922 4 13099 12329 61 1872 102 24 18 4 16 11 22419 86 9026 19 14 7472 45017 11 6724 36 11 2771 188 14 18 450 14 335 10362 45017 230 11 2231 11 6724 53 274 45016 988 4724 16 18 88 35600 1380 16311 189 16 143 4904 50 13099 12329 5617 38700 1576 4 632 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.877188 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.877407 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.877496 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 38 39 41 45 46 62 107 112 143 192 193 198 205 223 241 243 247 255 302 304\n",
            "I0629 17:37:37.877599 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 12275 33757 17 94 4770 45019 18 18 11 13724 4916 791 24 37 2901 11 50 2404 34 676\n",
            "I0629 17:37:37.877690 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.877763 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.881098 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.881490 140555365861248 create_pretraining_data.py:151] tokens: [CLS] margaret cameron , duri ##ng the brothers ' visit to the cameron estate in south carolina [MASK] represent ##ing the old south . meanwhile , young ben cameron idoliz ##es a picture of elsie stoneman . when the civil war arrives , the young men of both families enlist in thei ##r respective armies [MASK] the younger stoneman and two of the cameron brothers are killed in combat . meanwhile , the cameron women are rescue ##d by confederat ##e [MASK] who rout a black militia , after an attack [MASK] the cameron home . ben cameron lead ##s a heroic charge at the siege of [MASK] , earning the nickname of \" the ##izen colonel \" , but he is [MASK] wounded and captured . he is then taken to a union hospital in washington , d . c . [MASK] [MASK] his stay at the hospital [MASK] he is told that he will be hanged . also at the hospital , he meet ##s elsie [MASK] , whos ##e picture he has been carrying ; she is working there as a nurse . elsie takes cameron ' s mother , who had travele ##d to washington to tend her son , to see abraham lincoln , and mrs . cameron persuade ##s the president to pardon ben . when lincoln is assassinate ##d at ford ' s theatre , his concilia ##tory post ##war policy expire ##s with him . in the wake of the president ' s death , austin stoneman and other radical republicans are determined to punish the south , employ ##ing harsh measures that griffith depict [MASK] as havi ##ng been typical of the western ##sick . stoneman [MASK] his prot ##ege silas lynch , [MASK] psychopath ##ic mulatto , head to south carolina to observe the implementation of reconstruction policies firsthand . duri ##ng the [SEP] meanwhile , inspired by observing white children pretend ##ing to be ghosts to scare black children , ben fight ##s back by form ##ing the ku klu ##x klan . as a result , elsie , out of loyalty to her father , breaks off her relationship with ben . later , flora cameron goes off alone into the woods to fet ##ch water and is followed by gus , a freed ##man and soldier who is now a captain . he [MASK] [MASK] flora and tells her that he desires to get married . frighten ##ed , she flee ##s into the forest , pursued by gus . trapped on a precipi ##ce , flora warn ##s gus [MASK] will jump if he comes any closer . when he does , she leap ##s to her death . havi ##ng run through the forest look ##ing for her , ben has seen her jump ; he holds her as she dies , then carrie ##s her body back to the cameron home . in response , the klan hunt ##s down gus , trie ##s him , finds him guilty , and lynch ##es him . [SEP]\n",
            "I0629 17:37:37.881785 140555365861248 create_pretraining_data.py:161] input_ids: 2 5006 3384 45015 940 389 11 3493 45010 1973 17 11 3384 4914 16 262 6680 4 1240 28 11 400 262 45017 3213 45015 1046 2802 3384 30533 197 18 3825 14 6609 4558 45017 94 11 510 133 9685 45015 11 1046 392 14 138 1496 8068 16 10026 179 5028 4434 4 11 2896 4558 15 97 14 11 3384 3493 36 1271 16 2372 45017 3213 45015 11 3384 547 36 3020 48 31 13628 66 4 74 35924 18 672 11348 45015 85 41 788 4 11 3384 331 45017 2802 3384 438 19 18 9534 1779 50 11 6662 14 4 45015 6953 11 5418 14 45005 11 20140 8031 45005 45015 62 54 21 4 7529 15 3639 45017 54 21 211 610 17 18 363 3452 16 3055 45015 210 45017 206 45017 4 4 44 1869 50 11 3452 4 54 21 1444 29 54 169 42 10499 45017 60 50 11 3452 45015 54 1598 19 6609 4 45015 2741 66 3825 54 57 65 4011 45020 232 21 1187 89 24 18 9479 45017 6609 1372 3384 45010 294 1080 45015 74 61 43785 48 17 3055 17 1281 143 542 45015 17 626 4931 4613 45015 15 12022 45017 3384 7338 19 11 432 17 9306 2802 45017 94 4613 21 13818 48 50 9022 45010 294 1987 45015 44 21035 5117 457 1668 847 30499 19 33 164 45017 16 11 5713 14 11 432 45010 294 344 45015 9495 4558 15 69 2807 13632 36 1917 17 4955 11 262 45015 2692 28 6632 2677 29 2501 5042 4 24 23762 389 65 1891 14 11 533 42279 45017 4558 4 44 36399 36137 14814 5204 45015 4 24105 282 20774 45015 730 17 262 6680 17 12561 11 3185 14 2539 2267 30502 45017 940 389 11 3 3213 45015 2141 31 9481 635 560 13317 28 17 42 18315 17 36308 672 560 45015 2802 3302 19 346 31 182 28 11 4096 44592 961 3707 45017 24 18 295 45015 6609 45015 148 14 8126 17 143 790 45015 9087 678 143 1475 33 2802 45017 139 45015 7799 3384 3346 678 2024 96 11 6178 17 31443 1822 332 15 21 778 31 8379 45015 18 7267 767 15 10116 74 21 258 18 2221 45017 54 4 4 7799 15 2782 143 29 54 23871 17 1025 1536 45017 29848 35 45015 232 8346 19 96 11 1979 45015 8201 31 8379 45017 10408 34 18 39781 1284 45015 7799 13760 19 8379 4 169 4950 155 54 1823 154 4028 45017 94 54 373 45015 232 13914 19 17 143 344 45017 23762 389 755 176 11 1979 1508 28 27 143 45015 2802 57 494 143 4950 45020 54 2409 143 24 232 19876 45015 211 6146 19 143 679 346 17 11 3384 331 45017 16 1172 45015 11 3707 5118 19 417 8379 45015 31990 19 164 45015 4946 164 7507 45015 15 5204 197 164 45017 3\n",
            "I0629 17:37:37.882016 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.882227 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.882317 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 17 55 81 91 107 115 122 142 143 149 168 272 273 281 282 285 292 396 397 433\n",
            "I0629 17:37:37.882406 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45015 45017 1974 34 6367 764 60 940 389 45015 4558 5042 19 2539 1036 15 18 6581 19 232\n",
            "I0629 17:37:37.882497 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.882585 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 17:37:37.883571 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.883696 140555365861248 create_pretraining_data.py:151] tokens: [CLS] unlock ##confirm [SEP] [MASK] [SEP]\n",
            "I0629 17:37:37.883924 140555365861248 create_pretraining_data.py:161] input_ids: 2 23501 20046 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.884141 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.884350 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.884439 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.884517 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 21872 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.884622 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.884697 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.885636 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.885776 140555365861248 create_pretraining_data.py:151] tokens: [CLS] commercial eater ##ies existed duri ##ng the roman period [MASK] with [MASK] of 150 \" thermo ##poli ##a \" , a form of fast food [MASK] , found in pompe ##ii referenc and urban [MASK] of tourist foods [MASK] have existed in china duri ##ng the song dynasty . [SEP] zientzia eta [MASK] [MASK] jaiotza ##k heriotzak - - - - agintari ##ak [SEP]\n",
            "I0629 17:37:37.886005 140555365861248 create_pretraining_data.py:161] input_ids: 2 835 25407 784 2996 940 389 11 525 236 4 33 4 14 3810 45005 18821 12049 46 45005 45015 18 182 14 2877 222 4 45015 188 16 22180 12307 15914 15 1222 4 14 3110 950 4 55 2996 16 375 940 389 11 376 630 45017 3 383 40 4 4 655 72 503 45016 45016 45016 45016 861 106 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.886217 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.886431 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.886516 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 10 12 14 26 32 35 37 39 53 54 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.981138 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45015 475 3810 3816 45015 4337 3345 87 505 509 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.981422 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.981549 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.983632 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.983776 140555365861248 create_pretraining_data.py:151] tokens: [CLS] [MASK] [SEP] unexpected [SEP]\n",
            "I0629 17:37:37.984028 140555365861248 create_pretraining_data.py:161] input_ids: 2 4 3 7473 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.984249 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.984471 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.984573 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.984657 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45013 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.984744 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.984815 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.985812 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.986067 140555365861248 create_pretraining_data.py:151] tokens: [CLS] copyrightpage ##name [SEP] duri ##ng jack ##ie mason ' [MASK] october 1964 performance on a show that had been shortened by ten minutes [MASK] to an address by president lynd ##on johnson , sullivan — on - stage but off - cho — signal ##ed mason that he had two minutes left by holding up two fingers . sullivan ' s signal distract ##ed the studio audience , and to television viewers unaware of the circumstances , it seemed as though mason ' s joke ##s were falling flat . mason , motor a bid to regain the audience ' s attention , cri ##ed , \" i ' m gett ##ing fingers here ! \" [MASK] [MASK] his own frantic hand gesture : \" here ' s a finger for [MASK] ! \" videotape ##s of the incident are inc ##on ##clu ##sive as to whet ##her mason ' s ups ##wept hand ( which was just off - camera ) was intended to be an ind ##ece ##nt gesture , but sullivan was convince ##d that it was , and banned mason from future appearances on the program . mason later insist ##ed that he did not know what the \" middle finger \" meant , and that he did not make the gesture anyway . in september 1965 , sullivan — who , according to mason , [MASK] \" deeply apologetic \" — brought mason on the show for a [MASK] surprise grand re ##union \" . \" he [MASK] they were old pal ##s , \" nachman wrote , \" news to mason [MASK] who never got a repeat invitation . \" [MASK] added that his earning power \" . . [MASK] was cut right in [MASK] after [MASK] . i never [MASK] worked my way back until i opened on broadway in 1986 . \" [SEP]\n",
            "I0629 17:37:37.986316 140555365861248 create_pretraining_data.py:161] input_ids: 2 29463 6479 3 940 389 2981 1374 5222 45010 4 676 2779 1031 34 18 343 29 61 65 6262 31 1277 2788 4 17 41 1939 31 432 24253 191 3980 45015 539 850 34 45016 1035 62 678 45016 36566 850 1059 35 5222 29 54 61 97 2788 308 31 3641 130 97 8349 45017 539 45010 294 1059 18568 35 11 2168 1785 45015 15 17 973 14001 8327 14 11 2650 45015 39 4837 24 323 5222 45010 294 24631 19 53 5070 3950 45017 5222 45015 2365 18 11112 17 6814 11 1785 45010 294 1731 45015 33724 35 45015 45005 135 45010 319 10163 28 8349 1219 45004 45005 4 4 44 269 18736 735 14248 45019 45005 1219 45010 294 18 3502 27 4 45004 45005 20872 19 14 11 3680 36 3764 191 43750 18591 24 17 13724 4916 5222 45010 294 33508 12163 735 45011 47 26 599 678 45016 3651 45012 26 1262 17 42 41 36210 18580 3168 14248 45015 62 539 26 7386 48 29 39 26 45015 15 4117 5222 37 1078 5344 34 11 429 45017 5222 139 9215 35 29 54 278 58 1630 305 11 45005 575 3502 45005 1841 45015 15 29 54 278 58 326 11 14248 10448 45017 16 625 3003 45015 539 850 74 45015 299 17 5222 45015 4 45005 5248 26937 45005 850 791 5222 34 11 343 27 18 4 8851 1744 274 7876 45005 45017 45005 54 4 76 53 400 7685 19 45015 45005 11340 364 45015 45005 2426 17 5222 4 74 597 3265 18 5406 10470 45017 45005 4 1216 29 44 6953 263 45005 45017 45017 4 26 2758 516 16 4 85 4 45017 135 597 4 1661 1316 371 346 254 135 1959 34 6673 16 2393 45017 45005 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.986541 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.986774 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.986878 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 9 10 24 42 93 100 117 118 128 132 192 231 244 253 268 277 286 291 293 297\n",
            "I0629 17:37:37.986969 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45010 294 255 3651 16 45010 15 153 294 763 5222 26 45005 425 45015 5222 45017 573 29 3769\n",
            "I0629 17:37:37.987058 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.987132 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.988137 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.988361 140555365861248 create_pretraining_data.py:151] tokens: [CLS] where formula _ 60 is the universal gas constant and [MASK] = pv / ( r ##t [MASK] is the compressi ##bility factor . in 1972 [MASK] . soave replaced the 1 / term of [MASK] redlich [MASK] kw ##ong equation with a function [UNK] ( [MASK] , settling ) involv ##ing the temperature and the ace ##nt ##ric factor ( the result ##ing equation is also known as the soave - redlich - kw ##ong equation of state ; s ##rk eos ) . the [UNK] function was devise ##d to fit the vapor [MASK] data of hydro ##carbons and the equation does fair ##ly well for these [MASK] . note especially that this [MASK] changes the definition of \" a \" slightly , as [MASK] formula [MASK] 61 is now to the second power . the s ##rk [MASK] may be written as where where formula [MASK] 54 and other parts of the s ##rk eos is defined in the s ##rk eos section . [MASK] downs ##ide of [MASK] s ##rk [MASK] , and other cubic eos , is that the liquid molar volume is significantly less [MASK] than the gas molar volume . penelo ##ux et ali ##os ( 1982 ) proposed a simple correction for this by introduc ##ing a volume translation [SEP] erabiltzaile kontua sortu da , baina ez da saioa hasi . ( e ) k cookie ##ak erabiltzen ditu [MASK] [MASK] eta ezgaituta dauzka ##zu . gaitu itza ##zu mesedez , eta ondoren saiat ##u saioa hasten zure erabiltzaile izen eta pasahitz berriak erabiliz . [SEP]\n",
            "I0629 17:37:37.988616 140555365861248 create_pretraining_data.py:161] input_ids: 2 118 231 45030 2830 21 11 1595 605 1381 15 4 45022 23055 45018 45011 725 121 4 21 11 34542 10963 1470 45017 16 3165 4 45017 12991 1123 11 277 45018 246 14 4 15167 4 15392 16116 1102 33 18 283 1 45011 4 45015 10321 45012 2874 28 11 647 15 11 34092 3168 4948 1470 45011 11 295 28 1102 21 60 127 24 11 12991 45016 15167 45016 15392 16116 1102 14 172 45020 294 11955 5746 45012 45017 11 1 283 26 15735 48 17 2720 11 5680 4 752 14 6044 16282 15 11 1102 373 395 56 147 27 91 4 45017 2485 540 29 51 4 614 11 995 14 45005 18 45005 2183 45015 24 4 231 4 6523 21 258 17 11 215 263 45017 11 294 11955 4 87 42 312 24 118 118 231 4 5939 15 69 738 14 11 294 11955 5746 21 706 16 11 294 11955 5746 1623 45017 4 13842 7182 14 4 294 11955 4 45015 15 69 5925 5746 45015 21 29 11 3115 6687 1504 21 2059 374 4 92 11 605 6687 1504 45017 24360 13350 1901 5892 2550 45011 1845 45012 1427 18 982 9710 27 51 31 4862 28 18 1504 1714 3 3556 12208 921 112 45015 852 196 112 4024 923 45017 45011 284 45012 440 22652 106 1870 1308 4 4 40 12623 14970 10741 45017 24596 44638 10741 5963 45015 40 1529 15691 229 4024 9015 3675 3556 5089 40 10996 3996 5448 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.988836 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.989056 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:38.084238 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 11 18 27 36 38 47 49 96 110 116 127 129 141 149 168 172 175 191 238 239\n",
            "I0629 17:37:38.084521 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 2494 45012 564 11 45016 223 1 854 1983 4378 11 45030 5746 45030 18 11 5746 3606 8581 2429\n",
            "I0629 17:37:38.084726 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:38.085001 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:38:14.223631 140555365861248 create_pretraining_data.py:166] Wrote 45311 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "76322cd8-2820-41ab-832c-a8a1131d4d3b",
        "id": "o1hyibNBPg8o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "!python src/run_pretraining.py \\\n",
        "  --config_file eu.congif.ini \\\n",
        "  --input_file=$GS/pretraining.tf.data \\\n",
        "  --output_dir=$GS/eu.gureBERT \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=1000000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "'''\n",
        "\n",
        "!python bert/run_pretraining.py \\\n",
        "  --input_file=$GS/wordpiece/pretraining-en_eu.tf.data \\ \\\n",
        "  --output_dir=$GS/wordpiece/model-en_eu \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=20000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  --init_checkpoint=$GS/wordpiece/model-en_eu/ \\\n",
        "  #--num_train_steps=1000000 \\\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 20:18:10.190464 140712882820992 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 20:18:10.191617 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:493: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 20:18:10.192262 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:407: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 20:18:10.192418 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:407: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 20:18:10.192595 140712882820992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 20:18:10.193293 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:414: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0629 20:18:11.568209 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:418: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 20:18:11.736206 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:420: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 20:18:11.736473 140712882820992 run_pretraining.py:420] *** Input Files ***\n",
            "I0629 20:18:11.736608 140712882820992 run_pretraining.py:422]   gs://gurebert/gureBERT/wordpiece/pretraining-en_eu.tf.data\n",
            "W0629 20:18:12.787111 140712882820992 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 20:18:13.792729 140712882820992 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7ffa0f492950>) includes params argument, but params are not passed to Estimator.\n",
            "I0629 20:18:13.794410 140712882820992 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/wordpiece/model-en_eu', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.2.81.2:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa1b8f8908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.2.81.2:8470', '_evaluation_master': 'grpc://10.2.81.2:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7ffa1b901320>}\n",
            "I0629 20:18:13.794831 140712882820992 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0629 20:18:13.795627 140712882820992 run_pretraining.py:459] ***** Running training *****\n",
            "I0629 20:18:13.795734 140712882820992 run_pretraining.py:460]   Batch size = 64\n",
            "I0629 20:18:16.023085 140712882820992 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.2.81.2:8470) for TPU system metadata.\n",
            "2019-06-29 20:18:16.024497: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 20:18:16.039941 140712882820992 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0629 20:18:16.040171 140712882820992 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0629 20:18:16.040273 140712882820992 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0629 20:18:16.040341 140712882820992 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0629 20:18:16.040412 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5201560055092340784)\n",
            "I0629 20:18:16.041247 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15235922008977912019)\n",
            "I0629 20:18:16.041333 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16587921834020422802)\n",
            "I0629 20:18:16.041401 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 18249457006028921626)\n",
            "I0629 20:18:16.041469 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9769923856279949422)\n",
            "I0629 20:18:16.041539 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14707561106018069366)\n",
            "I0629 20:18:16.041627 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11546342222793515035)\n",
            "I0629 20:18:16.041691 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15225485262720068302)\n",
            "I0629 20:18:16.041753 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 18107303912184056565)\n",
            "I0629 20:18:16.041816 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12015888753985987776)\n",
            "I0629 20:18:16.041878 140712882820992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6671482654539601268)\n",
            "W0629 20:18:16.048361 140712882820992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0629 20:18:16.062718 140712882820992 estimator.py:1145] Calling model_fn.\n",
            "W0629 20:18:16.063365 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0629 20:18:16.069745 140712882820992 deprecation.py:323] From bert/run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0629 20:18:16.069996 140712882820992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0629 20:18:16.098318 140712882820992 deprecation.py:323] From bert/run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0629 20:18:16.098724 140712882820992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0629 20:18:16.100428 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:393: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0629 20:18:16.106316 140712882820992 deprecation.py:323] From bert/run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0629 20:18:16.208067 140712882820992 run_pretraining.py:117] *** Features ***\n",
            "I0629 20:18:16.208323 140712882820992 run_pretraining.py:119]   name = input_ids, shape = (8, 512)\n",
            "I0629 20:18:16.208430 140712882820992 run_pretraining.py:119]   name = input_mask, shape = (8, 512)\n",
            "I0629 20:18:16.208518 140712882820992 run_pretraining.py:119]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0629 20:18:16.208627 140712882820992 run_pretraining.py:119]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0629 20:18:16.208711 140712882820992 run_pretraining.py:119]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0629 20:18:16.208802 140712882820992 run_pretraining.py:119]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0629 20:18:16.208881 140712882820992 run_pretraining.py:119]   name = segment_ids, shape = (8, 512)\n",
            "W0629 20:18:16.209119 140712882820992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0629 20:18:16.212249 140712882820992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0629 20:18:16.249358 140712882820992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0629 20:18:16.420191 140712882820992 deprecation.py:506] From /content/gureBERT/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0629 20:18:16.443064 140712882820992 deprecation.py:323] From /content/gureBERT/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0629 20:18:21.445815 140712882820992 run_pretraining.py:167] **** Trainable Variables ****\n",
            "I0629 20:18:21.446103 140712882820992 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.446246 140712882820992 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.446344 140712882820992 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.446436 140712882820992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.446524 140712882820992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.446627 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.446714 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.446801 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.446882 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.446961 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447047 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447120 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447195 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447268 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447348 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447426 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447506 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447597 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447679 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447763 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447840 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447915 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.447994 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448069 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448148 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448224 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448312 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448394 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448474 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448548 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448659 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448736 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448820 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448902 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.448981 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449058 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449133 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449207 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449286 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449363 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449442 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449516 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449611 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449687 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449773 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449850 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.449946 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450024 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450119 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450196 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450276 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450352 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450427 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450503 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450596 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450672 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450762 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450847 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.450933 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.451058 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.451157 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.451239 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.451352 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.451436 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.451525 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.451637 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.451730 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.451836 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.451944 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.452035 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.452130 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.452217 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.452311 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.452417 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.452528 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.452681 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.452815 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.452931 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.453043 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.453153 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.453267 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.453393 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.453513 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.453653 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.453777 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.453893 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.454011 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.454123 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.454240 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.454353 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.454471 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.454604 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.454743 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.454874 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.454986 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.455096 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.455214 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.455337 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.455455 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.455584 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.455698 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.455820 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.455958 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.456075 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.456196 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.456311 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.456433 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.456570 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.456698 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.456827 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.456944 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.457060 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.457182 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.457298 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.457422 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.457536 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.457676 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.457804 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.457930 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.458045 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.458165 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.458280 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.458404 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.458518 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.458668 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.458798 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.458914 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.459040 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.459165 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.459281 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.459402 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.459516 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.459653 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.459793 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.459916 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.460033 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.460156 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.460269 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.460390 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.460541 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.460690 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.460818 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.460952 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.461063 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.461178 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.461304 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.461428 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.461541 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.461675 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.461801 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.461921 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.462030 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.462144 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.462257 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.462376 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.462487 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.462625 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.462741 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.462864 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.462972 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.463086 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.463209 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.463321 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.463427 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.463535 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.463666 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.463790 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.463898 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.464011 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.464119 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.464230 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.464332 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.464444 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.464567 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.464679 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.464798 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.464913 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.465018 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.465130 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.465238 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.465343 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.465448 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.465578 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.465692 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.465816 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.465924 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.466040 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.466148 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.466260 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.466367 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.466473 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.466615 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.466730 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.466852 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.466967 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.467075 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.467177 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.467281 140712882820992 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.467395 140712882820992 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.467500 140712882820992 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.467633 140712882820992 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.467745 140712882820992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.467862 140712882820992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.467967 140712882820992 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30000,), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.468074 140712882820992 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:18:21.468186 140712882820992 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,), *INIT_FROM_CKPT*\n",
            "W0629 20:18:21.468390 140712882820992 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0629 20:18:21.470288 140712882820992 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0629 20:18:21.479325 140712882820992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0629 20:18:21.804603 140712882820992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0629 20:18:35.540236 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:160: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "W0629 20:18:37.729781 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:161: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
            "\n",
            "I0629 20:18:38.744231 140712882820992 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0629 20:18:39.152412 140712882820992 estimator.py:1147] Done calling model_fn.\n",
            "I0629 20:18:42.854603 140712882820992 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 20:18:44.657225 140712882820992 monitored_session.py:240] Graph was finalized.\n",
            "W0629 20:18:44.819008 140712882820992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0629 20:18:44.981522 140712882820992 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/wordpiece/model-en_eu/model.ckpt-10000\n",
            "W0629 20:19:10.638264 140712882820992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0629 20:19:12.828870 140712882820992 session_manager.py:500] Running local_init_op.\n",
            "I0629 20:19:13.993973 140712882820992 session_manager.py:502] Done running local_init_op.\n",
            "I0629 20:19:27.309626 140712882820992 basic_session_run_hooks.py:606] Saving checkpoints for 10000 into gs://gurebert/gureBERT/wordpiece/model-en_eu/model.ckpt.\n",
            "W0629 20:19:57.611086 140712882820992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0629 20:19:59.973348 140712882820992 util.py:98] Initialized dataset iterators in 1 seconds\n",
            "I0629 20:19:59.974666 140712882820992 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-06-29 20:19:59.975128: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 20:19:59.980675 140712882820992 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0629 20:19:59.983019 140712882820992 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0629 20:19:59.988372 140712882820992 tpu_estimator.py:557] Init TPU system\n",
            "I0629 20:20:04.682377 140712882820992 tpu_estimator.py:566] Initialized TPU in 4 seconds\n",
            "I0629 20:20:04.683443 140711723276032 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 20:20:04.684202 140711695226624 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 20:20:05.821498 140712882820992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 20:20:05.822628 140712882820992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 20:20:39.903292 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 20:21:39.958231 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (0, 252)\n",
            "I0629 20:22:40.011874 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (0, 504)\n",
            "I0629 20:23:40.066781 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (0, 756)\n",
            "I0629 20:24:39.755668 140712882820992 basic_session_run_hooks.py:262] loss = 3.2556705, step = 11000\n",
            "I0629 20:24:39.759305 140712882820992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 20:24:39.759849 140712882820992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 20:24:49.781334 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (1, 0)\n",
            "I0629 20:25:49.836062 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (1, 252)\n",
            "I0629 20:26:49.891644 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (1, 504)\n",
            "I0629 20:27:49.947203 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (1, 756)\n",
            "I0629 20:28:48.755933 140712882820992 basic_session_run_hooks.py:260] loss = 2.9618354, step = 12000 (249.000 sec)\n",
            "I0629 20:28:48.758151 140712882820992 tpu_estimator.py:2159] global_step/sec: 4.01606\n",
            "I0629 20:28:48.759306 140712882820992 tpu_estimator.py:2160] examples/sec: 257.028\n",
            "I0629 20:28:49.683512 140712882820992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 20:28:49.683853 140712882820992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 20:28:51.041439 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (2, 0)\n",
            "I0629 20:29:51.095525 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (2, 252)\n",
            "I0629 20:30:51.151162 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (2, 504)\n",
            "I0629 20:31:51.206058 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (2, 756)\n",
            "I0629 20:32:49.997325 140712882820992 basic_session_run_hooks.py:260] loss = 2.8716023, step = 13000 (241.241 sec)\n",
            "I0629 20:32:49.999362 140712882820992 tpu_estimator.py:2159] global_step/sec: 4.14523\n",
            "I0629 20:32:49.999823 140712882820992 tpu_estimator.py:2160] examples/sec: 265.295\n",
            "I0629 20:32:50.959680 140712882820992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 20:32:50.960015 140712882820992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 20:32:52.333502 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (3, 0)\n",
            "I0629 20:33:52.388526 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (3, 252)\n",
            "I0629 20:34:52.445987 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (3, 504)\n",
            "I0629 20:35:52.500709 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (3, 756)\n",
            "I0629 20:36:52.313234 140712882820992 basic_session_run_hooks.py:260] loss = 2.460558, step = 14000 (242.316 sec)\n",
            "I0629 20:36:52.315257 140712882820992 tpu_estimator.py:2159] global_step/sec: 4.12685\n",
            "I0629 20:36:52.315517 140712882820992 tpu_estimator.py:2160] examples/sec: 264.118\n",
            "I0629 20:36:52.316992 140712882820992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 20:36:52.317208 140712882820992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 20:36:53.665136 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (4, 0)\n",
            "I0629 20:37:53.719744 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (4, 252)\n",
            "I0629 20:38:53.777186 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (4, 504)\n",
            "I0629 20:39:53.833005 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (4, 756)\n",
            "I0629 20:40:52.622029 140712882820992 basic_session_run_hooks.py:260] loss = 1.7457441, step = 15000 (240.309 sec)\n",
            "I0629 20:40:52.624217 140712882820992 tpu_estimator.py:2159] global_step/sec: 4.16131\n",
            "I0629 20:40:52.624454 140712882820992 tpu_estimator.py:2160] examples/sec: 266.324\n",
            "I0629 20:40:54.353357 140712882820992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 20:40:54.354184 140712882820992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 20:40:55.710204 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (5, 0)\n",
            "I0629 20:41:55.761606 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (5, 252)\n",
            "I0629 20:42:55.813831 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (5, 504)\n",
            "I0629 20:43:55.865823 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (5, 756)\n",
            "I0629 20:44:54.594416 140712882820992 basic_session_run_hooks.py:260] loss = 1.7847469, step = 16000 (241.972 sec)\n",
            "I0629 20:44:54.596583 140712882820992 tpu_estimator.py:2159] global_step/sec: 4.1327\n",
            "I0629 20:44:54.596846 140712882820992 tpu_estimator.py:2160] examples/sec: 264.493\n",
            "I0629 20:44:55.617458 140712882820992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 20:44:55.617993 140712882820992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 20:44:57.044981 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (6, 0)\n",
            "I0629 20:45:57.099756 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (6, 252)\n",
            "I0629 20:46:57.154464 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (6, 504)\n",
            "I0629 20:47:57.209594 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (6, 756)\n",
            "I0629 20:48:56.870232 140712882820992 basic_session_run_hooks.py:260] loss = 2.1283202, step = 17000 (242.276 sec)\n",
            "I0629 20:48:56.872080 140712882820992 tpu_estimator.py:2159] global_step/sec: 4.12753\n",
            "I0629 20:48:56.872458 140712882820992 tpu_estimator.py:2160] examples/sec: 264.162\n",
            "I0629 20:48:56.874208 140712882820992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 20:48:56.874437 140712882820992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 20:48:58.217583 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (7, 0)\n",
            "I0629 20:49:58.275308 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (7, 252)\n",
            "I0629 20:50:58.332636 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (7, 504)\n",
            "I0629 20:51:58.390512 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (7, 756)\n",
            "I0629 20:52:57.165342 140712882820992 basic_session_run_hooks.py:260] loss = 1.6025633, step = 18000 (240.295 sec)\n",
            "I0629 20:52:57.167861 140712882820992 tpu_estimator.py:2159] global_step/sec: 4.16154\n",
            "I0629 20:52:57.168198 140712882820992 tpu_estimator.py:2160] examples/sec: 266.339\n",
            "I0629 20:52:58.159711 140712882820992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 20:52:58.160210 140712882820992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 20:52:59.555979 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (8, 0)\n",
            "I0629 20:53:59.609657 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (8, 252)\n",
            "I0629 20:54:59.663789 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (8, 504)\n",
            "I0629 20:55:59.717403 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (8, 756)\n",
            "I0629 20:56:58.526847 140712882820992 basic_session_run_hooks.py:260] loss = 1.2746632, step = 19000 (241.362 sec)\n",
            "I0629 20:56:58.528596 140712882820992 tpu_estimator.py:2159] global_step/sec: 4.14317\n",
            "I0629 20:56:58.528784 140712882820992 tpu_estimator.py:2160] examples/sec: 265.163\n",
            "I0629 20:56:59.451761 140712882820992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 20:56:59.452306 140712882820992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 20:57:00.892508 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (9, 0)\n",
            "I0629 20:58:00.948791 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (9, 252)\n",
            "I0629 20:59:01.006361 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (9, 504)\n",
            "I0629 21:00:01.064506 140711695226624 tpu_estimator.py:275] Outfeed finished for iteration (9, 756)\n",
            "I0629 21:00:59.811510 140712882820992 basic_session_run_hooks.py:606] Saving checkpoints for 20000 into gs://gurebert/gureBERT/wordpiece/model-en_eu/model.ckpt.\n",
            "I0629 21:01:28.129186 140712882820992 basic_session_run_hooks.py:260] loss = 1.7434584, step = 20000 (269.602 sec)\n",
            "I0629 21:01:28.131376 140712882820992 tpu_estimator.py:2159] global_step/sec: 3.70916\n",
            "I0629 21:01:28.131606 140712882820992 tpu_estimator.py:2160] examples/sec: 237.386\n",
            "I0629 21:01:29.381633 140712882820992 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 21:01:29.381961 140712882820992 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 21:01:29.382301 140711723276032 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 21:01:29.382711 140711723276032 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 21:01:29.382988 140712882820992 error_handling.py:96] infeed marked as finished\n",
            "I0629 21:01:29.383163 140712882820992 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 21:01:29.383226 140712882820992 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 21:01:29.383388 140711695226624 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 21:01:29.383481 140711695226624 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 21:01:29.383973 140712882820992 error_handling.py:96] outfeed marked as finished\n",
            "I0629 21:01:29.384069 140712882820992 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 21:01:31.357252 140712882820992 estimator.py:368] Loss for final step: 1.7434584.\n",
            "I0629 21:01:31.358864 140712882820992 error_handling.py:96] training_loop marked as finished\n",
            "I0629 21:01:31.359066 140712882820992 run_pretraining.py:469] ***** Running evaluation *****\n",
            "I0629 21:01:31.359169 140712882820992 run_pretraining.py:470]   Batch size = 8\n",
            "I0629 21:01:32.074526 140712882820992 estimator.py:1145] Calling model_fn.\n",
            "I0629 21:01:32.182905 140712882820992 run_pretraining.py:117] *** Features ***\n",
            "I0629 21:01:32.183198 140712882820992 run_pretraining.py:119]   name = input_ids, shape = (1, 512)\n",
            "I0629 21:01:32.183317 140712882820992 run_pretraining.py:119]   name = input_mask, shape = (1, 512)\n",
            "I0629 21:01:32.183414 140712882820992 run_pretraining.py:119]   name = masked_lm_ids, shape = (1, 20)\n",
            "I0629 21:01:32.183507 140712882820992 run_pretraining.py:119]   name = masked_lm_positions, shape = (1, 20)\n",
            "I0629 21:01:32.183621 140712882820992 run_pretraining.py:119]   name = masked_lm_weights, shape = (1, 20)\n",
            "I0629 21:01:32.183720 140712882820992 run_pretraining.py:119]   name = next_sentence_labels, shape = (1, 1)\n",
            "I0629 21:01:32.183808 140712882820992 run_pretraining.py:119]   name = segment_ids, shape = (1, 512)\n",
            "I0629 21:01:36.538098 140712882820992 run_pretraining.py:167] **** Trainable Variables ****\n",
            "I0629 21:01:36.538389 140712882820992 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.538537 140712882820992 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.538654 140712882820992 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.538748 140712882820992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.538839 140712882820992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.538924 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539008 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539088 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539169 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539245 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539324 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539402 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539491 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539582 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539665 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539742 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539822 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539899 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.539982 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540059 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540135 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540215 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540301 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540378 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540485 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540585 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540669 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540747 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540827 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540902 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.540979 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541055 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541135 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541212 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541290 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541366 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541446 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541522 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541626 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541707 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541787 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541864 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.541946 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542020 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542099 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542172 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542253 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542329 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542420 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542499 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542597 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542676 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542753 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542829 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542908 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.542984 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543061 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543134 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543211 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543286 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543369 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543454 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543531 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543623 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543705 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543781 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543860 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.543937 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544015 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544091 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544176 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544251 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544328 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544401 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544494 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544587 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544671 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544746 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544825 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544898 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.544981 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545056 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545131 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545204 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545277 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545353 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545438 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545512 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545603 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545679 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545756 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545829 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545907 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.545982 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546056 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546136 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546217 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546303 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546383 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546466 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546542 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546636 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546717 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546793 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546871 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.546951 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547034 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547110 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547188 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547263 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547339 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547421 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547503 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547598 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547680 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547758 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547835 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547911 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.547993 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.548070 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.610738 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.611024 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.611167 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.611275 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.611381 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.611511 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.611647 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.611758 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.611881 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.611989 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.612102 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.612209 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.612317 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.612435 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.612578 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.612707 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.612829 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.612938 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.613053 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.613164 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.613279 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.613386 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.613504 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.613634 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.613752 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.613860 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.613972 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.614081 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.614186 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.614293 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.614406 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.614528 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.614661 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.614773 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.614887 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.614995 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.615106 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.615215 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.615321 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.615435 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.615566 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.615680 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.615792 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.615894 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.616004 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.616114 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.616228 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.616338 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.616463 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.616589 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.616704 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.616817 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.616930 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.617036 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.617141 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.617248 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.617358 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.617473 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.617601 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.617714 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.617820 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.617924 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.618037 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.618144 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.618255 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.618361 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.618484 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.618609 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.618722 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.618829 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.618935 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.619042 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.619150 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.619256 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.619369 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.619482 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.619605 140712882820992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.619714 140712882820992 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.619827 140712882820992 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.619933 140712882820992 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.620046 140712882820992 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.620153 140712882820992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.620257 140712882820992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.620360 140712882820992 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30000,), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.620496 140712882820992 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "I0629 21:01:36.620630 140712882820992 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,), *INIT_FROM_CKPT*\n",
            "W0629 21:01:38.396276 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:198: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
            "\n",
            "W0629 21:01:38.413738 140712882820992 deprecation_wrapper.py:119] From bert/run_pretraining.py:202: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "I0629 21:01:39.472394 140712882820992 estimator.py:1147] Done calling model_fn.\n",
            "I0629 21:01:39.491189 140712882820992 evaluation.py:255] Starting evaluation at 2019-06-29T21:01:39Z\n",
            "I0629 21:01:39.491476 140712882820992 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 21:01:40.949216 140712882820992 monitored_session.py:240] Graph was finalized.\n",
            "I0629 21:01:41.114040 140712882820992 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/wordpiece/model-en_eu/model.ckpt-20000\n",
            "I0629 21:02:47.801605 140712882820992 session_manager.py:500] Running local_init_op.\n",
            "I0629 21:02:48.018493 140712882820992 session_manager.py:502] Done running local_init_op.\n",
            "I0629 21:02:48.586952 140712882820992 tpu_estimator.py:557] Init TPU system\n",
            "I0629 21:02:57.110647 140712882820992 tpu_estimator.py:566] Initialized TPU in 8 seconds\n",
            "I0629 21:02:57.111933 140711899186944 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 21:02:57.112710 140711890794240 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 21:02:57.361456 140712882820992 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0629 21:02:57.567521 140712882820992 tpu_estimator.py:590] Enqueue next (100) batch(es) of data to infeed.\n",
            "I0629 21:02:57.567899 140712882820992 tpu_estimator.py:594] Dequeue next (100) batch(es) of data from outfeed.\n",
            "I0629 21:03:03.065591 140711890794240 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 21:03:04.321702 140712882820992 evaluation.py:167] Evaluation [100/100]\n",
            "I0629 21:03:04.322143 140712882820992 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 21:03:04.322240 140712882820992 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 21:03:04.322529 140711899186944 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 21:03:04.322635 140711899186944 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 21:03:04.323579 140712882820992 error_handling.py:96] infeed marked as finished\n",
            "I0629 21:03:04.323889 140712882820992 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 21:03:04.323980 140712882820992 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 21:03:04.946518 140711890794240 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 21:03:04.946842 140711890794240 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 21:03:04.947118 140712882820992 error_handling.py:96] outfeed marked as finished\n",
            "I0629 21:03:04.947399 140712882820992 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 21:03:05.587191 140712882820992 evaluation.py:275] Finished evaluation at 2019-06-29-21:03:05\n",
            "I0629 21:03:05.587611 140712882820992 estimator.py:2039] Saving dict for global step 20000: global_step = 20000, loss = 0.7622752, masked_lm_accuracy = 0.6767516, masked_lm_loss = 0.8268516, next_sentence_accuracy = 1.0, next_sentence_loss = 0.00072842697\n",
            "I0629 21:03:08.830130 140712882820992 estimator.py:2099] Saving 'checkpoint_path' summary for global step 20000: gs://gurebert/gureBERT/wordpiece/model-en_eu/model.ckpt-20000\n",
            "I0629 21:03:09.673155 140712882820992 error_handling.py:96] evaluation_loop marked as finished\n",
            "I0629 21:03:09.674668 140712882820992 run_pretraining.py:483] ***** Eval results *****\n",
            "I0629 21:03:09.674855 140712882820992 run_pretraining.py:485]   global_step = 20000\n",
            "I0629 21:03:09.675244 140712882820992 run_pretraining.py:485]   loss = 0.7622752\n",
            "I0629 21:03:09.675373 140712882820992 run_pretraining.py:485]   masked_lm_accuracy = 0.6767516\n",
            "I0629 21:03:09.675453 140712882820992 run_pretraining.py:485]   masked_lm_loss = 0.8268516\n",
            "I0629 21:03:09.675528 140712882820992 run_pretraining.py:485]   next_sentence_accuracy = 1.0\n",
            "I0629 21:03:09.675620 140712882820992 run_pretraining.py:485]   next_sentence_loss = 0.00072842697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zAbTlosgZiP",
        "colab_type": "text"
      },
      "source": [
        "### SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrHSiJmsa_up",
        "colab_type": "code",
        "outputId": "1eb159dd-ad35-422c-95a6-68b284effd09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# data English da!! baino kodea badabil \n",
        "\n",
        "!python bert/run_squad.py \\\n",
        "  --vocab_file=vocab-en_eu.txt \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --do_lower_case=True \\\n",
        "  --do_train=True \\\n",
        "  --train_file=train-v2.0.json \\\n",
        "  --do_predict=True \\\n",
        "  --predict_file=dev-v2.0.json \\\n",
        "  --train_batch_size=24 \\\n",
        "  --learning_rate=3e-5 \\\n",
        "  --num_train_epochs=0.1 \\\n",
        "  --max_seq_length=384 \\\n",
        "  --doc_stride=128 \\\n",
        "  --output_dir=gs://gurebert/gureBERT/wordpiece/squad/ \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name=$TPU_NAME \\\n",
        "  --version_2_with_negative=True \\\n",
        "  --init_checkpoint=gs://gurebert/gureBERT/wordpiece/model-en_eu/ \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 19:50:28.474892 140493874636672 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 19:50:28.477128 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:1283: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 19:50:28.477829 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:1127: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 19:50:28.477985 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:1127: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 19:50:28.478116 140493874636672 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 19:50:28.479142 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:1133: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0629 19:50:30.901882 140493874636672 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 19:50:31.908073 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:229: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 19:50:43.742524 140493874636672 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fc710e81b70>) includes params argument, but params are not passed to Estimator.\n",
            "I0629 19:50:43.744081 140493874636672 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/wordpiece/squad/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.2.81.2:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc710e83400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.2.81.2:8470', '_evaluation_master': 'grpc://10.2.81.2:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc71cec3048>}\n",
            "I0629 19:50:43.744420 140493874636672 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "W0629 19:50:43.745228 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:1065: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0629 19:50:43.749988 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:431: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 19:50:43.750156 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:43.750240 140493874636672 run_squad.py:432] unique_id: 1000000000\n",
            "I0629 19:50:43.750310 140493874636672 run_squad.py:433] example_index: 0\n",
            "I0629 19:50:43.750376 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:43.750535 140493874636672 run_squad.py:436] tokens: [CLS] what is the name of the act that was a success in creati ##ng boundaries for the crown and the ei ##c for being subjective ? [SEP] pit ##t ' s act was deem ##ed a failure because it quickly became apparent that the boundaries between government control and the company ' s powers were ne ##bu ##lous and highl ##y subjective . the government felt oblige ##d to respond to human ##itarian calls for better treatment of local peoples in british - occupied territories . edm ##und burk ##e , a former east india company share ##hold ##er and diplomat , was moved to address the situation and introduced a new regulat ##ing bill in 1783 . the bill was defeated amid lobb ##ying by company loyalists and accusations of ne ##pot ##ism in the bill ' s recommendations for the appointment of councillors . [SEP]\n",
            "I0629 19:50:43.750697 140493874636672 run_squad.py:438] token_to_orig_map: 28:0 29:0 30:0 31:0 32:1 33:2 34:3 35:3 36:4 37:5 38:6 39:7 40:8 41:9 42:10 43:11 44:12 45:13 46:14 47:15 48:16 49:17 50:18 51:19 52:19 53:19 54:20 55:21 56:22 57:22 58:22 59:23 60:24 61:24 62:25 63:25 64:26 65:27 66:28 67:29 68:29 69:30 70:31 71:32 72:33 73:33 74:34 75:35 76:36 77:37 78:38 79:39 80:40 81:41 82:42 83:42 84:42 85:43 86:43 87:44 88:44 89:45 90:45 91:45 92:46 93:47 94:48 95:49 96:50 97:51 98:51 99:51 100:52 101:53 102:53 103:54 104:55 105:56 106:57 107:58 108:59 109:60 110:61 111:62 112:63 113:64 114:64 115:65 116:66 117:67 118:67 119:68 120:69 121:70 122:71 123:72 124:73 125:73 126:74 127:75 128:76 129:77 130:78 131:79 132:80 133:80 134:80 135:81 136:82 137:83 138:83 139:83 140:84 141:85 142:86 143:87 144:88 145:89 146:89\n",
            "I0629 19:50:43.750828 140493874636672 run_squad.py:440] token_is_max_context: 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True\n",
            "I0629 19:50:43.751042 140493874636672 run_squad.py:442] input_ids: 2 305 21 11 170 14 11 372 29 26 18 1195 16 34791 389 4108 27 11 2934 15 11 33418 165 27 142 8760 45024 3 8427 121 45010 294 372 26 15797 35 18 2392 186 39 2094 163 5206 29 11 4108 105 125 478 15 11 492 45010 294 1194 53 2512 9305 19467 15 15348 86 8760 45017 11 125 2638 15762 48 17 5629 17 342 10932 2546 27 986 709 14 339 3847 16 216 45016 3438 3623 45017 34831 32028 19236 66 45015 18 567 377 1264 492 1956 25650 175 15 16788 45015 26 925 17 1939 11 1854 15 600 18 88 9849 28 1413 16 12431 45017 11 1413 26 2428 13482 12453 4589 31 492 13119 15 11592 14 2512 34940 1613 16 11 1413 45010 294 9568 27 11 13031 14 30348 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.751198 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.751352 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.751419 140493874636672 run_squad.py:448] impossible example\n",
            "I0629 19:50:43.758349 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:43.758581 140493874636672 run_squad.py:432] unique_id: 1000000001\n",
            "I0629 19:50:43.758666 140493874636672 run_squad.py:433] example_index: 1\n",
            "I0629 19:50:43.758738 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:43.758908 140493874636672 run_squad.py:436] tokens: [CLS] what do winners of the continental competition get to do ? [SEP] after the world cup , the most important international football competitions are the continental championships , which are organised by each continental confederation and contested between national teams . these are the european championship ( u ##efa ) , the cop ##a america ( con ##me ##bol ) , african cup of nations ( caf ) , the asian cup ( af ##c ) , the conc ##aca ##f gold cup ( conc ##aca ##f ) and the of ##c nations cup ( of ##c ) . the fif ##a confederation ##s cup is contested by the winners of all six continental championships , the current fif ##a world cup champions and the country which is host ##ing the confederation ##s cup . this is generally regarded as a warm - up tournament for the up ##coming fif ##a world cup and does not carry the same prestige as the world cup itself . the most prestigio ##us competitions in club football are the respective continental championships , which are generally contested between national champions , for example the u ##efa champions league in europe and the cop ##a libert ##adore ##s in south america . the winners of each continental competition contest the fif ##a club world cup . [SEP]\n",
            "I0629 19:50:43.759086 140493874636672 run_squad.py:438] token_to_orig_map: 13:0 14:1 15:2 16:3 17:3 18:4 19:5 20:6 21:7 22:8 23:9 24:10 25:11 26:12 27:13 28:13 29:14 30:15 31:16 32:17 33:18 34:19 35:20 36:21 37:22 38:23 39:24 40:25 41:25 42:26 43:27 44:28 45:29 46:30 47:31 48:31 49:31 50:31 51:31 52:32 53:33 54:33 55:34 56:35 57:35 58:35 59:35 60:35 61:35 62:36 63:37 64:38 65:39 66:40 67:40 68:40 69:40 70:41 71:42 72:43 73:44 74:44 75:44 76:44 77:44 78:45 79:46 80:46 81:46 82:47 83:48 84:49 85:49 86:49 87:49 88:49 89:50 90:51 91:52 92:52 93:53 94:54 95:55 96:55 97:55 98:55 99:55 100:56 101:57 102:57 103:58 104:58 105:59 106:60 107:61 108:62 109:63 110:64 111:65 112:66 113:67 114:68 115:69 116:69 117:70 118:71 119:72 120:72 121:73 122:74 123:75 124:76 125:77 126:78 127:79 128:80 129:81 130:81 131:82 132:83 133:83 134:84 135:84 136:85 137:86 138:87 139:88 140:89 141:90 142:91 143:91 144:91 145:92 146:93 147:94 148:95 149:95 150:96 151:96 152:97 153:98 154:99 155:100 156:101 157:102 158:103 159:104 160:105 161:106 162:107 163:108 164:109 165:110 166:110 167:111 168:112 169:113 170:113 171:114 172:115 173:116 174:117 175:118 176:119 177:120 178:121 179:122 180:122 181:123 182:124 183:125 184:126 185:127 186:128 187:129 188:129 189:130 190:131 191:132 192:133 193:133 194:134 195:135 196:136 197:137 198:138 199:139 200:140 201:140 202:141 203:141 204:141 205:142 206:143 207:144 208:144 209:145 210:146 211:147 212:148 213:149 214:150 215:151 216:152 217:153 218:153 219:154 220:155 221:156 222:156\n",
            "I0629 19:50:43.759256 140493874636672 run_squad.py:440] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True\n",
            "I0629 19:50:43.759482 140493874636672 run_squad.py:442] input_ids: 2 305 264 5304 14 11 2910 2025 1025 17 264 45024 3 85 11 114 1156 45015 11 78 325 266 473 3800 36 11 2910 3894 45015 47 36 5027 31 194 2910 12813 15 8384 105 207 1793 45017 91 36 11 180 4223 45011 267 2785 45012 45015 11 9029 46 1099 45011 3030 3506 24530 45012 45015 703 1156 14 914 45011 43574 45012 45015 11 2834 1156 45011 9888 165 45012 45015 11 16656 34600 451 1928 1156 45011 16656 34600 451 45012 15 11 14 165 914 1156 45011 14 165 45012 45017 11 4076 46 12813 19 1156 21 8384 31 11 5304 14 84 589 2910 3894 45015 11 543 4076 46 114 1156 5259 15 11 187 47 21 1249 28 11 12813 19 1156 45017 51 21 517 1590 24 18 3293 45016 130 4567 27 11 130 7771 4076 46 114 1156 15 373 58 3036 11 185 14674 24 11 114 1156 555 45017 11 78 17695 306 3800 16 913 473 36 11 5028 2910 3894 45015 47 36 517 8384 105 207 5259 45015 27 178 11 267 2785 5259 744 16 414 15 11 9029 46 15746 41280 19 16 262 1099 45017 11 5304 14 194 2910 2025 704 11 4076 46 913 114 1156 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.759655 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.759816 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.759892 140493874636672 run_squad.py:451] start_position: 215\n",
            "I0629 19:50:43.759960 140493874636672 run_squad.py:452] end_position: 221\n",
            "I0629 19:50:43.760030 140493874636672 run_squad.py:454] answer: contest the fif ##a club world cup\n",
            "I0629 19:50:43.764655 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:43.764843 140493874636672 run_squad.py:432] unique_id: 1000000002\n",
            "I0629 19:50:43.764925 140493874636672 run_squad.py:433] example_index: 2\n",
            "I0629 19:50:43.764997 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:43.765129 140493874636672 run_squad.py:436] tokens: [CLS] what did frederick zu ##gib ##e study in detail for this book ? [SEP] in his book the crucifi ##xion of jesus , physician and fore ##ns ##ic pathologist frederick zu ##gib ##e studied the likely circumstances of the death of jesus in great detail . zu ##gib ##e carried out a number of experiments over several years to test his theories while he was a medical examine ##r . these studies included experiments in which volunteers with specific weight ##s were hanging at specific angles and the amount of pull on each hand was measured , in cases where the feet were also secured or not . in these cases the amount of pull and the corresponding pain was found to be significant . [SEP]\n",
            "I0629 19:50:43.810125 140493874636672 run_squad.py:438] token_to_orig_map: 15:0 16:1 17:2 18:3 19:4 20:4 21:5 22:6 23:6 24:7 25:8 26:9 27:9 28:9 29:10 30:11 31:12 32:12 33:12 34:13 35:14 36:15 37:16 38:17 39:18 40:19 41:20 42:21 43:22 44:23 45:24 46:24 47:25 48:25 49:25 50:26 51:27 52:28 53:29 54:30 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:40 65:41 66:42 67:43 68:44 69:44 70:44 71:45 72:46 73:47 74:48 75:49 76:50 77:51 78:52 79:53 80:54 81:54 82:55 83:56 84:57 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:66 94:67 95:68 96:69 97:69 98:70 99:71 100:72 101:73 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:81 111:82 112:83 113:84 114:85 115:86 116:87 117:88 118:89 119:90 120:91 121:92 122:93 123:94 124:95 125:95\n",
            "I0629 19:50:43.810417 140493874636672 run_squad.py:440] token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True\n",
            "I0629 19:50:43.810747 140493874636672 run_squad.py:442] input_ids: 2 305 278 2711 7181 33580 66 434 16 3285 27 51 380 45024 3 16 44 380 11 39920 42631 14 737 45015 8551 15 13634 2531 282 21640 2711 7181 33580 66 2713 11 817 2650 14 11 344 14 737 16 228 3285 45017 7181 33580 66 1229 148 18 158 14 4179 110 195 124 17 945 44 2332 120 54 26 18 1651 6764 179 45017 91 998 405 4179 16 47 4493 33 742 3374 19 53 31339 50 742 23380 15 11 1070 14 10070 34 194 735 26 3348 45015 16 803 118 11 3984 53 60 7633 43 58 45017 16 91 803 11 1070 14 10070 15 11 3650 2945 26 188 17 42 677 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.811013 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.811266 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.811393 140493874636672 run_squad.py:451] start_position: 35\n",
            "I0629 19:50:43.811503 140493874636672 run_squad.py:452] end_position: 42\n",
            "I0629 19:50:43.811619 140493874636672 run_squad.py:454] answer: the likely circumstances of the death of jesus\n",
            "I0629 19:50:43.816456 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:43.816685 140493874636672 run_squad.py:432] unique_id: 1000000003\n",
            "I0629 19:50:43.816772 140493874636672 run_squad.py:433] example_index: 3\n",
            "I0629 19:50:43.816842 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:43.816979 140493874636672 run_squad.py:436] tokens: [CLS] how many millions of tourists does greece ban each year ? [SEP] greece attract ##s more than 16 million tourists each year , thus contribut ##ing 18 . 2 % to the nation ' s gdp in 2008 according to an oecd report . the same survey showe ##d that the average tourist expenditure while in greece was $ 1 , 0 ##73 , ranking greece 10 ##th in the world . the number of jobs directly or indirect ##ly related to the tourism sector were 84 ##0 , 0 ##00 in 2008 and represented 19 % of the country ' s total labor force . in 2009 , greece welcome ##d over 19 . 3 million tourists , a major increase from the 17 . 7 million tourists the country welcome ##d in 2008 . [SEP]\n",
            "I0629 19:50:43.817119 140493874636672 run_squad.py:438] token_to_orig_map: 13:0 14:1 15:1 16:2 17:3 18:4 19:5 20:6 21:7 22:8 23:8 24:9 25:10 26:10 27:11 28:11 29:11 30:11 31:12 32:13 33:14 34:14 35:14 36:15 37:16 38:17 39:18 40:19 41:20 42:21 43:22 44:22 45:23 46:24 47:25 48:26 49:26 50:27 51:28 52:29 53:30 54:31 55:32 56:33 57:34 58:35 59:36 60:36 61:36 62:36 63:36 64:36 65:37 66:38 67:39 68:39 69:40 70:41 71:42 72:42 73:43 74:44 75:45 76:46 77:47 78:48 79:49 80:49 81:50 82:51 83:52 84:53 85:54 86:55 87:56 88:56 89:56 90:56 91:56 92:57 93:58 94:59 95:60 96:61 97:61 98:62 99:63 100:64 101:64 102:64 103:65 104:66 105:67 106:67 107:68 108:69 109:69 110:70 111:71 112:71 113:72 114:73 115:73 116:73 117:74 118:75 119:75 120:76 121:77 122:78 123:79 124:80 125:81 126:81 127:81 128:82 129:83 130:84 131:85 132:86 133:86 134:87 135:88 136:88\n",
            "I0629 19:50:43.817253 140493874636672 run_squad.py:440] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True\n",
            "I0629 19:50:43.817475 140493874636672 run_squad.py:442] input_ids: 2 507 98 6709 14 4048 373 3845 2840 194 183 45024 3 3845 2727 19 81 92 713 272 4048 194 183 45015 345 3091 28 577 45017 386 45008 17 11 529 45010 294 2537 16 549 299 17 41 5960 1039 45017 11 185 3905 24772 48 29 11 565 3110 7286 120 16 3845 26 45007 277 45015 3884 13431 45015 5740 3845 524 108 16 11 114 45017 11 158 14 4535 1020 43 9899 56 719 17 11 2467 2400 53 18909 1657 45015 3884 3683 16 549 15 1857 453 45008 14 11 187 45010 294 367 3338 333 45017 16 760 45015 3845 7264 48 110 453 45017 413 272 4048 45015 18 219 637 37 11 781 45017 1001 272 4048 11 187 7264 48 16 549 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.817651 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.817810 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.817880 140493874636672 run_squad.py:448] impossible example\n",
            "I0629 19:50:43.823289 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:43.823518 140493874636672 run_squad.py:432] unique_id: 1000000004\n",
            "I0629 19:50:43.823621 140493874636672 run_squad.py:433] example_index: 4\n",
            "I0629 19:50:43.823693 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:43.823843 140493874636672 run_squad.py:436] tokens: [CLS] what is the main role of the cabinet of government ministers ? [SEP] when the party is represented by members in the lower house of parliament , the party leader simultaneously serve ##s as the leader of the parliament ##ary group of that full party representation ; depend ##ing on a minimum number of seats held , west ##minster - based parties typically allow for leaders to form front ##bench teams of senior fellow members of the parliament ##ary group to serve as critics of aspects of government policy . when a party becomes the largest party not part of the government , the party ' s parliament ##ary group forms the official opposition , with official opposition front ##bench team members often form ##ing the official opposition shadow cabinet . when a party achieve ##s enough seats in an election to form a majority , the party ' s front ##bench becomes the cabinet of government ministers . [SEP]\n",
            "I0629 19:50:43.823996 140493874636672 run_squad.py:438] token_to_orig_map: 14:0 15:1 16:2 17:3 18:4 19:5 20:6 21:7 22:8 23:9 24:10 25:11 26:12 27:12 28:13 29:14 30:15 31:16 32:17 33:17 34:18 35:19 36:20 37:21 38:22 39:23 40:23 41:24 42:25 43:26 44:27 45:28 46:29 47:29 48:30 49:30 50:31 51:32 52:33 53:34 54:35 55:36 56:37 57:37 58:38 59:38 60:38 61:38 62:39 63:40 64:41 65:42 66:43 67:44 68:45 69:46 70:46 71:47 72:48 73:49 74:50 75:51 76:52 77:53 78:54 79:54 80:55 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:64 91:65 92:66 93:67 94:68 95:69 96:70 97:71 98:72 99:73 100:74 101:75 102:76 103:76 104:77 105:78 106:78 107:78 108:79 109:79 110:80 111:81 112:82 113:83 114:84 115:84 116:85 117:86 118:87 119:88 120:88 121:89 122:90 123:91 124:92 125:92 126:93 127:94 128:95 129:96 130:97 131:97 132:98 133:99 134:100 135:101 136:101 137:102 138:103 139:104 140:105 141:106 142:107 143:108 144:109 145:110 146:110 147:111 148:112 149:112 150:112 151:113 152:113 153:114 154:115 155:116 156:117 157:118 158:119 159:119\n",
            "I0629 19:50:43.824137 140493874636672 run_squad.py:440] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True\n",
            "I0629 19:50:43.824348 140493874636672 run_squad.py:442] input_ids: 2 305 21 11 245 388 14 11 2471 14 125 3173 45024 3 94 11 212 21 1857 31 314 16 11 711 368 14 551 45015 11 212 1066 3977 1021 19 24 11 1066 14 11 551 798 173 14 29 580 212 3323 45020 888 28 34 18 3958 158 14 2154 514 45015 357 9173 45016 261 692 822 789 27 2379 17 182 1350 39114 1793 14 3633 4395 314 14 11 551 798 173 17 1021 24 2847 14 2551 14 125 847 45017 94 18 212 2556 11 411 212 58 144 14 11 125 45015 11 212 45010 294 551 798 173 902 11 734 2009 45015 33 734 2009 1350 39114 359 314 145 182 28 11 734 2009 7432 2471 45017 94 18 212 2186 19 1641 2154 16 41 951 17 182 18 745 45015 11 212 45010 294 1350 39114 2556 11 2471 14 125 3173 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.824522 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.824696 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.824767 140493874636672 run_squad.py:448] impossible example\n",
            "I0629 19:50:43.831784 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:43.832009 140493874636672 run_squad.py:432] unique_id: 1000000005\n",
            "I0629 19:50:43.832095 140493874636672 run_squad.py:433] example_index: 5\n",
            "I0629 19:50:43.832164 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:43.832344 140493874636672 run_squad.py:436] tokens: [CLS] what does the fe ##pc stand for ? [SEP] f ##dr ' s new deal programs often containe ##d equal opportunity clauses stating \" no discrimination shall be made on account of race , color or creed \" , : 11 but the true fore ##runner to affirmative action was the interior secretary of the time , haro ##ld l . i ##ck ##es . i ##ck ##es prohibited discrimination in hiri ##ng for public works administration funded projects and over ##saw not only the institution of a quot ##a system , where contractors were required to employ a fixed percentage of black workers , by robert c . weaver and clark fore ##man , : 12 but also the equal pay of women proposed by harry hopkins . : 14 ##f ##dr ' s largest contribution to affirmative action , however , lay in his executive order 88 ##02 which prohibited discrimination in the defense industry or government . : 22 the executive order promote ##d the idea that if tax ##pay ##er funds were accepted through a government contract , then all tax ##pay ##ers should have an equal opportunity to work through the contractor . : 23 – 4 to enforce this idea , roosevelt created the fair employment practices committee ( fe ##pc ) with the power to investigate hiri ##ng practices by government contractors . : 22 [SEP]\n",
            "I0629 19:50:43.832534 140493874636672 run_squad.py:438] token_to_orig_map: 10:0 11:0 12:0 13:0 14:1 15:2 16:3 17:4 18:5 19:5 20:6 21:7 22:8 23:9 24:10 25:10 26:11 27:12 28:13 29:14 30:15 31:16 32:17 33:18 34:18 35:19 36:20 37:21 38:21 39:21 40:21 41:21 42:22 43:23 44:24 45:25 46:25 47:26 48:27 49:28 50:29 51:30 52:31 53:32 54:33 55:34 56:35 57:35 58:36 59:36 60:37 61:37 62:38 63:38 64:38 65:38 66:39 67:39 68:39 69:40 70:41 71:42 72:43 73:43 74:44 75:45 76:46 77:47 78:48 79:49 80:50 81:51 82:51 83:52 84:53 85:54 86:55 87:56 88:57 89:58 90:58 91:59 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:66 100:67 101:68 102:69 103:70 104:71 105:71 106:72 107:73 108:74 109:74 110:75 111:76 112:77 113:78 114:78 115:78 116:78 117:78 118:79 119:80 120:81 121:82 122:83 123:84 124:85 125:86 126:87 127:88 128:89 129:89 130:89 131:89 132:89 133:89 134:89 135:89 136:90 137:91 138:92 139:93 140:94 141:94 142:95 143:95 144:96 145:97 146:98 147:99 148:100 149:101 150:101 151:102 152:103 153:104 154:105 155:106 156:107 157:108 158:109 159:110 160:110 161:110 162:110 163:111 164:112 165:113 166:114 167:114 168:115 169:116 170:117 171:118 172:119 173:119 174:119 175:120 176:121 177:122 178:123 179:124 180:125 181:126 182:126 183:127 184:128 185:129 186:129 187:129 188:130 189:131 190:132 191:133 192:134 193:135 194:136 195:137 196:138 197:139 198:139 199:139 200:139 201:139 202:139 203:140 204:141 205:142 206:143 207:143 208:144 209:145 210:146 211:147 212:148 213:149 214:150 215:151 216:151 217:151 218:151 219:152 220:153 221:154 222:155 223:156 224:157 225:157 226:158 227:159 228:160 229:161 230:161 231:161 232:161\n",
            "I0629 19:50:43.832731 140493874636672 run_squad.py:440] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True\n",
            "I0629 19:50:43.913725 140493874636672 run_squad.py:442] input_ids: 2 305 373 11 6537 6841 2350 27 45024 3 270 7782 45010 294 88 1958 855 145 16089 48 1244 2417 11808 3340 45005 111 7547 3669 42 153 34 1115 14 2020 45015 3759 43 1417 45005 45015 45019 930 62 11 1706 13634 37260 17 15067 766 26 11 2266 2478 14 11 102 45015 40972 13403 844 45017 135 10777 197 45017 135 10777 197 5998 7547 16 1961 389 27 260 500 1732 6380 2022 15 110 11909 58 104 11 3766 14 18 15207 46 119 45015 118 30396 53 881 17 2692 18 2589 2527 14 672 2558 45015 31 756 206 45017 28733 15 6200 13634 767 45015 45019 596 62 60 11 1244 1351 14 547 1427 31 5688 12713 45017 45019 777 451 7782 45010 294 411 4086 17 15067 766 45015 115 45015 4581 16 44 1710 320 16106 13353 47 5998 7547 16 11 1170 650 43 125 45017 45019 1576 11 1710 320 1628 48 11 1113 29 155 1127 25800 175 4510 53 1317 176 18 125 1712 45015 211 84 1127 25800 901 433 55 41 1244 2417 17 150 176 11 30467 45017 45019 1546 288 501 17 6378 51 1113 45015 8015 544 11 395 3534 3451 1517 45011 6537 6841 45012 33 11 263 17 7720 1961 389 3451 31 125 30396 45017 45019 1576 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.914095 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.914364 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.914502 140493874636672 run_squad.py:451] start_position: 211\n",
            "I0629 19:50:43.914625 140493874636672 run_squad.py:452] end_position: 214\n",
            "I0629 19:50:43.914725 140493874636672 run_squad.py:454] answer: fair employment practices committee\n",
            "I0629 19:50:43.917734 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:43.917881 140493874636672 run_squad.py:432] unique_id: 1000000006\n",
            "I0629 19:50:43.917976 140493874636672 run_squad.py:433] example_index: 6\n",
            "I0629 19:50:43.918054 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:43.918152 140493874636672 run_squad.py:436] tokens: [CLS] the structures that still remain are open to whom ? [SEP] because it was designate ##d as the national capital , many structures were built around that time . even today , some of them still remain which are open to tourists . [SEP]\n",
            "I0629 19:50:43.918246 140493874636672 run_squad.py:438] token_to_orig_map: 12:0 13:1 14:2 15:3 16:3 17:4 18:5 19:6 20:7 21:7 22:8 23:9 24:10 25:11 26:12 27:13 28:14 29:14 30:15 31:16 32:16 33:17 34:18 35:19 36:20 37:21 38:22 39:23 40:24 41:25 42:26 43:26\n",
            "I0629 19:50:43.918335 140493874636672 run_squad.py:440] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True\n",
            "I0629 19:50:43.918529 140493874636672 run_squad.py:442] input_ids: 2 11 2145 29 307 1599 36 523 17 1306 45024 3 186 39 26 5642 48 24 11 207 587 45015 98 2145 53 562 247 29 102 45017 220 636 45015 75 14 171 307 1599 47 36 523 17 4048 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.918735 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.918922 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.919000 140493874636672 run_squad.py:451] start_position: 42\n",
            "I0629 19:50:43.919066 140493874636672 run_squad.py:452] end_position: 42\n",
            "I0629 19:50:43.919130 140493874636672 run_squad.py:454] answer: tourists\n",
            "I0629 19:50:43.923704 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:43.923865 140493874636672 run_squad.py:432] unique_id: 1000000007\n",
            "I0629 19:50:43.923945 140493874636672 run_squad.py:433] example_index: 7\n",
            "I0629 19:50:43.924015 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:43.924144 140493874636672 run_squad.py:436] tokens: [CLS] when was the mann act passed ? [SEP] the bureau ' s first official task was visiting and making surveys of the houses of prostitution in preparation for enforc ##ing the \" white slave traffic act , \" or mann act , passed on june 25 , 1910 . in 1932 , it was rename ##d the united states bureau of investigation . the following year it was linked to the bureau of prohibition and rec ##hri ##sten ##ed the division of investigation ( do ##i ) before finally becoming an independent service within the department of justice in 1935 . in the same year , its name was officially changed from the division of investigation to the present - day federal bureau of investigation , or fbi . [SEP]\n",
            "I0629 19:50:43.924277 140493874636672 run_squad.py:438] token_to_orig_map: 9:0 10:1 11:1 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:18 30:18 31:19 32:20 33:20 34:21 35:22 36:23 37:23 38:23 39:24 40:25 41:26 42:26 43:27 44:28 45:29 46:30 47:30 48:31 49:31 50:32 51:33 52:33 53:34 54:35 55:36 56:36 57:37 58:38 59:39 60:40 61:41 62:42 63:42 64:43 65:44 66:45 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:54 76:55 77:55 78:55 79:55 80:56 81:57 82:58 83:59 84:60 85:60 86:60 87:60 88:61 89:62 90:63 91:64 92:65 93:66 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:73 102:74 103:75 104:76 105:77 106:77 107:78 108:79 109:80 110:81 111:82 112:83 113:84 114:85 115:86 116:87 117:88 118:89 119:90 120:90 121:90 122:91 123:92 124:93 125:94 126:94 127:95 128:96 129:96\n",
            "I0629 19:50:43.924401 140493874636672 run_squad.py:440] token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True\n",
            "I0629 19:50:43.924630 140493874636672 run_squad.py:442] input_ids: 2 94 26 11 7842 372 1827 45024 3 11 6218 45010 294 73 734 3120 26 4500 15 591 18894 14 11 1548 14 27961 16 4430 27 9178 28 11 45005 635 4190 1801 372 45015 45005 43 7842 372 45015 1827 34 776 1006 45015 3909 45017 16 4474 45015 39 26 4533 48 11 161 151 6218 14 3056 45017 11 230 183 39 26 3356 17 11 6218 14 6934 15 13793 33336 32969 35 11 1749 14 3056 45011 264 129 45012 177 1541 1652 41 933 240 237 11 2860 14 2276 16 5598 45017 16 11 185 183 45015 67 170 26 1777 1594 37 11 1749 14 3056 17 11 528 45016 336 1014 6218 14 3056 45015 43 13070 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.924787 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.924938 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.925009 140493874636672 run_squad.py:451] start_position: 45\n",
            "I0629 19:50:43.925074 140493874636672 run_squad.py:452] end_position: 48\n",
            "I0629 19:50:43.925141 140493874636672 run_squad.py:454] answer: june 25 , 1910\n",
            "I0629 19:50:43.929083 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:43.929238 140493874636672 run_squad.py:432] unique_id: 1000000008\n",
            "I0629 19:50:43.929332 140493874636672 run_squad.py:433] example_index: 8\n",
            "I0629 19:50:43.929402 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:43.929538 140493874636672 run_squad.py:436] tokens: [CLS] what is the result of / e / being the same in central , western and bel ##eari ##c ? [SEP] central , western , and bale ##ari ##c differ in the lexical inc ##iden ##ce of stresse ##d / e / and / [UNK] / . usually , words with / [UNK] / in central catalan correspond to / [UNK] / in bale ##ari ##c and / e / in western catalan . words with / e / in bale ##ari ##c almost always have / e / in central and western catalan as well . [ vague ] as a result , central catalan has a much higher inc ##iden ##ce of / e / . [SEP]\n",
            "I0629 19:50:43.929678 140493874636672 run_squad.py:438] token_to_orig_map: 22:0 23:0 24:1 25:1 26:2 27:3 28:3 29:3 30:4 31:5 32:6 33:7 34:8 35:8 36:8 37:9 38:10 39:10 40:11 41:11 42:11 43:12 44:13 45:13 46:13 47:13 48:14 49:14 50:15 51:16 52:17 53:17 54:17 55:18 56:19 57:20 58:21 59:22 60:23 61:23 62:23 63:24 64:25 65:25 66:25 67:26 68:27 69:27 70:27 71:28 72:29 73:30 74:30 75:31 76:32 77:33 78:33 79:33 80:34 81:35 82:35 83:35 84:36 85:37 86:38 87:39 88:39 89:39 90:40 91:41 92:42 93:43 94:44 95:45 96:46 97:46 98:46 99:46 100:46 101:47 102:48 103:49 104:49 105:50 106:51 107:52 108:53 109:54 110:55 111:56 112:56 113:56 114:57 115:58 116:58 117:58 118:58\n",
            "I0629 19:50:43.929801 140493874636672 run_squad.py:440] token_is_max_context: 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True\n",
            "I0629 19:50:43.930007 140493874636672 run_squad.py:442] input_ids: 2 305 21 11 295 14 45018 284 45018 142 11 185 16 521 45015 533 15 11455 43830 165 45024 3 521 45015 533 45015 15 18561 1868 165 2199 16 11 12728 3764 33600 1284 14 35304 48 45018 284 45018 15 45018 1 45018 45017 318 45015 518 33 45018 1 45018 16 521 21502 4093 17 45018 1 45018 16 18561 1868 165 15 45018 284 45018 16 533 21502 45017 518 33 45018 284 45018 16 18561 1868 165 739 984 55 45018 284 45018 16 521 15 533 21502 24 147 45017 45026 21338 45028 24 18 295 45015 521 21502 57 18 221 741 3764 33600 1284 14 45018 284 45018 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.930201 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.930392 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:43.930473 140493874636672 run_squad.py:451] start_position: 110\n",
            "I0629 19:50:44.017151 140493874636672 run_squad.py:452] end_position: 113\n",
            "I0629 19:50:44.017362 140493874636672 run_squad.py:454] answer: higher inc ##iden ##ce\n",
            "I0629 19:50:44.024166 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.024451 140493874636672 run_squad.py:432] unique_id: 1000000009\n",
            "I0629 19:50:44.024605 140493874636672 run_squad.py:433] example_index: 9\n",
            "I0629 19:50:44.024704 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.024930 140493874636672 run_squad.py:436] tokens: [CLS] what did the old letter [UNK] [UNK] [UNK] become ? [SEP] older letters of the russian alphabet include [UNK] [UNK] [UNK] , which merge ##d to [UNK] [UNK] [UNK] ( / je / or / [UNK] / ) ; [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] , which both merge ##d to [UNK] [UNK] [UNK] ( / i / ) ; [UNK] [UNK] [UNK] , which merge ##d to [UNK] [UNK] [UNK] ( / f / ) ; [UNK] [UNK] [UNK] , which merge ##d to [UNK] [UNK] [UNK] ( / u / ) ; [UNK] [UNK] [UNK] , which merge ##d to [UNK] [UNK] [UNK] ( / ju / or / [UNK] / ) ; and [UNK] [UNK] / [UNK] [UNK] [UNK] [UNK] , which later were graphical ##ly res ##hap ##ed into [UNK] [UNK] [UNK] and merge ##d phonetic ##ally to / ja / or / [UNK] / . while these older letters have been abandoned at one time or another , they may be used in this and related articles . the yer ##s [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] originally indicated the pronunciation of ultra - short or reduced / u / , / i / . [SEP]\n",
            "I0629 19:50:44.025195 140493874636672 run_squad.py:438] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:7 21:7 22:7 23:8 24:9 25:9 26:10 27:11 28:11 29:11 30:12 31:12 32:12 33:12 34:13 35:14 36:14 37:14 38:14 39:14 40:15 41:15 42:15 43:16 44:17 45:17 46:17 47:17 48:18 49:19 50:20 51:20 52:21 53:22 54:22 55:22 56:23 57:23 58:23 59:23 60:23 61:23 62:24 63:24 64:24 65:24 66:25 67:26 68:26 69:27 70:28 71:28 72:28 73:29 74:29 75:29 76:29 77:29 78:29 79:30 80:30 81:30 82:30 83:31 84:32 85:32 86:33 87:34 88:34 89:34 90:35 91:35 92:35 93:35 94:35 95:35 96:36 97:36 98:36 99:36 100:37 101:38 102:38 103:39 104:40 105:40 106:40 107:41 108:41 109:41 110:41 111:42 112:43 113:43 114:43 115:43 116:43 117:44 118:45 119:45 120:45 121:45 122:45 123:45 124:45 125:45 126:46 127:47 128:48 129:49 130:49 131:50 132:50 133:50 134:51 135:52 136:52 137:52 138:53 139:54 140:54 141:55 142:55 143:56 144:57 145:57 146:57 147:58 148:59 149:59 150:59 151:59 152:60 153:61 154:62 155:63 156:64 157:65 158:66 159:67 160:68 161:69 162:70 163:71 164:71 165:72 166:73 167:74 168:75 169:76 170:77 171:78 172:79 173:80 174:80 175:81 176:82 177:82 178:83 179:83 180:83 181:84 182:85 183:85 184:85 185:86 186:87 187:88 188:89 189:90 190:91 191:91 192:91 193:92 194:93 195:94 196:94 197:94 198:94 199:95 200:95 201:95 202:95\n",
            "I0629 19:50:44.025413 140493874636672 run_squad.py:440] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True\n",
            "I0629 19:50:44.025722 140493874636672 run_squad.py:442] input_ids: 2 305 278 11 400 396 1 1 1 361 45024 3 1586 1566 14 11 2161 9517 193 1 1 1 45015 47 15924 48 17 1 1 1 45011 45018 7138 45018 43 45018 1 45018 45012 45020 1 1 1 15 1 1 1 45015 47 138 15924 48 17 1 1 1 45011 45018 135 45018 45012 45020 1 1 1 45015 47 15924 48 17 1 1 1 45011 45018 270 45018 45012 45020 1 1 1 45015 47 15924 48 17 1 1 1 45011 45018 267 45018 45012 45020 1 1 1 45015 47 15924 48 17 1 1 1 45011 45018 7990 45018 43 45018 1 45018 45012 45020 15 1 1 45018 1 1 1 1 45015 47 139 53 22079 56 13707 34916 35 96 1 1 1 15 15924 48 6373 1176 17 45018 3562 45018 43 45018 1 45018 45017 120 91 1586 1566 55 65 3131 50 63 102 43 242 45015 76 87 42 90 16 51 15 719 2814 45017 11 15634 19 1 1 1 15 1 1 1 909 4879 11 9248 14 5965 45016 570 43 1248 45018 267 45018 45015 45018 135 45018 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.027892 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.028150 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.028249 140493874636672 run_squad.py:451] start_position: 27\n",
            "I0629 19:50:44.028581 140493874636672 run_squad.py:452] end_position: 29\n",
            "I0629 19:50:44.029048 140493874636672 run_squad.py:454] answer: [UNK] [UNK] [UNK]\n",
            "I0629 19:50:44.037728 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.037903 140493874636672 run_squad.py:432] unique_id: 1000000010\n",
            "I0629 19:50:44.037985 140493874636672 run_squad.py:433] example_index: 10\n",
            "I0629 19:50:44.038056 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.038242 140493874636672 run_squad.py:436] tokens: [CLS] what was the problem with the palace ' s chim ##ney ##s ? [SEP] buck ##ingham palace finally became the principal royal residence in 1837 , on the accession of queen victoria , who was the first monarch to reside there ; her predecessor william iv had died before its completi ##on . while the state rooms were a riot of gil ##t and colour , the ne ##ces ##sit ##ies of the new palace were somewh ##at less luxurious . for one thing , it was reported the chim ##ney ##s smoked so much that the fires had to be allowed to die down , and consequently the court shiver ##ed in icy magnific ##ence . ventilation was so bad that the interior smell ##ed , and when a decision was taken to install gas lamps , there was a serious worry about the build - up of gas on the lower floor ##s . it was also said that staff were la ##x and laz ##y and the palace was dirty . following the queen ' s marriage in 1840 , her husband , prince albert , concerned himself with a reorganisation of the household offices and staff , and with the design faults of the palace . the problems were all rec ##tified by the close of 1840 . however , the builder ##s were to return within the decade . [SEP]\n",
            "I0629 19:50:44.038425 140493874636672 run_squad.py:438] token_to_orig_map: 15:0 16:0 17:1 18:2 19:3 20:4 21:5 22:6 23:7 24:8 25:9 26:9 27:10 28:11 29:12 30:13 31:14 32:15 33:15 34:16 35:17 36:18 37:19 38:20 39:21 40:22 41:23 42:23 43:24 44:25 45:26 46:27 47:28 48:29 49:30 50:31 51:32 52:32 53:32 54:33 55:34 56:35 57:36 58:37 59:38 60:39 61:40 62:41 63:41 64:42 65:43 66:43 67:44 68:45 69:45 70:45 71:45 72:46 73:47 74:48 75:49 76:50 77:51 78:51 79:52 80:53 81:53 82:54 83:55 84:56 85:56 86:57 87:58 88:59 89:60 90:61 91:61 92:61 93:62 94:63 95:64 96:65 97:66 98:67 99:68 100:69 101:70 102:71 103:72 104:73 105:74 106:74 107:75 108:76 109:77 110:78 111:79 112:79 113:80 114:81 115:82 116:82 117:82 118:83 119:84 120:85 121:86 122:87 123:88 124:89 125:90 126:90 127:90 128:91 129:92 130:93 131:94 132:95 133:96 134:97 135:98 136:99 137:100 138:100 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:108 148:108 149:109 150:110 151:111 152:112 153:113 154:114 155:114 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:123 167:124 168:124 169:125 170:126 171:127 172:128 173:129 174:129 175:130 176:131 177:132 178:132 179:132 180:133 181:134 182:135 183:135 184:136 185:137 186:137 187:138 188:139 189:139 190:140 191:141 192:142 193:143 194:144 195:145 196:146 197:147 198:148 199:149 200:150 201:150 202:151 203:152 204:153 205:154 206:155 207:156 208:157 209:158 210:158 211:159 212:160 213:161 214:162 215:163 216:163 217:164 218:165 219:166 220:167 221:168 222:168 223:169 224:169 225:170 226:171 227:171 228:172 229:173 230:174 231:175 232:176 233:177 234:177\n",
            "I0629 19:50:44.038632 140493874636672 run_squad.py:440] token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True\n",
            "I0629 19:50:44.038852 140493874636672 run_squad.py:442] input_ids: 2 305 26 11 948 33 11 5956 45010 294 35997 4731 19 45024 3 9448 31307 5956 1541 163 11 2095 783 5862 16 11515 45015 34 11 8804 14 1209 6580 45015 74 26 11 73 10118 17 14302 89 45020 143 6919 553 4310 61 936 177 67 20466 191 45017 120 11 172 4900 53 18 30842 14 10878 121 15 6244 45015 11 2512 14062 43711 784 14 11 88 5956 53 18611 907 374 28335 45017 27 63 5615 45015 39 26 746 11 35997 4731 19 22627 141 221 29 11 35014 61 17 42 701 17 4164 417 45015 15 4420 11 360 30269 35 16 32698 20609 11917 45017 17185 26 141 2462 29 11 2266 21281 35 45015 15 94 18 1389 26 610 17 4484 605 17997 45015 89 26 18 2371 18335 113 11 2107 45016 130 14 605 34 11 711 6035 19 45017 39 26 60 425 29 3403 53 561 961 15 16682 86 15 11 5956 26 15313 45017 230 11 1209 45010 294 1454 16 7521 45015 143 2306 45015 2508 1451 45015 3082 657 33 18 27357 14 11 4202 4840 15 3403 45015 15 33 11 511 10639 14 11 5956 45017 11 1067 53 84 13793 12090 31 11 471 14 7521 45017 115 45015 11 12122 19 53 17 370 237 11 2866 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.039072 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.039265 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.039339 140493874636672 run_squad.py:451] start_position: 89\n",
            "I0629 19:50:44.039404 140493874636672 run_squad.py:452] end_position: 93\n",
            "I0629 19:50:44.039475 140493874636672 run_squad.py:454] answer: the chim ##ney ##s smoked\n",
            "I0629 19:50:44.046259 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.046434 140493874636672 run_squad.py:432] unique_id: 1000000011\n",
            "I0629 19:50:44.046522 140493874636672 run_squad.py:433] example_index: 11\n",
            "I0629 19:50:44.046617 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.046786 140493874636672 run_squad.py:436] tokens: [CLS] who has noted that binding precedent did not exist when the constitution was written ? [SEP] as federal judge alex ko ##zin ##ski has pointe ##d out , binding precedent as we know it today simply did not exist at the time the constitution was frame ##d . judicial decisions were not consistent ##ly , accurately , and faithful ##ly reported on both sides of the atlantic ( reporter ##s often simply re ##wrote or failed to publish decisions which they dislike ##d ) , and the united kingdom lack ##ed a coherent court hierarchy prior to the end of the 19 ##th century . furthermore , english judges in the eighteenth century subscribe ##d to now - obsolete natural law theories of law , by which law was believed to have an existence independent of what individual judges said . judges saw themselves as mere ##ly declaring the law which had always theoretical ##ly existed , and not as making the law . therefore , a judge could reject another judge ' s opinion as simply an incorrect statement of the law , in the way that scientists regularly reject each other ' s conclusions as incorrect statements of the laws of science . [SEP]\n",
            "I0629 19:50:44.046959 140493874636672 run_squad.py:438] token_to_orig_map: 17:0 18:1 19:2 20:3 21:4 22:4 23:4 24:5 25:6 26:6 27:7 28:7 29:8 30:9 31:10 32:11 33:12 34:13 35:14 36:15 37:16 38:17 39:18 40:19 41:20 42:21 43:22 44:23 45:24 46:25 47:25 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:30 56:31 57:31 58:32 59:33 60:33 61:34 62:35 63:36 64:37 65:38 66:39 67:40 68:41 69:41 70:41 71:42 72:43 73:44 74:44 75:45 76:46 77:47 78:48 79:49 80:50 81:51 82:52 83:52 84:52 85:52 86:53 87:54 88:55 89:56 90:57 91:57 92:58 93:59 94:60 95:61 96:62 97:63 98:64 99:65 100:66 101:67 102:68 103:68 104:69 105:69 106:70 107:70 108:71 109:72 110:73 111:74 112:75 113:76 114:77 115:77 116:78 117:79 118:79 119:79 120:80 121:81 122:82 123:83 124:84 125:84 126:85 127:86 128:87 129:88 130:89 131:90 132:91 133:92 134:93 135:94 136:95 137:96 138:97 139:98 140:99 141:99 142:100 143:101 144:102 145:103 146:104 147:104 148:105 149:106 150:107 151:108 152:109 153:110 154:111 155:111 156:112 157:112 158:113 159:114 160:115 161:116 162:117 163:118 164:118 165:119 166:119 167:120 168:121 169:122 170:123 171:124 172:125 173:125 174:125 175:126 176:127 177:128 178:129 179:130 180:131 181:132 182:133 183:134 184:134 185:135 186:136 187:137 188:138 189:139 190:140 191:141 192:142 193:143 194:143 195:143 196:144 197:145 198:146 199:147 200:148 201:149 202:150 203:151 204:152 205:152\n",
            "I0629 19:50:44.047119 140493874636672 run_squad.py:440] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True\n",
            "I0629 19:50:44.047343 140493874636672 run_squad.py:442] input_ids: 2 74 57 1460 29 8870 10744 278 58 963 94 11 1173 26 312 45024 3 24 1014 2381 11029 3833 22800 7349 57 44648 48 148 45015 8870 10744 24 522 1630 39 636 1213 278 58 963 50 11 102 11 1173 26 7388 48 45017 3630 3936 53 58 2344 56 45015 6689 45015 15 6163 56 746 34 138 4357 14 11 896 45011 24758 19 145 1213 274 16361 43 1610 17 7116 3936 47 76 9456 48 45012 45015 15 11 161 536 1096 35 18 10497 360 8630 1490 17 11 248 14 11 453 108 159 45017 2637 45015 335 5107 16 11 8756 159 15378 48 17 258 45016 14448 602 201 2332 14 201 45015 31 47 201 26 1004 17 55 41 2285 933 14 305 609 5107 425 45017 5107 836 749 24 3271 56 10332 11 201 47 61 984 3963 56 2996 45015 15 58 24 591 11 201 45017 804 45015 18 2381 208 3706 242 2381 45010 294 3186 24 1213 41 13804 2675 14 11 201 45015 16 11 371 29 2837 3146 3706 194 69 45010 294 10728 24 13804 3674 14 11 1018 14 584 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.137835 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.138257 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.138401 140493874636672 run_squad.py:451] start_position: 18\n",
            "I0629 19:50:44.138520 140493874636672 run_squad.py:452] end_position: 23\n",
            "I0629 19:50:44.138642 140493874636672 run_squad.py:454] answer: federal judge alex ko ##zin ##ski\n",
            "I0629 19:50:44.143569 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.143771 140493874636672 run_squad.py:432] unique_id: 1000000012\n",
            "I0629 19:50:44.143857 140493874636672 run_squad.py:433] example_index: 12\n",
            "I0629 19:50:44.143928 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.144064 140493874636672 run_squad.py:436] tokens: [CLS] what shortcoming was noticeabl ##e , from the start , for wesley clark ? [SEP] in september 2003 , retired four - star general wesley clark announced his intention to run in the presidential primary election for the democratic party nomination . his campaign focuse ##d on themes of leadership and patriotism ; early campaign ad ##s relied heavily on biography . his late start left him with relative ##ly few detail ##ed policy proposals . this weakness was apparent in his first few debates , although he soon presented a range of position papers , including a major tax - relief plan . nevertheless , the democrats did not flock to support his campaign . [SEP]\n",
            "I0629 19:50:44.144197 140493874636672 run_squad.py:438] token_to_orig_map: 16:0 17:1 18:2 19:2 20:3 21:4 22:4 23:4 24:5 25:6 26:7 27:8 28:9 29:10 30:11 31:12 32:13 33:14 34:15 35:16 36:17 37:18 38:19 39:20 40:21 41:22 42:22 43:23 44:24 45:25 46:25 47:26 48:27 49:28 50:29 51:30 52:31 53:31 54:32 55:33 56:34 57:34 58:35 59:36 60:37 61:38 62:38 63:39 64:40 65:41 66:42 67:43 68:44 69:45 70:45 71:46 72:47 73:47 74:48 75:49 76:49 77:50 78:51 79:52 80:53 81:54 82:55 83:56 84:57 85:58 86:58 87:59 88:60 89:61 90:62 91:63 92:64 93:65 94:66 95:67 96:67 97:68 98:69 99:70 100:71 101:71 102:71 103:72 104:72 105:73 106:73 107:74 108:75 109:76 110:77 111:78 112:79 113:80 114:81 115:82 116:82\n",
            "I0629 19:50:44.144321 140493874636672 run_squad.py:440] token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True\n",
            "I0629 19:50:44.144546 140493874636672 run_squad.py:442] input_ids: 2 305 39401 26 34088 66 45015 37 11 2080 45015 27 12582 6200 45024 3 16 625 1520 45015 2887 271 45016 2757 297 12582 6200 1094 44 4586 17 755 16 11 4146 1267 951 27 11 1471 212 7367 45017 44 1564 6892 48 34 2205 14 2524 15 21214 45020 152 1564 1263 19 5717 1897 34 8147 45017 44 337 2080 308 164 33 816 56 393 3285 35 847 9030 45017 51 10513 26 5206 16 44 73 393 7762 45015 192 54 1159 1798 18 439 14 726 5603 45015 162 18 219 1127 45016 10078 1739 45017 2322 45015 11 2547 278 58 7718 17 292 44 1564 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.144731 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.144893 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.144965 140493874636672 run_squad.py:451] start_position: 71\n",
            "I0629 19:50:44.145037 140493874636672 run_squad.py:452] end_position: 75\n",
            "I0629 19:50:44.145101 140493874636672 run_squad.py:454] answer: few detail ##ed policy proposals\n",
            "I0629 19:50:44.149783 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.150008 140493874636672 run_squad.py:432] unique_id: 1000000013\n",
            "I0629 19:50:44.150097 140493874636672 run_squad.py:433] example_index: 13\n",
            "I0629 19:50:44.150169 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.150307 140493874636672 run_squad.py:436] tokens: [CLS] the digesti ##ve chambers of the c ##ten ##oph ##ora and the cn ##ida ##ria serve as what ? [SEP] among the other phyl ##a , the c ##ten ##oph ##ora and the cn ##ida ##ria , which includes sea an ##emon ##es , coral ##s , and jellyfish , are radial ##ly symmetric and have digesti ##ve chambers with a single opening , which serve ##s as both the mouth and the an ##us . both have distinct tissues , but they are not organized into organs . there are only two main germ layers , the ec ##to ##der ##m and endo ##der ##m , with only scattered cells between them . as such , these animals are sometimes called dip ##lo ##blast ##ic . the tin ##y plac ##ozoa ##ns are similar , but they do not have a permanent digesti ##ve chamber . [SEP]\n",
            "I0629 19:50:44.150443 140493874636672 run_squad.py:438] token_to_orig_map: 21:0 22:1 23:2 24:3 25:3 26:3 27:4 28:5 29:5 30:5 31:5 32:6 33:7 34:8 35:8 36:8 37:8 38:9 39:10 40:11 41:12 42:12 43:12 44:12 45:13 46:13 47:13 48:14 49:15 50:15 51:16 52:17 53:17 54:18 55:19 56:20 57:21 58:21 59:22 60:23 61:24 62:25 63:26 64:26 65:27 66:28 67:28 68:29 69:30 70:31 71:32 72:33 73:34 74:35 75:35 76:35 77:36 78:37 79:38 80:39 81:39 82:40 83:41 84:42 85:43 86:44 87:45 88:46 89:46 90:47 91:48 92:49 93:50 94:51 95:52 96:53 97:53 98:54 99:55 100:55 101:55 102:55 103:56 104:57 105:57 106:57 107:57 108:58 109:59 110:60 111:61 112:62 113:63 114:63 115:64 116:65 117:65 118:66 119:67 120:68 121:69 122:70 123:71 124:71 125:71 126:71 127:71 128:72 129:73 130:73 131:74 132:74 133:74 134:75 135:76 136:76 137:77 138:78 139:79 140:80 141:81 142:82 143:83 144:84 145:84 146:85 147:85\n",
            "I0629 19:50:44.150579 140493874636672 run_squad.py:440] token_is_max_context: 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True\n",
            "I0629 19:50:44.150763 140493874636672 run_squad.py:442] input_ids: 2 11 35485 436 32594 14 11 206 1916 35116 11927 15 11 36466 9935 5442 1021 24 305 45024 3 265 11 69 34843 46 45015 11 206 1916 35116 11927 15 11 36466 9935 5442 45015 47 771 244 41 35688 197 45015 8837 19 45015 15 28069 45015 36 35020 56 5078 15 55 35485 436 32594 33 18 409 2015 45015 47 1021 19 24 138 11 4844 15 11 41 306 45017 138 55 2003 15488 45015 62 76 36 58 2190 96 18462 45017 89 36 104 97 245 21842 6080 45015 11 10075 313 6506 328 15 13839 6506 328 45015 33 104 12915 1735 105 171 45017 24 70 45015 91 1358 36 442 156 42766 4145 19955 282 45017 11 11992 86 44476 33734 2531 36 352 45015 62 76 264 58 55 18 2272 35485 436 5619 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.150949 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.151109 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.151183 140493874636672 run_squad.py:451] start_position: 69\n",
            "I0629 19:50:44.151248 140493874636672 run_squad.py:452] end_position: 75\n",
            "I0629 19:50:44.151313 140493874636672 run_squad.py:454] answer: both the mouth and the an ##us\n",
            "I0629 19:50:44.154258 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.154468 140493874636672 run_squad.py:432] unique_id: 1000000014\n",
            "I0629 19:50:44.154569 140493874636672 run_squad.py:433] example_index: 14\n",
            "I0629 19:50:44.154648 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.154743 140493874636672 run_squad.py:436] tokens: [CLS] when did buddhism leave zhejiang ? [SEP] in mid - 2015 the government of zhejiang recognise ##d folk religion as \" civil religion \" beginning the registration of more than twenty thousand folk religious associations . buddhism has an important presence since its arrival in zhejiang 1 , 800 years ago . [SEP]\n",
            "I0629 19:50:44.154838 140493874636672 run_squad.py:438] token_to_orig_map: 8:0 9:1 10:1 11:1 12:2 13:3 14:4 15:5 16:6 17:6 18:7 19:8 20:9 21:10 22:10 23:11 24:11 25:12 26:13 27:14 28:15 29:16 30:17 31:18 32:19 33:20 34:21 35:22 36:22 37:23 38:24 39:25 40:26 41:27 42:28 43:29 44:30 45:31 46:32 47:33 48:33 49:33 50:34 51:35 52:35\n",
            "I0629 19:50:44.154924 140493874636672 run_squad.py:440] token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True\n",
            "I0629 19:50:44.155103 140493874636672 run_squad.py:442] input_ids: 2 94 278 8017 2444 27438 45024 3 16 699 45016 1107 11 125 14 27438 4944 48 1949 2202 24 45005 510 2202 45005 758 11 9245 14 81 92 2963 3012 1949 1050 12018 45017 8017 57 41 325 1426 131 67 2766 16 27438 277 45015 9734 124 3201 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.155266 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.155420 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.155495 140493874636672 run_squad.py:448] impossible example\n",
            "I0629 19:50:44.162685 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.162923 140493874636672 run_squad.py:432] unique_id: 1000000015\n",
            "I0629 19:50:44.163012 140493874636672 run_squad.py:433] example_index: 15\n",
            "I0629 19:50:44.163083 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.163251 140493874636672 run_squad.py:436] tokens: [CLS] who embrace ##d his family according to the new testament ? [SEP] the ha ##gio ##graph ##y of mary and the holy family can be contrast ##ed with other material in the gospels . these references include an incident which can be interpreted as jesus reject ##ing his family in the new testament : \" and his mother and his brothers arrived , and standing outside , they sent in a message aski ##ng for him . . . and look ##ing at those who sat in a circle around him , jesus said , ' these are my mother and my brothers . who ##ever does the will of god is my brother , and sister , and mother ' . \" [ 3 : 31 - 35 ] other verses suggest a conflict between jesus and his family , including an attempt to have jesus restrained because \" he is out of his mind \" , and the famous quote : \" a prophet is not without honor except in his own town , among his relatives and in his own home . \" a lead ##ing bibli ##cal scholar commented : \" there are clear sign ##s not only that jesus ' s family rejected his message duri ##ng his public ministry but that he in turn spur ##ned them publicly \" . [SEP]\n",
            "I0629 19:50:44.240628 140493874636672 run_squad.py:438] token_to_orig_map: 13:0 14:1 15:1 16:1 17:1 18:2 19:3 20:4 21:5 22:6 23:7 24:8 25:9 26:10 27:10 28:11 29:12 30:13 31:14 32:15 33:16 34:16 35:17 36:18 37:19 38:20 39:21 40:22 41:23 42:24 43:25 44:26 45:27 46:28 47:28 48:29 49:30 50:31 51:32 52:33 53:34 54:34 55:35 56:35 57:36 58:37 59:38 60:39 61:40 62:41 63:41 64:42 65:43 66:44 67:44 68:45 69:46 70:47 71:48 72:49 73:50 74:50 75:51 76:52 77:53 78:53 79:53 80:54 81:55 82:55 83:56 84:57 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:64 93:65 94:66 95:66 96:67 97:67 98:68 99:69 100:70 101:71 102:72 103:73 104:73 105:74 106:74 107:75 108:76 109:77 110:78 111:79 112:80 113:81 114:82 115:82 116:83 117:84 118:84 119:85 120:86 121:86 122:86 123:86 124:86 125:86 126:86 127:86 128:86 129:86 130:86 131:87 132:88 133:89 134:90 135:91 136:92 137:93 138:94 139:95 140:96 141:96 142:97 143:98 144:99 145:100 146:101 147:102 148:103 149:104 150:105 151:105 152:106 153:107 154:108 155:109 156:110 157:110 158:110 159:111 160:112 161:113 162:114 163:114 164:115 165:115 166:116 167:117 168:118 169:119 170:120 171:121 172:122 173:123 174:124 175:125 176:125 177:126 178:127 179:128 180:129 181:130 182:131 183:132 184:133 185:133 186:133 187:134 188:135 189:135 190:136 191:136 192:137 193:138 194:138 195:139 196:139 197:140 198:141 199:142 200:142 201:143 202:144 203:145 204:146 205:146 206:146 207:147 208:148 209:149 210:150 211:151 212:151 213:152 214:153 215:154 216:155 217:156 218:157 219:158 220:159 221:160 222:160 223:161 224:162 225:162 226:162\n",
            "I0629 19:50:44.241004 140493874636672 run_squad.py:440] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True\n",
            "I0629 19:50:44.241302 140493874636672 run_squad.py:442] input_ids: 2 74 7915 48 44 416 299 17 11 88 1458 45024 3 11 1058 19978 36668 86 14 3354 15 11 1774 416 79 42 1367 35 33 69 870 16 11 24428 45017 91 3395 193 41 3680 47 79 42 3071 24 737 3706 28 44 416 16 11 88 1458 45019 45005 15 44 1080 15 44 3493 1767 45015 15 3272 801 45015 76 645 16 18 2075 24259 389 27 164 45017 45017 45017 15 1508 28 50 224 74 10087 16 18 1477 247 164 45015 737 425 45015 45010 91 36 1316 1080 15 1316 3493 45017 74 4770 373 11 169 14 322 21 1316 1754 45015 15 2678 45015 15 1080 45010 45017 45005 45026 413 45019 2624 45016 3255 45028 69 4515 1851 18 1751 105 737 15 44 416 45015 162 41 1008 17 55 737 29399 186 45005 54 21 148 14 44 2386 45005 45015 15 11 813 11147 45019 45005 18 2201 21 58 390 1708 1746 16 44 269 652 45015 265 44 14286 15 16 44 269 331 45017 45005 18 438 28 7257 2590 2571 5233 45019 45005 89 36 1393 2461 19 58 104 29 737 45010 294 416 2424 44 2075 940 389 44 260 2522 62 29 54 16 1032 8445 5097 171 5099 45005 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.241615 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.241922 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.242058 140493874636672 run_squad.py:448] impossible example\n",
            "I0629 19:50:44.248106 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.248388 140493874636672 run_squad.py:432] unique_id: 1000000016\n",
            "I0629 19:50:44.248494 140493874636672 run_squad.py:433] example_index: 16\n",
            "I0629 19:50:44.248585 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.248739 140493874636672 run_squad.py:436] tokens: [CLS] where does a bill go once the president sign ##s it into effect ? [SEP] after the president sign ##s a bill into law ( or congress enact ##s it over his veto ) , it is delivered to the office of the federal register ( of ##r ) of the national archives and records administration ( nar ##a ) where it is assigne ##d a law number , and prepared for publication as a slip law . public laws , but not private laws , are also given legal statut ##ory citation by the of ##r . at the end of each session of congress , the slip laws are compiled into bound volumes called the united states statutes at large , and they are known as session laws . the statutes at large present a chronological arrangement of the laws in the exact order that they have been enact ##ed . [SEP]\n",
            "I0629 19:50:44.248884 140493874636672 run_squad.py:438] token_to_orig_map: 16:0 17:1 18:2 19:3 20:3 21:4 22:5 23:6 24:7 25:8 26:8 27:9 28:10 29:10 30:11 31:12 32:13 33:14 34:14 35:14 36:15 37:16 38:17 39:18 40:19 41:20 42:21 43:22 44:23 45:24 46:25 47:25 48:25 49:25 50:26 51:27 52:28 53:29 54:30 55:31 56:32 57:33 58:33 59:33 60:33 61:34 62:35 63:36 64:37 65:37 66:38 67:39 68:40 69:40 70:41 71:42 72:43 73:44 74:45 75:46 76:47 77:48 78:48 79:49 80:50 81:50 82:51 83:52 84:53 85:54 86:54 87:55 88:56 89:57 90:58 91:59 92:59 93:60 94:61 95:62 96:63 97:63 98:63 99:64 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:71 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:79 116:80 117:81 118:82 119:83 120:84 121:85 122:86 123:86 124:87 125:88 126:89 127:90 128:91 129:92 130:93 131:93 132:94 133:95 134:96 135:97 136:98 137:99 138:100 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:111 150:112 151:113 152:113 153:113\n",
            "I0629 19:50:44.249015 140493874636672 run_squad.py:440] token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True\n",
            "I0629 19:50:44.249206 140493874636672 run_squad.py:442] input_ids: 2 118 373 18 1413 837 621 11 432 2461 19 39 96 634 45024 3 85 11 432 2461 19 18 1413 96 201 45011 43 2224 24664 19 39 110 44 11560 45012 45015 39 21 2773 17 11 1012 14 11 1014 3682 45011 14 179 45012 14 11 207 8905 15 1038 1732 45011 30953 46 45012 118 39 21 13830 48 18 201 158 45015 15 3345 27 2446 24 18 15363 201 45017 260 1018 45015 62 58 779 1018 45015 36 60 324 899 15946 2366 10506 31 11 14 179 45017 50 11 248 14 194 8570 14 2224 45015 11 15363 1018 36 3866 96 3645 7900 156 11 161 151 12028 50 181 45015 15 76 36 127 24 8570 1018 45017 11 12028 50 181 528 18 13039 5145 14 11 1018 16 11 2280 320 29 76 55 65 24664 35 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.249390 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.249577 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.249658 140493874636672 run_squad.py:451] start_position: 38\n",
            "I0629 19:50:44.249726 140493874636672 run_squad.py:452] end_position: 60\n",
            "I0629 19:50:44.249793 140493874636672 run_squad.py:454] answer: delivered to the office of the federal register ( of ##r ) of the national archives and records administration ( nar ##a )\n",
            "I0629 19:50:44.256338 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.256602 140493874636672 run_squad.py:432] unique_id: 1000000017\n",
            "I0629 19:50:44.256692 140493874636672 run_squad.py:433] example_index: 17\n",
            "I0629 19:50:44.256765 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.256922 140493874636672 run_squad.py:436] tokens: [CLS] what other beginning ##s of originat ##ion do some of the last names of the greeks share ? [SEP] greek surnames were widely in use by the 9 ##th century supplant ##ing the ancient tradition of us ##ing the father [UNK] s name , however greek surnames are most commonly patron ##ym ##ics . commonly , greek male surnames end in - s , which is the common ending for greek masculin ##e proper nouns in the nominat ##ive case . exceptional ##ly , some end in - ou , indicati ##ng the geni ##tive case of this proper noun for patron ##ym ##ic reasons . although surnames in mainland greece are static today , dynamic and changing patron ##ym ##ic usage survives in middle names where the geni ##tive of father ' s first name is commonly the middle name ( this usage havi ##ng been passed on to the russians ) . in cyprus , by contrast , surnames follow the ancient tradition of being given according to the father [UNK] s name . finally , in addition to greek - derived surnames many have latin , turkish and italian origin . [SEP]\n",
            "I0629 19:50:44.257068 140493874636672 run_squad.py:438] token_to_orig_map: 20:0 21:1 22:2 23:3 24:4 25:5 26:6 27:7 28:8 29:8 30:9 31:10 32:10 33:11 34:12 35:13 36:14 37:15 38:15 39:16 40:17 41:17 42:17 43:18 44:18 45:19 46:20 47:21 48:22 49:23 50:24 51:25 52:25 53:25 54:25 55:26 56:26 57:27 58:28 59:29 60:30 61:31 62:32 63:32 64:32 65:33 66:34 67:35 68:36 69:37 70:38 71:39 72:40 73:40 74:41 75:42 76:43 77:44 78:45 79:45 80:46 81:46 82:47 83:47 84:47 85:48 86:49 87:50 88:51 89:51 90:51 91:52 92:52 93:53 94:54 95:54 96:55 97:56 98:57 99:58 100:59 101:60 102:61 103:61 104:61 105:62 106:62 107:63 108:64 109:65 110:66 111:67 112:68 113:69 114:70 115:70 116:71 117:72 118:73 119:74 120:74 121:74 122:75 123:76 124:77 125:78 126:79 127:80 128:81 129:82 130:82 131:83 132:84 133:84 134:84 135:85 136:86 137:87 138:88 139:89 140:90 141:91 142:92 143:92 144:93 145:94 146:94 147:95 148:96 149:97 150:98 151:99 152:100 153:100 154:100 155:101 156:102 157:102 158:103 159:104 160:104 161:105 162:106 163:107 164:108 165:109 166:110 167:111 168:112 169:113 170:114 171:115 172:116 173:116 174:116 175:117 176:117 177:118 178:118 179:119 180:120 181:121 182:122 183:122 184:122 185:123 186:124 187:125 188:126 189:126 190:127 191:128 192:129 193:130 194:130\n",
            "I0629 19:50:44.257202 140493874636672 run_squad.py:440] token_is_max_context: 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True\n",
            "I0629 19:50:44.257384 140493874636672 run_squad.py:442] input_ids: 2 305 69 758 19 14 4062 583 264 75 14 11 316 1161 14 11 23264 1956 45024 3 865 31214 53 787 16 101 31 11 1065 108 159 7488 28 11 929 1227 14 280 28 11 790 1 294 170 45015 115 865 31214 36 78 935 5705 34968 31058 45017 935 45015 865 1489 31214 248 16 45016 294 45015 47 21 11 251 2947 27 865 14525 66 1978 11647 16 11 10782 1049 446 45017 8495 56 45015 75 248 16 45016 24739 45015 37194 389 11 42267 10128 446 14 51 1978 18344 27 5705 34968 282 1600 45017 192 31214 16 1734 3845 36 7430 636 45015 4556 15 2479 5705 34968 282 2893 10698 16 575 1161 118 11 42267 10128 14 790 45010 294 73 170 21 935 11 575 170 45011 51 2893 23762 389 65 1827 34 17 11 19347 45012 45017 16 9511 45015 31 1367 45015 31214 1725 11 929 1227 14 142 324 299 17 11 790 1 294 170 45017 1541 45015 16 418 17 865 45016 1450 31214 98 55 833 45015 3736 15 1073 1466 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.257563 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.257726 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.257800 140493874636672 run_squad.py:451] start_position: 186\n",
            "I0629 19:50:44.257864 140493874636672 run_squad.py:452] end_position: 194\n",
            "I0629 19:50:44.257931 140493874636672 run_squad.py:454] answer: many have latin , turkish and italian origin .\n",
            "I0629 19:50:44.260786 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.260986 140493874636672 run_squad.py:432] unique_id: 1000000018\n",
            "I0629 19:50:44.261090 140493874636672 run_squad.py:433] example_index: 18\n",
            "I0629 19:50:44.261162 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.261260 140493874636672 run_squad.py:436] tokens: [CLS] what is the function of a \" beer engine \" ? [SEP] a \" beer engine \" is a device for pump ##ing beer , originally manual ##ly operated and typically used to dispense beer from a cas ##k or container in a pu ##b ' s basement or cell ##ar . [SEP]\n",
            "I0629 19:50:44.261353 140493874636672 run_squad.py:438] token_to_orig_map: 13:0 14:1 15:1 16:2 17:2 18:3 19:4 20:5 21:6 22:7 23:7 24:8 25:8 26:9 27:10 28:10 29:11 30:12 31:13 32:14 33:15 34:16 35:17 36:18 37:19 38:20 39:20 40:21 41:22 42:23 43:24 44:25 45:25 46:25 47:25 48:26 49:27 50:28 51:28 52:28\n",
            "I0629 19:50:44.261445 140493874636672 run_squad.py:440] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True\n",
            "I0629 19:50:44.261635 140493874636672 run_squad.py:442] input_ids: 2 305 21 11 283 14 18 45005 11812 3802 45005 45024 3 18 45005 11812 3802 45005 21 18 2027 27 15122 28 11812 45015 909 4880 56 2489 15 822 90 17 24600 11812 37 18 7706 72 43 9295 16 18 5977 477 45010 294 18487 43 1138 1072 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.344021 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.344409 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.344599 140493874636672 run_squad.py:451] start_position: 33\n",
            "I0629 19:50:44.344708 140493874636672 run_squad.py:452] end_position: 51\n",
            "I0629 19:50:44.344805 140493874636672 run_squad.py:454] answer: to dispense beer from a cas ##k or container in a pu ##b ' s basement or cell ##ar\n",
            "I0629 19:50:44.349639 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 19:50:44.349866 140493874636672 run_squad.py:432] unique_id: 1000000019\n",
            "I0629 19:50:44.349953 140493874636672 run_squad.py:433] example_index: 19\n",
            "I0629 19:50:44.350028 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 19:50:44.350165 140493874636672 run_squad.py:436] tokens: [CLS] what coat ##s uranium metal in liquid ? [SEP] uranium metal react ##s with almost all non - metal elements ( with an exception of the noble gases ) and thei ##r compounds , with reactiv ##ity increasing with temperature . hydro ##chlor ##ic and nitri ##c acids dissolve uranium , but non - oxidiz ##ing acids other than hydro ##chlor ##ic acid attack the element very slowly . when fine ##ly divided , it can react with cold water ; in air , uranium metal becomes coat ##ed with a dark layer of uranium oxide . uranium in ores is extracted chemical ##ly and converted into uranium dioxide or other chemical forms usa ##ble in industry . [SEP]\n",
            "I0629 19:50:44.350301 140493874636672 run_squad.py:438] token_to_orig_map: 10:0 11:1 12:2 13:2 14:3 15:4 16:5 17:6 18:6 19:6 20:7 21:8 22:8 23:9 24:10 25:11 26:12 27:13 28:14 29:14 30:15 31:16 32:16 33:17 34:17 35:18 36:19 37:19 38:20 39:21 40:22 41:22 42:23 43:23 44:23 45:24 46:25 47:25 48:26 49:27 50:28 51:28 52:29 53:30 54:30 55:30 56:30 57:31 58:32 59:33 60:34 61:34 62:34 63:35 64:36 65:37 66:38 67:39 68:40 69:40 70:41 71:42 72:42 73:43 74:43 75:44 76:45 77:46 78:47 79:48 80:49 81:49 82:50 83:51 84:51 85:52 86:53 87:54 88:55 89:55 90:56 91:57 92:58 93:59 94:60 95:61 96:62 97:62 98:63 99:64 100:65 101:66 102:67 103:68 104:68 105:69 106:70 107:71 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:78 116:79 117:80 118:80\n",
            "I0629 19:50:44.350429 140493874636672 run_squad.py:440] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True\n",
            "I0629 19:50:44.350646 140493874636672 run_squad.py:442] input_ids: 2 305 9838 19 18081 1491 16 3115 45024 3 18081 1491 3593 19 33 739 84 275 45016 1491 911 45011 33 41 2126 14 11 5110 2427 45012 15 10026 179 3885 45015 33 8392 1185 1425 33 647 45017 6044 25325 282 15 24843 165 5064 9692 18081 45015 62 275 45016 16757 28 5064 69 92 6044 25325 282 848 788 11 2337 268 5988 45017 94 4129 56 1140 45015 39 79 3593 33 1656 332 45020 16 234 45015 18081 1491 2556 9838 35 33 18 5783 4233 14 18081 7958 45017 18081 16 19223 21 11492 1568 56 15 1560 96 18081 3615 43 69 1568 902 5782 2463 16 650 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.350822 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.350995 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 19:50:44.351067 140493874636672 run_squad.py:448] impossible example\n",
            "I0629 20:02:31.261943 140493874636672 run_squad.py:1203] ***** Running training *****\n",
            "I0629 20:02:31.262363 140493874636672 run_squad.py:1204]   Num orig examples = 130319\n",
            "I0629 20:02:31.262495 140493874636672 run_squad.py:1205]   Num split examples = 132304\n",
            "I0629 20:02:31.262616 140493874636672 run_squad.py:1206]   Batch size = 24\n",
            "I0629 20:02:31.262696 140493874636672 run_squad.py:1207]   Num steps = 542\n",
            "W0629 20:02:31.502569 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:691: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "I0629 20:02:31.837538 140493874636672 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.2.81.2:8470) for TPU system metadata.\n",
            "2019-06-29 20:02:31.839726: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 20:02:31.853380 140493874636672 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0629 20:02:31.853649 140493874636672 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0629 20:02:31.853746 140493874636672 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0629 20:02:31.853816 140493874636672 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0629 20:02:31.853886 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5201560055092340784)\n",
            "I0629 20:02:31.854749 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15235922008977912019)\n",
            "I0629 20:02:31.854825 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16587921834020422802)\n",
            "I0629 20:02:31.854893 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 18249457006028921626)\n",
            "I0629 20:02:31.854963 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9769923856279949422)\n",
            "I0629 20:02:31.855032 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14707561106018069366)\n",
            "I0629 20:02:31.855093 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11546342222793515035)\n",
            "I0629 20:02:31.855155 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15225485262720068302)\n",
            "I0629 20:02:31.855216 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 18107303912184056565)\n",
            "I0629 20:02:31.855277 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12015888753985987776)\n",
            "I0629 20:02:31.855357 140493874636672 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6671482654539601268)\n",
            "W0629 20:02:31.863505 140493874636672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0629 20:02:31.877984 140493874636672 estimator.py:1145] Calling model_fn.\n",
            "W0629 20:02:31.904130 140493874636672 deprecation.py:323] From bert/run_squad.py:730: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0629 20:02:31.904402 140493874636672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0629 20:02:31.906207 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:703: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0629 20:02:31.913299 140493874636672 deprecation.py:323] From bert/run_squad.py:710: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0629 20:02:31.997655 140493874636672 run_squad.py:598] *** Features ***\n",
            "I0629 20:02:31.997916 140493874636672 run_squad.py:600]   name = end_positions, shape = (3,)\n",
            "I0629 20:02:31.998023 140493874636672 run_squad.py:600]   name = input_ids, shape = (3, 384)\n",
            "I0629 20:02:31.998111 140493874636672 run_squad.py:600]   name = input_mask, shape = (3, 384)\n",
            "I0629 20:02:31.998194 140493874636672 run_squad.py:600]   name = segment_ids, shape = (3, 384)\n",
            "I0629 20:02:31.998274 140493874636672 run_squad.py:600]   name = start_positions, shape = (3,)\n",
            "I0629 20:02:31.998351 140493874636672 run_squad.py:600]   name = unique_ids, shape = (3,)\n",
            "W0629 20:02:31.998633 140493874636672 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0629 20:02:32.001117 140493874636672 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0629 20:02:32.039119 140493874636672 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0629 20:02:32.097624 140493874636672 deprecation.py:506] From /content/gureBERT/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0629 20:02:32.123269 140493874636672 deprecation.py:323] From /content/gureBERT/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0629 20:02:37.173002 140493874636672 run_squad.py:634] **** Trainable Variables ****\n",
            "I0629 20:02:37.173288 140493874636672 run_squad.py:640]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.173445 140493874636672 run_squad.py:640]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.173544 140493874636672 run_squad.py:640]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.173653 140493874636672 run_squad.py:640]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.173742 140493874636672 run_squad.py:640]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.173824 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.173916 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174003 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174082 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174156 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174233 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174307 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174386 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174478 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174566 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174648 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174726 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174806 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174885 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.174970 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175047 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175122 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175200 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175275 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175353 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175428 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175507 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175595 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175679 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175754 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175827 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175904 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.175989 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176064 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176141 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176215 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176290 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176363 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176438 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176511 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176609 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176689 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176768 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176843 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.176923 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177008 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177083 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177157 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177235 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177310 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177388 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177464 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177539 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177634 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177713 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177788 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177868 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.177950 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178028 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178102 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178181 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178257 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178344 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178441 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178523 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178623 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178703 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178778 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178851 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.178925 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179012 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179085 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179173 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179247 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179325 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179401 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179481 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179571 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179650 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179724 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179802 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179877 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.179960 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180035 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180108 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180181 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180258 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180333 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180412 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180504 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180600 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180680 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180759 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180839 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.180919 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181004 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181087 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181176 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181257 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181339 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181417 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181496 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181594 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181683 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181766 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181844 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.181924 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182012 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182095 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182178 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182261 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182339 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182422 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182502 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182605 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182686 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182762 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182844 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.182927 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.183016 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.183097 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.225997 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.226299 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.226435 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.226613 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.226755 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.226877 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.226989 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.227115 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.227231 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.227363 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.227512 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.227653 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.227768 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.227883 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.227993 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.228103 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.228211 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.228323 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.228428 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.228581 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.228697 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.228804 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.228909 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.229024 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.229129 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.229238 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.229343 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.229462 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.229598 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.229716 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.229823 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.229932 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.230039 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.230146 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.230250 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.230359 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.230477 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.230609 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.230720 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.230832 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.230937 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.231050 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.231155 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.231259 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.231362 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.231487 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.231616 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.231730 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.231837 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.231946 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.232054 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.232162 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.232268 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.232370 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.232508 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.232644 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.232749 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.232851 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.232947 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.233049 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.233148 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.233253 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.233352 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.233466 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.233590 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.233722 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.233825 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.233929 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.234031 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.234131 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.234229 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.234333 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.234431 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.234578 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.234694 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.234799 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.234902 140493874636672 run_squad.py:640]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.235014 140493874636672 run_squad.py:640]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:02:37.235121 140493874636672 run_squad.py:640]   name = cls/squad/output_weights:0, shape = (2, 768)\n",
            "I0629 20:02:37.235231 140493874636672 run_squad.py:640]   name = cls/squad/output_bias:0, shape = (2,)\n",
            "W0629 20:02:37.259290 140493874636672 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0629 20:02:37.260711 140493874636672 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0629 20:02:37.269497 140493874636672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0629 20:02:37.589076 140493874636672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0629 20:02:50.752052 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:627: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "W0629 20:02:52.543212 140493874636672 deprecation_wrapper.py:119] From bert/run_squad.py:628: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
            "\n",
            "I0629 20:02:53.557838 140493874636672 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0629 20:02:53.942099 140493874636672 estimator.py:1147] Done calling model_fn.\n",
            "I0629 20:02:57.913097 140493874636672 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 20:02:59.683392 140493874636672 monitored_session.py:240] Graph was finalized.\n",
            "I0629 20:03:36.391830 140493874636672 session_manager.py:500] Running local_init_op.\n",
            "I0629 20:03:37.323231 140493874636672 session_manager.py:502] Done running local_init_op.\n",
            "I0629 20:03:49.517123 140493874636672 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://gurebert/gureBERT/wordpiece/squad/model.ckpt.\n",
            "W0629 20:04:18.614612 140493874636672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0629 20:04:20.607192 140493874636672 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0629 20:04:20.608288 140493874636672 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-06-29 20:04:20.608700: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 20:04:20.613370 140493874636672 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0629 20:04:20.615488 140493874636672 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0629 20:04:20.620305 140493874636672 tpu_estimator.py:557] Init TPU system\n",
            "I0629 20:04:28.226780 140493874636672 tpu_estimator.py:566] Initialized TPU in 7 seconds\n",
            "I0629 20:04:28.227538 140492669212416 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 20:04:28.227949 140492660819712 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 20:04:29.163921 140493874636672 tpu_estimator.py:590] Enqueue next (542) batch(es) of data to infeed.\n",
            "I0629 20:04:29.164979 140493874636672 tpu_estimator.py:594] Dequeue next (542) batch(es) of data from outfeed.\n",
            "I0629 20:05:01.042129 140492660819712 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 20:05:43.706050 140493874636672 basic_session_run_hooks.py:262] loss = 4.4738016, step = 542\n",
            "I0629 20:05:43.708451 140493874636672 basic_session_run_hooks.py:606] Saving checkpoints for 542 into gs://gurebert/gureBERT/wordpiece/squad/model.ckpt.\n",
            "I0629 20:06:11.058093 140493874636672 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 20:06:11.058371 140493874636672 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 20:06:11.058635 140492669212416 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 20:06:11.058757 140492669212416 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 20:06:11.058920 140493874636672 error_handling.py:96] infeed marked as finished\n",
            "I0629 20:06:11.059043 140493874636672 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 20:06:11.059143 140493874636672 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 20:06:11.059301 140492660819712 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 20:06:11.059383 140492660819712 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 20:06:11.059508 140493874636672 error_handling.py:96] outfeed marked as finished\n",
            "I0629 20:06:11.059643 140493874636672 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 20:06:12.817024 140493874636672 estimator.py:368] Loss for final step: 4.4738016.\n",
            "I0629 20:06:12.818370 140493874636672 error_handling.py:96] training_loop marked as finished\n",
            "I0629 20:06:13.540153 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.540436 140493874636672 run_squad.py:432] unique_id: 1000000000\n",
            "I0629 20:06:13.540591 140493874636672 run_squad.py:433] example_index: 0\n",
            "I0629 20:06:13.540659 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.540791 140493874636672 run_squad.py:436] tokens: [CLS] in what country is normandy located ? [SEP] the norman ##s ( norman : no ##ur ##mand ##s ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10 ##th and 11 ##th centuries gave thei ##r name to normandy , a region in france . they were descende ##d from norse ( \" norman \" comes from \" norse ##man \" ) raider ##s and pirates from denmark , iceland and norway who , under thei ##r leader roll ##o , agreed to swear fea ##lty to king charles iii of west francia . through generations of assimilation and mixing with the native frankis ##h and roman - gaul ##ish populations , thei ##r descendants would gradually merge with the carolin ##gian - based cultures of west francia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10 ##th century , and it continued to evolve over the succeed ##ing centuries . [SEP]\n",
            "I0629 20:06:13.540925 140493874636672 run_squad.py:438] token_to_orig_map: 9:0 10:1 11:1 12:2 13:2 14:2 15:3 16:3 17:3 18:3 19:3 20:4 21:4 22:5 23:5 24:5 25:6 26:6 27:7 28:7 29:7 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:14 38:15 39:16 40:16 41:17 42:18 43:19 44:19 45:20 46:21 47:22 48:22 49:23 50:24 51:25 52:26 53:26 54:27 55:28 56:29 57:29 58:30 59:31 60:32 61:32 62:32 63:32 64:33 65:34 66:35 67:35 68:35 69:35 70:35 71:36 72:36 73:37 74:38 75:39 76:40 77:40 78:41 79:42 80:43 81:44 82:44 83:45 84:46 85:46 86:47 87:48 88:48 89:48 90:49 91:50 92:51 93:52 94:52 95:53 96:54 97:55 98:56 99:57 100:58 101:59 102:59 103:60 104:61 105:62 106:63 107:64 108:65 109:66 110:67 111:68 112:69 113:69 114:70 115:71 116:71 117:71 118:71 119:72 120:72 121:73 122:73 123:74 124:75 125:76 126:77 127:78 128:79 129:80 130:80 131:80 132:80 133:81 134:82 135:83 136:84 137:84 138:85 139:86 140:87 141:88 142:89 143:90 144:91 145:92 146:93 147:93 148:94 149:95 150:96 151:97 152:98 153:99 154:100 155:101 156:102 157:102 158:103 159:103 160:104 161:105 162:106 163:107 164:108 165:109 166:110 167:111 168:111 169:112 170:112\n",
            "I0629 20:06:13.541061 140493874636672 run_squad.py:440] token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True\n",
            "I0629 20:06:13.541253 140493874636672 run_squad.py:442] input_ids: 2 16 305 187 21 6610 750 45024 3 11 7583 19 45011 7583 45019 111 1738 36122 19 45020 140 45019 7583 11607 45020 833 45019 7583 3709 45012 53 11 126 74 16 11 524 108 15 930 108 1344 770 10026 179 170 17 6610 45015 18 526 16 204 45017 76 53 15969 48 37 3133 45011 45005 7583 45005 1823 37 45005 3133 767 45005 45012 32432 19 15 8682 37 1512 45015 4967 15 2229 74 45015 128 10026 179 1066 2513 99 45015 1773 17 29741 34650 44632 17 249 1093 1539 14 357 17643 45017 176 7263 14 10197 15 5522 33 11 1196 6962 137 15 525 45016 9765 2297 3513 45015 10026 179 5995 107 3132 15924 33 11 10684 31888 45016 261 2193 14 357 17643 45017 11 2003 858 15 1542 1697 14 11 7583 19 2054 1296 16 11 73 573 14 11 524 108 159 45015 15 39 617 17 8828 110 11 5883 28 1344 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.541397 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.541565 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.546876 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.547115 140493874636672 run_squad.py:432] unique_id: 1000000001\n",
            "I0629 20:06:13.547186 140493874636672 run_squad.py:433] example_index: 1\n",
            "I0629 20:06:13.547249 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.547382 140493874636672 run_squad.py:436] tokens: [CLS] when were the norman ##s in normandy ? [SEP] the norman ##s ( norman : no ##ur ##mand ##s ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10 ##th and 11 ##th centuries gave thei ##r name to normandy , a region in france . they were descende ##d from norse ( \" norman \" comes from \" norse ##man \" ) raider ##s and pirates from denmark , iceland and norway who , under thei ##r leader roll ##o , agreed to swear fea ##lty to king charles iii of west francia . through generations of assimilation and mixing with the native frankis ##h and roman - gaul ##ish populations , thei ##r descendants would gradually merge with the carolin ##gian - based cultures of west francia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10 ##th century , and it continued to evolve over the succeed ##ing centuries . [SEP]\n",
            "I0629 20:06:13.547514 140493874636672 run_squad.py:438] token_to_orig_map: 10:0 11:1 12:1 13:2 14:2 15:2 16:3 17:3 18:3 19:3 20:3 21:4 22:4 23:5 24:5 25:5 26:6 27:6 28:7 29:7 30:7 31:8 32:9 33:10 34:11 35:12 36:13 37:14 38:14 39:15 40:16 41:16 42:17 43:18 44:19 45:19 46:20 47:21 48:22 49:22 50:23 51:24 52:25 53:26 54:26 55:27 56:28 57:29 58:29 59:30 60:31 61:32 62:32 63:32 64:32 65:33 66:34 67:35 68:35 69:35 70:35 71:35 72:36 73:36 74:37 75:38 76:39 77:40 78:40 79:41 80:42 81:43 82:44 83:44 84:45 85:46 86:46 87:47 88:48 89:48 90:48 91:49 92:50 93:51 94:52 95:52 96:53 97:54 98:55 99:56 100:57 101:58 102:59 103:59 104:60 105:61 106:62 107:63 108:64 109:65 110:66 111:67 112:68 113:69 114:69 115:70 116:71 117:71 118:71 119:71 120:72 121:72 122:73 123:73 124:74 125:75 126:76 127:77 128:78 129:79 130:80 131:80 132:80 133:80 134:81 135:82 136:83 137:84 138:84 139:85 140:86 141:87 142:88 143:89 144:90 145:91 146:92 147:93 148:93 149:94 150:95 151:96 152:97 153:98 154:99 155:100 156:101 157:102 158:102 159:103 160:103 161:104 162:105 163:106 164:107 165:108 166:109 167:110 168:111 169:111 170:112 171:112\n",
            "I0629 20:06:13.547652 140493874636672 run_squad.py:440] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True\n",
            "I0629 20:06:13.547812 140493874636672 run_squad.py:442] input_ids: 2 94 53 11 7583 19 16 6610 45024 3 11 7583 19 45011 7583 45019 111 1738 36122 19 45020 140 45019 7583 11607 45020 833 45019 7583 3709 45012 53 11 126 74 16 11 524 108 15 930 108 1344 770 10026 179 170 17 6610 45015 18 526 16 204 45017 76 53 15969 48 37 3133 45011 45005 7583 45005 1823 37 45005 3133 767 45005 45012 32432 19 15 8682 37 1512 45015 4967 15 2229 74 45015 128 10026 179 1066 2513 99 45015 1773 17 29741 34650 44632 17 249 1093 1539 14 357 17643 45017 176 7263 14 10197 15 5522 33 11 1196 6962 137 15 525 45016 9765 2297 3513 45015 10026 179 5995 107 3132 15924 33 11 10684 31888 45016 261 2193 14 357 17643 45017 11 2003 858 15 1542 1697 14 11 7583 19 2054 1296 16 11 73 573 14 11 524 108 159 45015 15 39 617 17 8828 110 11 5883 28 1344 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.547959 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.548105 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.553262 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.553473 140493874636672 run_squad.py:432] unique_id: 1000000002\n",
            "I0629 20:06:13.553540 140493874636672 run_squad.py:433] example_index: 2\n",
            "I0629 20:06:13.553635 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.553762 140493874636672 run_squad.py:436] tokens: [CLS] from which countries did the norse originate ? [SEP] the norman ##s ( norman : no ##ur ##mand ##s ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10 ##th and 11 ##th centuries gave thei ##r name to normandy , a region in france . they were descende ##d from norse ( \" norman \" comes from \" norse ##man \" ) raider ##s and pirates from denmark , iceland and norway who , under thei ##r leader roll ##o , agreed to swear fea ##lty to king charles iii of west francia . through generations of assimilation and mixing with the native frankis ##h and roman - gaul ##ish populations , thei ##r descendants would gradually merge with the carolin ##gian - based cultures of west francia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10 ##th century , and it continued to evolve over the succeed ##ing centuries . [SEP]\n",
            "I0629 20:06:13.553889 140493874636672 run_squad.py:438] token_to_orig_map: 10:0 11:1 12:1 13:2 14:2 15:2 16:3 17:3 18:3 19:3 20:3 21:4 22:4 23:5 24:5 25:5 26:6 27:6 28:7 29:7 30:7 31:8 32:9 33:10 34:11 35:12 36:13 37:14 38:14 39:15 40:16 41:16 42:17 43:18 44:19 45:19 46:20 47:21 48:22 49:22 50:23 51:24 52:25 53:26 54:26 55:27 56:28 57:29 58:29 59:30 60:31 61:32 62:32 63:32 64:32 65:33 66:34 67:35 68:35 69:35 70:35 71:35 72:36 73:36 74:37 75:38 76:39 77:40 78:40 79:41 80:42 81:43 82:44 83:44 84:45 85:46 86:46 87:47 88:48 89:48 90:48 91:49 92:50 93:51 94:52 95:52 96:53 97:54 98:55 99:56 100:57 101:58 102:59 103:59 104:60 105:61 106:62 107:63 108:64 109:65 110:66 111:67 112:68 113:69 114:69 115:70 116:71 117:71 118:71 119:71 120:72 121:72 122:73 123:73 124:74 125:75 126:76 127:77 128:78 129:79 130:80 131:80 132:80 133:80 134:81 135:82 136:83 137:84 138:84 139:85 140:86 141:87 142:88 143:89 144:90 145:91 146:92 147:93 148:93 149:94 150:95 151:96 152:97 153:98 154:99 155:100 156:101 157:102 158:102 159:103 160:103 161:104 162:105 163:106 164:107 165:108 166:109 167:110 168:111 169:111 170:112 171:112\n",
            "I0629 20:06:13.554021 140493874636672 run_squad.py:440] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True\n",
            "I0629 20:06:13.606509 140493874636672 run_squad.py:442] input_ids: 2 37 47 289 278 11 3133 4521 45024 3 11 7583 19 45011 7583 45019 111 1738 36122 19 45020 140 45019 7583 11607 45020 833 45019 7583 3709 45012 53 11 126 74 16 11 524 108 15 930 108 1344 770 10026 179 170 17 6610 45015 18 526 16 204 45017 76 53 15969 48 37 3133 45011 45005 7583 45005 1823 37 45005 3133 767 45005 45012 32432 19 15 8682 37 1512 45015 4967 15 2229 74 45015 128 10026 179 1066 2513 99 45015 1773 17 29741 34650 44632 17 249 1093 1539 14 357 17643 45017 176 7263 14 10197 15 5522 33 11 1196 6962 137 15 525 45016 9765 2297 3513 45015 10026 179 5995 107 3132 15924 33 11 10684 31888 45016 261 2193 14 357 17643 45017 11 2003 858 15 1542 1697 14 11 7583 19 2054 1296 16 11 73 573 14 11 524 108 159 45015 15 39 617 17 8828 110 11 5883 28 1344 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.606904 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.607075 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.612672 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.612973 140493874636672 run_squad.py:432] unique_id: 1000000003\n",
            "I0629 20:06:13.613051 140493874636672 run_squad.py:433] example_index: 3\n",
            "I0629 20:06:13.613108 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.613246 140493874636672 run_squad.py:436] tokens: [CLS] who was the norse leader ? [SEP] the norman ##s ( norman : no ##ur ##mand ##s ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10 ##th and 11 ##th centuries gave thei ##r name to normandy , a region in france . they were descende ##d from norse ( \" norman \" comes from \" norse ##man \" ) raider ##s and pirates from denmark , iceland and norway who , under thei ##r leader roll ##o , agreed to swear fea ##lty to king charles iii of west francia . through generations of assimilation and mixing with the native frankis ##h and roman - gaul ##ish populations , thei ##r descendants would gradually merge with the carolin ##gian - based cultures of west francia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10 ##th century , and it continued to evolve over the succeed ##ing centuries . [SEP]\n",
            "I0629 20:06:13.613381 140493874636672 run_squad.py:438] token_to_orig_map: 8:0 9:1 10:1 11:2 12:2 13:2 14:3 15:3 16:3 17:3 18:3 19:4 20:4 21:5 22:5 23:5 24:6 25:6 26:7 27:7 28:7 29:8 30:9 31:10 32:11 33:12 34:13 35:14 36:14 37:15 38:16 39:16 40:17 41:18 42:19 43:19 44:20 45:21 46:22 47:22 48:23 49:24 50:25 51:26 52:26 53:27 54:28 55:29 56:29 57:30 58:31 59:32 60:32 61:32 62:32 63:33 64:34 65:35 66:35 67:35 68:35 69:35 70:36 71:36 72:37 73:38 74:39 75:40 76:40 77:41 78:42 79:43 80:44 81:44 82:45 83:46 84:46 85:47 86:48 87:48 88:48 89:49 90:50 91:51 92:52 93:52 94:53 95:54 96:55 97:56 98:57 99:58 100:59 101:59 102:60 103:61 104:62 105:63 106:64 107:65 108:66 109:67 110:68 111:69 112:69 113:70 114:71 115:71 116:71 117:71 118:72 119:72 120:73 121:73 122:74 123:75 124:76 125:77 126:78 127:79 128:80 129:80 130:80 131:80 132:81 133:82 134:83 135:84 136:84 137:85 138:86 139:87 140:88 141:89 142:90 143:91 144:92 145:93 146:93 147:94 148:95 149:96 150:97 151:98 152:99 153:100 154:101 155:102 156:102 157:103 158:103 159:104 160:105 161:106 162:107 163:108 164:109 165:110 166:111 167:111 168:112 169:112\n",
            "I0629 20:06:13.613514 140493874636672 run_squad.py:440] token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True\n",
            "I0629 20:06:13.613701 140493874636672 run_squad.py:442] input_ids: 2 74 26 11 3133 1066 45024 3 11 7583 19 45011 7583 45019 111 1738 36122 19 45020 140 45019 7583 11607 45020 833 45019 7583 3709 45012 53 11 126 74 16 11 524 108 15 930 108 1344 770 10026 179 170 17 6610 45015 18 526 16 204 45017 76 53 15969 48 37 3133 45011 45005 7583 45005 1823 37 45005 3133 767 45005 45012 32432 19 15 8682 37 1512 45015 4967 15 2229 74 45015 128 10026 179 1066 2513 99 45015 1773 17 29741 34650 44632 17 249 1093 1539 14 357 17643 45017 176 7263 14 10197 15 5522 33 11 1196 6962 137 15 525 45016 9765 2297 3513 45015 10026 179 5995 107 3132 15924 33 11 10684 31888 45016 261 2193 14 357 17643 45017 11 2003 858 15 1542 1697 14 11 7583 19 2054 1296 16 11 73 573 14 11 524 108 159 45015 15 39 617 17 8828 110 11 5883 28 1344 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.613852 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.613996 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.619489 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.619776 140493874636672 run_squad.py:432] unique_id: 1000000004\n",
            "I0629 20:06:13.619853 140493874636672 run_squad.py:433] example_index: 4\n",
            "I0629 20:06:13.619908 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.620048 140493874636672 run_squad.py:436] tokens: [CLS] what century did the norman ##s first gain thei ##r separate identity ? [SEP] the norman ##s ( norman : no ##ur ##mand ##s ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10 ##th and 11 ##th centuries gave thei ##r name to normandy , a region in france . they were descende ##d from norse ( \" norman \" comes from \" norse ##man \" ) raider ##s and pirates from denmark , iceland and norway who , under thei ##r leader roll ##o , agreed to swear fea ##lty to king charles iii of west francia . through generations of assimilation and mixing with the native frankis ##h and roman - gaul ##ish populations , thei ##r descendants would gradually merge with the carolin ##gian - based cultures of west francia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10 ##th century , and it continued to evolve over the succeed ##ing centuries . [SEP]\n",
            "I0629 20:06:13.620177 140493874636672 run_squad.py:438] token_to_orig_map: 15:0 16:1 17:1 18:2 19:2 20:2 21:3 22:3 23:3 24:3 25:3 26:4 27:4 28:5 29:5 30:5 31:6 32:6 33:7 34:7 35:7 36:8 37:9 38:10 39:11 40:12 41:13 42:14 43:14 44:15 45:16 46:16 47:17 48:18 49:19 50:19 51:20 52:21 53:22 54:22 55:23 56:24 57:25 58:26 59:26 60:27 61:28 62:29 63:29 64:30 65:31 66:32 67:32 68:32 69:32 70:33 71:34 72:35 73:35 74:35 75:35 76:35 77:36 78:36 79:37 80:38 81:39 82:40 83:40 84:41 85:42 86:43 87:44 88:44 89:45 90:46 91:46 92:47 93:48 94:48 95:48 96:49 97:50 98:51 99:52 100:52 101:53 102:54 103:55 104:56 105:57 106:58 107:59 108:59 109:60 110:61 111:62 112:63 113:64 114:65 115:66 116:67 117:68 118:69 119:69 120:70 121:71 122:71 123:71 124:71 125:72 126:72 127:73 128:73 129:74 130:75 131:76 132:77 133:78 134:79 135:80 136:80 137:80 138:80 139:81 140:82 141:83 142:84 143:84 144:85 145:86 146:87 147:88 148:89 149:90 150:91 151:92 152:93 153:93 154:94 155:95 156:96 157:97 158:98 159:99 160:100 161:101 162:102 163:102 164:103 165:103 166:104 167:105 168:106 169:107 170:108 171:109 172:110 173:111 174:111 175:112 176:112\n",
            "I0629 20:06:13.620300 140493874636672 run_squad.py:440] token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True\n",
            "I0629 20:06:13.620455 140493874636672 run_squad.py:442] input_ids: 2 305 159 278 11 7583 19 73 748 10026 179 1071 1697 45024 3 11 7583 19 45011 7583 45019 111 1738 36122 19 45020 140 45019 7583 11607 45020 833 45019 7583 3709 45012 53 11 126 74 16 11 524 108 15 930 108 1344 770 10026 179 170 17 6610 45015 18 526 16 204 45017 76 53 15969 48 37 3133 45011 45005 7583 45005 1823 37 45005 3133 767 45005 45012 32432 19 15 8682 37 1512 45015 4967 15 2229 74 45015 128 10026 179 1066 2513 99 45015 1773 17 29741 34650 44632 17 249 1093 1539 14 357 17643 45017 176 7263 14 10197 15 5522 33 11 1196 6962 137 15 525 45016 9765 2297 3513 45015 10026 179 5995 107 3132 15924 33 11 10684 31888 45016 261 2193 14 357 17643 45017 11 2003 858 15 1542 1697 14 11 7583 19 2054 1296 16 11 73 573 14 11 524 108 159 45015 15 39 617 17 8828 110 11 5883 28 1344 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.620639 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.620788 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.626206 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.626470 140493874636672 run_squad.py:432] unique_id: 1000000005\n",
            "I0629 20:06:13.626567 140493874636672 run_squad.py:433] example_index: 5\n",
            "I0629 20:06:13.626632 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.626772 140493874636672 run_squad.py:436] tokens: [CLS] who gave thei ##r name to normandy in the 1000 ' s and 110 ##0 ' s [SEP] the norman ##s ( norman : no ##ur ##mand ##s ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10 ##th and 11 ##th centuries gave thei ##r name to normandy , a region in france . they were descende ##d from norse ( \" norman \" comes from \" norse ##man \" ) raider ##s and pirates from denmark , iceland and norway who , under thei ##r leader roll ##o , agreed to swear fea ##lty to king charles iii of west francia . through generations of assimilation and mixing with the native frankis ##h and roman - gaul ##ish populations , thei ##r descendants would gradually merge with the carolin ##gian - based cultures of west francia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10 ##th century , and it continued to evolve over the succeed ##ing centuries . [SEP]\n",
            "I0629 20:06:13.626894 140493874636672 run_squad.py:438] token_to_orig_map: 19:0 20:1 21:1 22:2 23:2 24:2 25:3 26:3 27:3 28:3 29:3 30:4 31:4 32:5 33:5 34:5 35:6 36:6 37:7 38:7 39:7 40:8 41:9 42:10 43:11 44:12 45:13 46:14 47:14 48:15 49:16 50:16 51:17 52:18 53:19 54:19 55:20 56:21 57:22 58:22 59:23 60:24 61:25 62:26 63:26 64:27 65:28 66:29 67:29 68:30 69:31 70:32 71:32 72:32 73:32 74:33 75:34 76:35 77:35 78:35 79:35 80:35 81:36 82:36 83:37 84:38 85:39 86:40 87:40 88:41 89:42 90:43 91:44 92:44 93:45 94:46 95:46 96:47 97:48 98:48 99:48 100:49 101:50 102:51 103:52 104:52 105:53 106:54 107:55 108:56 109:57 110:58 111:59 112:59 113:60 114:61 115:62 116:63 117:64 118:65 119:66 120:67 121:68 122:69 123:69 124:70 125:71 126:71 127:71 128:71 129:72 130:72 131:73 132:73 133:74 134:75 135:76 136:77 137:78 138:79 139:80 140:80 141:80 142:80 143:81 144:82 145:83 146:84 147:84 148:85 149:86 150:87 151:88 152:89 153:90 154:91 155:92 156:93 157:93 158:94 159:95 160:96 161:97 162:98 163:99 164:100 165:101 166:102 167:102 168:103 169:103 170:104 171:105 172:106 173:107 174:108 175:109 176:110 177:111 178:111 179:112 180:112\n",
            "I0629 20:06:13.709047 140493874636672 run_squad.py:440] token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True\n",
            "I0629 20:06:13.709430 140493874636672 run_squad.py:442] input_ids: 2 74 770 10026 179 170 17 6610 16 11 4303 45010 294 15 6770 1657 45010 294 3 11 7583 19 45011 7583 45019 111 1738 36122 19 45020 140 45019 7583 11607 45020 833 45019 7583 3709 45012 53 11 126 74 16 11 524 108 15 930 108 1344 770 10026 179 170 17 6610 45015 18 526 16 204 45017 76 53 15969 48 37 3133 45011 45005 7583 45005 1823 37 45005 3133 767 45005 45012 32432 19 15 8682 37 1512 45015 4967 15 2229 74 45015 128 10026 179 1066 2513 99 45015 1773 17 29741 34650 44632 17 249 1093 1539 14 357 17643 45017 176 7263 14 10197 15 5522 33 11 1196 6962 137 15 525 45016 9765 2297 3513 45015 10026 179 5995 107 3132 15924 33 11 10684 31888 45016 261 2193 14 357 17643 45017 11 2003 858 15 1542 1697 14 11 7583 19 2054 1296 16 11 73 573 14 11 524 108 159 45015 15 39 617 17 8828 110 11 5883 28 1344 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.709643 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.709800 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.715298 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.715609 140493874636672 run_squad.py:432] unique_id: 1000000006\n",
            "I0629 20:06:13.715690 140493874636672 run_squad.py:433] example_index: 6\n",
            "I0629 20:06:13.715741 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.715878 140493874636672 run_squad.py:436] tokens: [CLS] what is france a region of ? [SEP] the norman ##s ( norman : no ##ur ##mand ##s ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10 ##th and 11 ##th centuries gave thei ##r name to normandy , a region in france . they were descende ##d from norse ( \" norman \" comes from \" norse ##man \" ) raider ##s and pirates from denmark , iceland and norway who , under thei ##r leader roll ##o , agreed to swear fea ##lty to king charles iii of west francia . through generations of assimilation and mixing with the native frankis ##h and roman - gaul ##ish populations , thei ##r descendants would gradually merge with the carolin ##gian - based cultures of west francia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10 ##th century , and it continued to evolve over the succeed ##ing centuries . [SEP]\n",
            "I0629 20:06:13.716014 140493874636672 run_squad.py:438] token_to_orig_map: 9:0 10:1 11:1 12:2 13:2 14:2 15:3 16:3 17:3 18:3 19:3 20:4 21:4 22:5 23:5 24:5 25:6 26:6 27:7 28:7 29:7 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:14 38:15 39:16 40:16 41:17 42:18 43:19 44:19 45:20 46:21 47:22 48:22 49:23 50:24 51:25 52:26 53:26 54:27 55:28 56:29 57:29 58:30 59:31 60:32 61:32 62:32 63:32 64:33 65:34 66:35 67:35 68:35 69:35 70:35 71:36 72:36 73:37 74:38 75:39 76:40 77:40 78:41 79:42 80:43 81:44 82:44 83:45 84:46 85:46 86:47 87:48 88:48 89:48 90:49 91:50 92:51 93:52 94:52 95:53 96:54 97:55 98:56 99:57 100:58 101:59 102:59 103:60 104:61 105:62 106:63 107:64 108:65 109:66 110:67 111:68 112:69 113:69 114:70 115:71 116:71 117:71 118:71 119:72 120:72 121:73 122:73 123:74 124:75 125:76 126:77 127:78 128:79 129:80 130:80 131:80 132:80 133:81 134:82 135:83 136:84 137:84 138:85 139:86 140:87 141:88 142:89 143:90 144:91 145:92 146:93 147:93 148:94 149:95 150:96 151:97 152:98 153:99 154:100 155:101 156:102 157:102 158:103 159:103 160:104 161:105 162:106 163:107 164:108 165:109 166:110 167:111 168:111 169:112 170:112\n",
            "I0629 20:06:13.716129 140493874636672 run_squad.py:440] token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True\n",
            "I0629 20:06:13.716284 140493874636672 run_squad.py:442] input_ids: 2 305 21 204 18 526 14 45024 3 11 7583 19 45011 7583 45019 111 1738 36122 19 45020 140 45019 7583 11607 45020 833 45019 7583 3709 45012 53 11 126 74 16 11 524 108 15 930 108 1344 770 10026 179 170 17 6610 45015 18 526 16 204 45017 76 53 15969 48 37 3133 45011 45005 7583 45005 1823 37 45005 3133 767 45005 45012 32432 19 15 8682 37 1512 45015 4967 15 2229 74 45015 128 10026 179 1066 2513 99 45015 1773 17 29741 34650 44632 17 249 1093 1539 14 357 17643 45017 176 7263 14 10197 15 5522 33 11 1196 6962 137 15 525 45016 9765 2297 3513 45015 10026 179 5995 107 3132 15924 33 11 10684 31888 45016 261 2193 14 357 17643 45017 11 2003 858 15 1542 1697 14 11 7583 19 2054 1296 16 11 73 573 14 11 524 108 159 45015 15 39 617 17 8828 110 11 5883 28 1344 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.716425 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.716595 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.722074 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.722353 140493874636672 run_squad.py:432] unique_id: 1000000007\n",
            "I0629 20:06:13.722440 140493874636672 run_squad.py:433] example_index: 7\n",
            "I0629 20:06:13.722509 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.722684 140493874636672 run_squad.py:436] tokens: [CLS] who did king charles iii swear fea ##lty to ? [SEP] the norman ##s ( norman : no ##ur ##mand ##s ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10 ##th and 11 ##th centuries gave thei ##r name to normandy , a region in france . they were descende ##d from norse ( \" norman \" comes from \" norse ##man \" ) raider ##s and pirates from denmark , iceland and norway who , under thei ##r leader roll ##o , agreed to swear fea ##lty to king charles iii of west francia . through generations of assimilation and mixing with the native frankis ##h and roman - gaul ##ish populations , thei ##r descendants would gradually merge with the carolin ##gian - based cultures of west francia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10 ##th century , and it continued to evolve over the succeed ##ing centuries . [SEP]\n",
            "I0629 20:06:13.722825 140493874636672 run_squad.py:438] token_to_orig_map: 12:0 13:1 14:1 15:2 16:2 17:2 18:3 19:3 20:3 21:3 22:3 23:4 24:4 25:5 26:5 27:5 28:6 29:6 30:7 31:7 32:7 33:8 34:9 35:10 36:11 37:12 38:13 39:14 40:14 41:15 42:16 43:16 44:17 45:18 46:19 47:19 48:20 49:21 50:22 51:22 52:23 53:24 54:25 55:26 56:26 57:27 58:28 59:29 60:29 61:30 62:31 63:32 64:32 65:32 66:32 67:33 68:34 69:35 70:35 71:35 72:35 73:35 74:36 75:36 76:37 77:38 78:39 79:40 80:40 81:41 82:42 83:43 84:44 85:44 86:45 87:46 88:46 89:47 90:48 91:48 92:48 93:49 94:50 95:51 96:52 97:52 98:53 99:54 100:55 101:56 102:57 103:58 104:59 105:59 106:60 107:61 108:62 109:63 110:64 111:65 112:66 113:67 114:68 115:69 116:69 117:70 118:71 119:71 120:71 121:71 122:72 123:72 124:73 125:73 126:74 127:75 128:76 129:77 130:78 131:79 132:80 133:80 134:80 135:80 136:81 137:82 138:83 139:84 140:84 141:85 142:86 143:87 144:88 145:89 146:90 147:91 148:92 149:93 150:93 151:94 152:95 153:96 154:97 155:98 156:99 157:100 158:101 159:102 160:102 161:103 162:103 163:104 164:105 165:106 166:107 167:108 168:109 169:110 170:111 171:111 172:112 173:112\n",
            "I0629 20:06:13.722949 140493874636672 run_squad.py:440] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True\n",
            "I0629 20:06:13.723120 140493874636672 run_squad.py:442] input_ids: 2 74 278 249 1093 1539 29741 34650 44632 17 45024 3 11 7583 19 45011 7583 45019 111 1738 36122 19 45020 140 45019 7583 11607 45020 833 45019 7583 3709 45012 53 11 126 74 16 11 524 108 15 930 108 1344 770 10026 179 170 17 6610 45015 18 526 16 204 45017 76 53 15969 48 37 3133 45011 45005 7583 45005 1823 37 45005 3133 767 45005 45012 32432 19 15 8682 37 1512 45015 4967 15 2229 74 45015 128 10026 179 1066 2513 99 45015 1773 17 29741 34650 44632 17 249 1093 1539 14 357 17643 45017 176 7263 14 10197 15 5522 33 11 1196 6962 137 15 525 45016 9765 2297 3513 45015 10026 179 5995 107 3132 15924 33 11 10684 31888 45016 261 2193 14 357 17643 45017 11 2003 858 15 1542 1697 14 11 7583 19 2054 1296 16 11 73 573 14 11 524 108 159 45015 15 39 617 17 8828 110 11 5883 28 1344 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.723276 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.723414 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.728820 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.729106 140493874636672 run_squad.py:432] unique_id: 1000000008\n",
            "I0629 20:06:13.729185 140493874636672 run_squad.py:433] example_index: 8\n",
            "I0629 20:06:13.729235 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.729372 140493874636672 run_squad.py:436] tokens: [CLS] when did the frankis ##h identity emerge ? [SEP] the norman ##s ( norman : no ##ur ##mand ##s ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10 ##th and 11 ##th centuries gave thei ##r name to normandy , a region in france . they were descende ##d from norse ( \" norman \" comes from \" norse ##man \" ) raider ##s and pirates from denmark , iceland and norway who , under thei ##r leader roll ##o , agreed to swear fea ##lty to king charles iii of west francia . through generations of assimilation and mixing with the native frankis ##h and roman - gaul ##ish populations , thei ##r descendants would gradually merge with the carolin ##gian - based cultures of west francia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10 ##th century , and it continued to evolve over the succeed ##ing centuries . [SEP]\n",
            "I0629 20:06:13.729491 140493874636672 run_squad.py:438] token_to_orig_map: 10:0 11:1 12:1 13:2 14:2 15:2 16:3 17:3 18:3 19:3 20:3 21:4 22:4 23:5 24:5 25:5 26:6 27:6 28:7 29:7 30:7 31:8 32:9 33:10 34:11 35:12 36:13 37:14 38:14 39:15 40:16 41:16 42:17 43:18 44:19 45:19 46:20 47:21 48:22 49:22 50:23 51:24 52:25 53:26 54:26 55:27 56:28 57:29 58:29 59:30 60:31 61:32 62:32 63:32 64:32 65:33 66:34 67:35 68:35 69:35 70:35 71:35 72:36 73:36 74:37 75:38 76:39 77:40 78:40 79:41 80:42 81:43 82:44 83:44 84:45 85:46 86:46 87:47 88:48 89:48 90:48 91:49 92:50 93:51 94:52 95:52 96:53 97:54 98:55 99:56 100:57 101:58 102:59 103:59 104:60 105:61 106:62 107:63 108:64 109:65 110:66 111:67 112:68 113:69 114:69 115:70 116:71 117:71 118:71 119:71 120:72 121:72 122:73 123:73 124:74 125:75 126:76 127:77 128:78 129:79 130:80 131:80 132:80 133:80 134:81 135:82 136:83 137:84 138:84 139:85 140:86 141:87 142:88 143:89 144:90 145:91 146:92 147:93 148:93 149:94 150:95 151:96 152:97 153:98 154:99 155:100 156:101 157:102 158:102 159:103 160:103 161:104 162:105 163:106 164:107 165:108 166:109 167:110 168:111 169:111 170:112 171:112\n",
            "I0629 20:06:13.812434 140493874636672 run_squad.py:440] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True\n",
            "I0629 20:06:13.812799 140493874636672 run_squad.py:442] input_ids: 2 94 278 11 6962 137 1697 8339 45024 3 11 7583 19 45011 7583 45019 111 1738 36122 19 45020 140 45019 7583 11607 45020 833 45019 7583 3709 45012 53 11 126 74 16 11 524 108 15 930 108 1344 770 10026 179 170 17 6610 45015 18 526 16 204 45017 76 53 15969 48 37 3133 45011 45005 7583 45005 1823 37 45005 3133 767 45005 45012 32432 19 15 8682 37 1512 45015 4967 15 2229 74 45015 128 10026 179 1066 2513 99 45015 1773 17 29741 34650 44632 17 249 1093 1539 14 357 17643 45017 176 7263 14 10197 15 5522 33 11 1196 6962 137 15 525 45016 9765 2297 3513 45015 10026 179 5995 107 3132 15924 33 11 10684 31888 45016 261 2193 14 357 17643 45017 11 2003 858 15 1542 1697 14 11 7583 19 2054 1296 16 11 73 573 14 11 524 108 159 45015 15 39 617 17 8828 110 11 5883 28 1344 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.812958 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.813107 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.823507 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.823817 140493874636672 run_squad.py:432] unique_id: 1000000009\n",
            "I0629 20:06:13.823891 140493874636672 run_squad.py:433] example_index: 9\n",
            "I0629 20:06:13.823946 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.824132 140493874636672 run_squad.py:436] tokens: [CLS] who was the duke in the battle of hasti ##ngs ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were fame ##d for thei ##r martial spirit and eventually for thei ##r christian piety , becoming exponent ##s of the catholic orthodoxy into which they assimilated . they adopted the gallo - romance language of the frankis ##h land they settled , thei ##r dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a cohesive and form ##ida ##ble principality in feudal tenure . the norman ##s are noted both for thei ##r culture , such as thei ##r unique romanesque architecture and musical traditions , and for thei ##r significant military accomplishments and innovations . norman adventurers founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the saracens and byzantine ##s , and an expedition on behalf of thei ##r duke , william the conqueror , led to the norman conquest of england at the battle of hasti ##ngs in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where thei ##r prince bo ##hem ##ond i founded the principality of antioch in the lev ##ant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\n",
            "I0629 20:06:13.824301 140493874636672 run_squad.py:438] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:6 21:7 22:8 23:9 24:10 25:11 26:12 27:13 28:14 29:15 30:16 31:17 32:18 33:18 34:19 35:20 36:20 37:21 38:22 39:22 40:23 41:24 42:24 43:25 44:26 45:27 46:28 47:29 48:30 49:30 50:31 51:32 52:32 53:33 54:34 55:34 56:35 57:36 58:37 59:38 60:39 61:40 62:41 63:42 64:42 65:43 66:44 67:45 68:46 69:46 70:46 71:47 72:48 73:49 74:50 75:50 76:51 77:52 78:53 79:53 80:54 81:54 82:55 83:56 84:57 85:58 86:59 87:59 88:60 89:60 90:61 91:62 92:63 93:63 94:64 95:65 96:66 97:67 98:67 99:68 100:69 101:70 102:71 103:71 104:72 105:73 106:74 107:75 108:76 109:77 110:78 111:79 112:80 113:80 114:81 115:82 116:83 117:84 118:84 119:85 120:86 121:87 122:87 123:88 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:102 140:103 141:104 142:104 143:105 144:106 145:106 146:107 147:108 148:109 149:110 150:111 151:111 152:112 153:112 154:113 155:114 156:115 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:121 165:122 166:123 167:124 168:124 169:125 170:126 171:127 172:128 173:129 174:129 175:130 176:131 177:132 178:133 179:134 180:135 181:136 182:137 183:138 184:139 185:140 186:141 187:141 188:142 189:143 190:144 191:145 192:146 193:147 194:148 195:148 196:148 197:149 198:150 199:151 200:152 201:153 202:154 203:155 204:155 205:156 206:156 207:157 208:158 209:159 210:159 211:160 212:161 213:162 214:163 215:164 216:165 217:166 218:167 219:168 220:169 221:170 222:171 223:171 224:172 225:173 226:173 227:173 228:174 229:175 230:176 231:177 232:178 233:179 234:180 235:181 236:182 237:183 238:184 239:185 240:186 241:187 242:188 243:189 244:190 245:191 246:192 247:192 248:193 249:194 250:194 251:195 252:196 253:196 254:196 255:197 256:198 257:199 258:200 259:201 260:202 261:203 262:204 263:205 264:205 265:205 266:206 267:207 268:208 269:209 270:210 271:211 272:212 273:212 274:213 275:214 276:214 277:215 278:216 279:217 280:218 281:219 282:220 283:221 284:222 285:223 286:224 287:225 288:225\n",
            "I0629 20:06:13.824469 140493874636672 run_squad.py:440] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True\n",
            "I0629 20:06:13.824699 140493874636672 run_squad.py:442] input_ids: 2 74 26 11 2041 16 11 1206 14 24482 43561 45024 3 11 7583 630 61 18 219 293 45015 858 15 213 1815 34 2223 414 15 220 11 696 377 45017 11 7583 19 53 1828 48 27 10026 179 14831 1795 15 728 27 10026 179 465 29183 45015 1652 11207 19 14 11 1201 10598 96 47 76 12604 45017 76 1676 11 34772 45016 8219 190 14 11 6962 137 341 76 1935 45015 10026 179 3017 1652 127 24 7583 45015 35078 32028 43 7583 140 45015 41 325 2052 190 45017 11 6998 14 6610 45015 47 76 663 31 2050 33 11 140 2934 45015 26 18 228 3733 18936 14 2223 204 45015 15 128 981 135 14 6610 26 13256 96 18 39850 15 182 9935 2463 17541 16 11678 8829 45017 11 7583 19 36 1460 138 27 10026 179 394 45015 70 24 10026 179 1645 21190 1985 15 1098 2216 45015 15 27 10026 179 677 213 18019 15 11841 45017 7583 22346 885 11 536 14 14518 128 4744 309 85 12026 28 487 1530 34 11 27728 15 6209 19 45015 15 41 2744 34 4132 14 10026 179 2041 45015 553 11 17894 45015 329 17 11 7583 3048 14 423 50 11 1206 14 24482 43561 16 13961 1572 45017 7583 858 15 213 340 2237 37 91 88 180 5046 17 11 11891 151 14 11 696 377 45015 118 10026 179 2508 2992 13983 34446 135 885 11 17541 14 4279 16 11 19493 2360 45015 17 2829 15 3432 16 228 1048 45015 17 2521 45015 15 17 11 8272 14 276 1282 15 11 966 100 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.824854 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.824993 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.833722 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.833976 140493874636672 run_squad.py:432] unique_id: 1000000010\n",
            "I0629 20:06:13.834044 140493874636672 run_squad.py:433] example_index: 10\n",
            "I0629 20:06:13.834100 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.834270 140493874636672 run_squad.py:436] tokens: [CLS] who rule ##d the duchy of normandy [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were fame ##d for thei ##r martial spirit and eventually for thei ##r christian piety , becoming exponent ##s of the catholic orthodoxy into which they assimilated . they adopted the gallo - romance language of the frankis ##h land they settled , thei ##r dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a cohesive and form ##ida ##ble principality in feudal tenure . the norman ##s are noted both for thei ##r culture , such as thei ##r unique romanesque architecture and musical traditions , and for thei ##r significant military accomplishments and innovations . norman adventurers founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the saracens and byzantine ##s , and an expedition on behalf of thei ##r duke , william the conqueror , led to the norman conquest of england at the battle of hasti ##ngs in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where thei ##r prince bo ##hem ##ond i founded the principality of antioch in the lev ##ant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\n",
            "I0629 20:06:13.834433 140493874636672 run_squad.py:438] token_to_orig_map: 9:0 10:1 11:2 12:3 13:4 14:5 15:6 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:18 29:18 30:19 31:20 32:20 33:21 34:22 35:22 36:23 37:24 38:24 39:25 40:26 41:27 42:28 43:29 44:30 45:30 46:31 47:32 48:32 49:33 50:34 51:34 52:35 53:36 54:37 55:38 56:39 57:40 58:41 59:42 60:42 61:43 62:44 63:45 64:46 65:46 66:46 67:47 68:48 69:49 70:50 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:57 81:58 82:59 83:59 84:60 85:60 86:61 87:62 88:63 89:63 90:64 91:65 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:71 100:72 101:73 102:74 103:75 104:76 105:77 106:78 107:79 108:80 109:80 110:81 111:82 112:83 113:84 114:84 115:85 116:86 117:87 118:87 119:88 120:89 121:90 122:91 123:92 124:93 125:94 126:95 127:96 128:97 129:98 130:99 131:100 132:100 133:100 134:101 135:102 136:103 137:104 138:104 139:105 140:106 141:106 142:107 143:108 144:109 145:110 146:111 147:111 148:112 149:112 150:113 151:114 152:115 153:115 154:116 155:117 156:118 157:119 158:120 159:121 160:121 161:122 162:123 163:124 164:124 165:125 166:126 167:127 168:128 169:129 170:129 171:130 172:131 173:132 174:133 175:134 176:135 177:136 178:137 179:138 180:139 181:140 182:141 183:141 184:142 185:143 186:144 187:145 188:146 189:147 190:148 191:148 192:148 193:149 194:150 195:151 196:152 197:153 198:154 199:155 200:155 201:156 202:156 203:157 204:158 205:159 206:159 207:160 208:161 209:162 210:163 211:164 212:165 213:166 214:167 215:168 216:169 217:170 218:171 219:171 220:172 221:173 222:173 223:173 224:174 225:175 226:176 227:177 228:178 229:179 230:180 231:181 232:182 233:183 234:184 235:185 236:186 237:187 238:188 239:189 240:190 241:191 242:192 243:192 244:193 245:194 246:194 247:195 248:196 249:196 250:196 251:197 252:198 253:199 254:200 255:201 256:202 257:203 258:204 259:205 260:205 261:205 262:206 263:207 264:208 265:209 266:210 267:211 268:212 269:212 270:213 271:214 272:214 273:215 274:216 275:217 276:218 277:219 278:220 279:221 280:222 281:223 282:224 283:225 284:225\n",
            "I0629 20:06:13.834636 140493874636672 run_squad.py:440] token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True\n",
            "I0629 20:06:13.915817 140493874636672 run_squad.py:442] input_ids: 2 74 499 48 11 6998 14 6610 3 11 7583 630 61 18 219 293 45015 858 15 213 1815 34 2223 414 15 220 11 696 377 45017 11 7583 19 53 1828 48 27 10026 179 14831 1795 15 728 27 10026 179 465 29183 45015 1652 11207 19 14 11 1201 10598 96 47 76 12604 45017 76 1676 11 34772 45016 8219 190 14 11 6962 137 341 76 1935 45015 10026 179 3017 1652 127 24 7583 45015 35078 32028 43 7583 140 45015 41 325 2052 190 45017 11 6998 14 6610 45015 47 76 663 31 2050 33 11 140 2934 45015 26 18 228 3733 18936 14 2223 204 45015 15 128 981 135 14 6610 26 13256 96 18 39850 15 182 9935 2463 17541 16 11678 8829 45017 11 7583 19 36 1460 138 27 10026 179 394 45015 70 24 10026 179 1645 21190 1985 15 1098 2216 45015 15 27 10026 179 677 213 18019 15 11841 45017 7583 22346 885 11 536 14 14518 128 4744 309 85 12026 28 487 1530 34 11 27728 15 6209 19 45015 15 41 2744 34 4132 14 10026 179 2041 45015 553 11 17894 45015 329 17 11 7583 3048 14 423 50 11 1206 14 24482 43561 16 13961 1572 45017 7583 858 15 213 340 2237 37 91 88 180 5046 17 11 11891 151 14 11 696 377 45015 118 10026 179 2508 2992 13983 34446 135 885 11 17541 14 4279 16 11 19493 2360 45015 17 2829 15 3432 16 228 1048 45015 17 2521 45015 15 17 11 8272 14 276 1282 15 11 966 100 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.916380 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.916630 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.926681 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.926937 140493874636672 run_squad.py:432] unique_id: 1000000011\n",
            "I0629 20:06:13.927017 140493874636672 run_squad.py:433] example_index: 11\n",
            "I0629 20:06:13.927093 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.927277 140493874636672 run_squad.py:436] tokens: [CLS] what religion were the norman ##s [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were fame ##d for thei ##r martial spirit and eventually for thei ##r christian piety , becoming exponent ##s of the catholic orthodoxy into which they assimilated . they adopted the gallo - romance language of the frankis ##h land they settled , thei ##r dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a cohesive and form ##ida ##ble principality in feudal tenure . the norman ##s are noted both for thei ##r culture , such as thei ##r unique romanesque architecture and musical traditions , and for thei ##r significant military accomplishments and innovations . norman adventurers founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the saracens and byzantine ##s , and an expedition on behalf of thei ##r duke , william the conqueror , led to the norman conquest of england at the battle of hasti ##ngs in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where thei ##r prince bo ##hem ##ond i founded the principality of antioch in the lev ##ant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\n",
            "I0629 20:06:13.927458 140493874636672 run_squad.py:438] token_to_orig_map: 8:0 9:1 10:2 11:3 12:4 13:5 14:6 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:13 23:14 24:15 25:16 26:17 27:18 28:18 29:19 30:20 31:20 32:21 33:22 34:22 35:23 36:24 37:24 38:25 39:26 40:27 41:28 42:29 43:30 44:30 45:31 46:32 47:32 48:33 49:34 50:34 51:35 52:36 53:37 54:38 55:39 56:40 57:41 58:42 59:42 60:43 61:44 62:45 63:46 64:46 65:46 66:47 67:48 68:49 69:50 70:50 71:51 72:52 73:53 74:53 75:54 76:54 77:55 78:56 79:57 80:58 81:59 82:59 83:60 84:60 85:61 86:62 87:63 88:63 89:64 90:65 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:71 99:72 100:73 101:74 102:75 103:76 104:77 105:78 106:79 107:80 108:80 109:81 110:82 111:83 112:84 113:84 114:85 115:86 116:87 117:87 118:88 119:89 120:90 121:91 122:92 123:93 124:94 125:95 126:96 127:97 128:98 129:99 130:100 131:100 132:100 133:101 134:102 135:103 136:104 137:104 138:105 139:106 140:106 141:107 142:108 143:109 144:110 145:111 146:111 147:112 148:112 149:113 150:114 151:115 152:115 153:116 154:117 155:118 156:119 157:120 158:121 159:121 160:122 161:123 162:124 163:124 164:125 165:126 166:127 167:128 168:129 169:129 170:130 171:131 172:132 173:133 174:134 175:135 176:136 177:137 178:138 179:139 180:140 181:141 182:141 183:142 184:143 185:144 186:145 187:146 188:147 189:148 190:148 191:148 192:149 193:150 194:151 195:152 196:153 197:154 198:155 199:155 200:156 201:156 202:157 203:158 204:159 205:159 206:160 207:161 208:162 209:163 210:164 211:165 212:166 213:167 214:168 215:169 216:170 217:171 218:171 219:172 220:173 221:173 222:173 223:174 224:175 225:176 226:177 227:178 228:179 229:180 230:181 231:182 232:183 233:184 234:185 235:186 236:187 237:188 238:189 239:190 240:191 241:192 242:192 243:193 244:194 245:194 246:195 247:196 248:196 249:196 250:197 251:198 252:199 253:200 254:201 255:202 256:203 257:204 258:205 259:205 260:205 261:206 262:207 263:208 264:209 265:210 266:211 267:212 268:212 269:213 270:214 271:214 272:215 273:216 274:217 275:218 276:219 277:220 278:221 279:222 280:223 281:224 282:225 283:225\n",
            "I0629 20:06:13.927651 140493874636672 run_squad.py:440] token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True\n",
            "I0629 20:06:13.927847 140493874636672 run_squad.py:442] input_ids: 2 305 2202 53 11 7583 19 3 11 7583 630 61 18 219 293 45015 858 15 213 1815 34 2223 414 15 220 11 696 377 45017 11 7583 19 53 1828 48 27 10026 179 14831 1795 15 728 27 10026 179 465 29183 45015 1652 11207 19 14 11 1201 10598 96 47 76 12604 45017 76 1676 11 34772 45016 8219 190 14 11 6962 137 341 76 1935 45015 10026 179 3017 1652 127 24 7583 45015 35078 32028 43 7583 140 45015 41 325 2052 190 45017 11 6998 14 6610 45015 47 76 663 31 2050 33 11 140 2934 45015 26 18 228 3733 18936 14 2223 204 45015 15 128 981 135 14 6610 26 13256 96 18 39850 15 182 9935 2463 17541 16 11678 8829 45017 11 7583 19 36 1460 138 27 10026 179 394 45015 70 24 10026 179 1645 21190 1985 15 1098 2216 45015 15 27 10026 179 677 213 18019 15 11841 45017 7583 22346 885 11 536 14 14518 128 4744 309 85 12026 28 487 1530 34 11 27728 15 6209 19 45015 15 41 2744 34 4132 14 10026 179 2041 45015 553 11 17894 45015 329 17 11 7583 3048 14 423 50 11 1206 14 24482 43561 16 13961 1572 45017 7583 858 15 213 340 2237 37 91 88 180 5046 17 11 11891 151 14 11 696 377 45015 118 10026 179 2508 2992 13983 34446 135 885 11 17541 14 4279 16 11 19493 2360 45015 17 2829 15 3432 16 228 1048 45015 17 2521 45015 15 17 11 8272 14 276 1282 15 11 966 100 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.928006 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.928161 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:13.938236 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:13.938510 140493874636672 run_squad.py:432] unique_id: 1000000012\n",
            "I0629 20:06:13.938603 140493874636672 run_squad.py:433] example_index: 12\n",
            "I0629 20:06:13.938666 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:13.938935 140493874636672 run_squad.py:436] tokens: [CLS] what type of major impact did the norman dynasty have on modern europe ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were fame ##d for thei ##r martial spirit and eventually for thei ##r christian piety , becoming exponent ##s of the catholic orthodoxy into which they assimilated . they adopted the gallo - romance language of the frankis ##h land they settled , thei ##r dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a cohesive and form ##ida ##ble principality in feudal tenure . the norman ##s are noted both for thei ##r culture , such as thei ##r unique romanesque architecture and musical traditions , and for thei ##r significant military accomplishments and innovations . norman adventurers founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the saracens and byzantine ##s , and an expedition on behalf of thei ##r duke , william the conqueror , led to the norman conquest of england at the battle of hasti ##ngs in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where thei ##r prince bo ##hem ##ond i founded the principality of antioch in the lev ##ant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\n",
            "I0629 20:06:13.939141 140493874636672 run_squad.py:438] token_to_orig_map: 16:0 17:1 18:2 19:3 20:4 21:5 22:6 23:6 24:7 25:8 26:9 27:10 28:11 29:12 30:13 31:14 32:15 33:16 34:17 35:18 36:18 37:19 38:20 39:20 40:21 41:22 42:22 43:23 44:24 45:24 46:25 47:26 48:27 49:28 50:29 51:30 52:30 53:31 54:32 55:32 56:33 57:34 58:34 59:35 60:36 61:37 62:38 63:39 64:40 65:41 66:42 67:42 68:43 69:44 70:45 71:46 72:46 73:46 74:47 75:48 76:49 77:50 78:50 79:51 80:52 81:53 82:53 83:54 84:54 85:55 86:56 87:57 88:58 89:59 90:59 91:60 92:60 93:61 94:62 95:63 96:63 97:64 98:65 99:66 100:67 101:67 102:68 103:69 104:70 105:71 106:71 107:72 108:73 109:74 110:75 111:76 112:77 113:78 114:79 115:80 116:80 117:81 118:82 119:83 120:84 121:84 122:85 123:86 124:87 125:87 126:88 127:89 128:90 129:91 130:92 131:93 132:94 133:95 134:96 135:97 136:98 137:99 138:100 139:100 140:100 141:101 142:102 143:103 144:104 145:104 146:105 147:106 148:106 149:107 150:108 151:109 152:110 153:111 154:111 155:112 156:112 157:113 158:114 159:115 160:115 161:116 162:117 163:118 164:119 165:120 166:121 167:121 168:122 169:123 170:124 171:124 172:125 173:126 174:127 175:128 176:129 177:129 178:130 179:131 180:132 181:133 182:134 183:135 184:136 185:137 186:138 187:139 188:140 189:141 190:141 191:142 192:143 193:144 194:145 195:146 196:147 197:148 198:148 199:148 200:149 201:150 202:151 203:152 204:153 205:154 206:155 207:155 208:156 209:156 210:157 211:158 212:159 213:159 214:160 215:161 216:162 217:163 218:164 219:165 220:166 221:167 222:168 223:169 224:170 225:171 226:171 227:172 228:173 229:173 230:173 231:174 232:175 233:176 234:177 235:178 236:179 237:180 238:181 239:182 240:183 241:184 242:185 243:186 244:187 245:188 246:189 247:190 248:191 249:192 250:192 251:193 252:194 253:194 254:195 255:196 256:196 257:196 258:197 259:198 260:199 261:200 262:201 263:202 264:203 265:204 266:205 267:205 268:205 269:206 270:207 271:208 272:209 273:210 274:211 275:212 276:212 277:213 278:214 279:214 280:215 281:216 282:217 283:218 284:219 285:220 286:221 287:222 288:223 289:224 290:225 291:225\n",
            "I0629 20:06:13.939347 140493874636672 run_squad.py:440] token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True\n",
            "I0629 20:06:14.020790 140493874636672 run_squad.py:442] input_ids: 2 305 700 14 219 1815 278 11 7583 630 55 34 243 414 45024 3 11 7583 630 61 18 219 293 45015 858 15 213 1815 34 2223 414 15 220 11 696 377 45017 11 7583 19 53 1828 48 27 10026 179 14831 1795 15 728 27 10026 179 465 29183 45015 1652 11207 19 14 11 1201 10598 96 47 76 12604 45017 76 1676 11 34772 45016 8219 190 14 11 6962 137 341 76 1935 45015 10026 179 3017 1652 127 24 7583 45015 35078 32028 43 7583 140 45015 41 325 2052 190 45017 11 6998 14 6610 45015 47 76 663 31 2050 33 11 140 2934 45015 26 18 228 3733 18936 14 2223 204 45015 15 128 981 135 14 6610 26 13256 96 18 39850 15 182 9935 2463 17541 16 11678 8829 45017 11 7583 19 36 1460 138 27 10026 179 394 45015 70 24 10026 179 1645 21190 1985 15 1098 2216 45015 15 27 10026 179 677 213 18019 15 11841 45017 7583 22346 885 11 536 14 14518 128 4744 309 85 12026 28 487 1530 34 11 27728 15 6209 19 45015 15 41 2744 34 4132 14 10026 179 2041 45015 553 11 17894 45015 329 17 11 7583 3048 14 423 50 11 1206 14 24482 43561 16 13961 1572 45017 7583 858 15 213 340 2237 37 91 88 180 5046 17 11 11891 151 14 11 696 377 45015 118 10026 179 2508 2992 13983 34446 135 885 11 17541 14 4279 16 11 19493 2360 45015 17 2829 15 3432 16 228 1048 45015 17 2521 45015 15 17 11 8272 14 276 1282 15 11 966 100 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.021056 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.021208 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.030349 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:14.030684 140493874636672 run_squad.py:432] unique_id: 1000000013\n",
            "I0629 20:06:14.030782 140493874636672 run_squad.py:433] example_index: 13\n",
            "I0629 20:06:14.030860 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:14.031082 140493874636672 run_squad.py:436] tokens: [CLS] who was fame ##d for thei ##r christian spirit ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were fame ##d for thei ##r martial spirit and eventually for thei ##r christian piety , becoming exponent ##s of the catholic orthodoxy into which they assimilated . they adopted the gallo - romance language of the frankis ##h land they settled , thei ##r dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a cohesive and form ##ida ##ble principality in feudal tenure . the norman ##s are noted both for thei ##r culture , such as thei ##r unique romanesque architecture and musical traditions , and for thei ##r significant military accomplishments and innovations . norman adventurers founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the saracens and byzantine ##s , and an expedition on behalf of thei ##r duke , william the conqueror , led to the norman conquest of england at the battle of hasti ##ngs in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where thei ##r prince bo ##hem ##ond i founded the principality of antioch in the lev ##ant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\n",
            "I0629 20:06:14.031265 140493874636672 run_squad.py:438] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:6 20:7 21:8 22:9 23:10 24:11 25:12 26:13 27:14 28:15 29:16 30:17 31:18 32:18 33:19 34:20 35:20 36:21 37:22 38:22 39:23 40:24 41:24 42:25 43:26 44:27 45:28 46:29 47:30 48:30 49:31 50:32 51:32 52:33 53:34 54:34 55:35 56:36 57:37 58:38 59:39 60:40 61:41 62:42 63:42 64:43 65:44 66:45 67:46 68:46 69:46 70:47 71:48 72:49 73:50 74:50 75:51 76:52 77:53 78:53 79:54 80:54 81:55 82:56 83:57 84:58 85:59 86:59 87:60 88:60 89:61 90:62 91:63 92:63 93:64 94:65 95:66 96:67 97:67 98:68 99:69 100:70 101:71 102:71 103:72 104:73 105:74 106:75 107:76 108:77 109:78 110:79 111:80 112:80 113:81 114:82 115:83 116:84 117:84 118:85 119:86 120:87 121:87 122:88 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:102 139:103 140:104 141:104 142:105 143:106 144:106 145:107 146:108 147:109 148:110 149:111 150:111 151:112 152:112 153:113 154:114 155:115 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:121 164:122 165:123 166:124 167:124 168:125 169:126 170:127 171:128 172:129 173:129 174:130 175:131 176:132 177:133 178:134 179:135 180:136 181:137 182:138 183:139 184:140 185:141 186:141 187:142 188:143 189:144 190:145 191:146 192:147 193:148 194:148 195:148 196:149 197:150 198:151 199:152 200:153 201:154 202:155 203:155 204:156 205:156 206:157 207:158 208:159 209:159 210:160 211:161 212:162 213:163 214:164 215:165 216:166 217:167 218:168 219:169 220:170 221:171 222:171 223:172 224:173 225:173 226:173 227:174 228:175 229:176 230:177 231:178 232:179 233:180 234:181 235:182 236:183 237:184 238:185 239:186 240:187 241:188 242:189 243:190 244:191 245:192 246:192 247:193 248:194 249:194 250:195 251:196 252:196 253:196 254:197 255:198 256:199 257:200 258:201 259:202 260:203 261:204 262:205 263:205 264:205 265:206 266:207 267:208 268:209 269:210 270:211 271:212 272:212 273:213 274:214 275:214 276:215 277:216 278:217 279:218 280:219 281:220 282:221 283:222 284:223 285:224 286:225 287:225\n",
            "I0629 20:06:14.031476 140493874636672 run_squad.py:440] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True\n",
            "I0629 20:06:14.032226 140493874636672 run_squad.py:442] input_ids: 2 74 26 1828 48 27 10026 179 465 1795 45024 3 11 7583 630 61 18 219 293 45015 858 15 213 1815 34 2223 414 15 220 11 696 377 45017 11 7583 19 53 1828 48 27 10026 179 14831 1795 15 728 27 10026 179 465 29183 45015 1652 11207 19 14 11 1201 10598 96 47 76 12604 45017 76 1676 11 34772 45016 8219 190 14 11 6962 137 341 76 1935 45015 10026 179 3017 1652 127 24 7583 45015 35078 32028 43 7583 140 45015 41 325 2052 190 45017 11 6998 14 6610 45015 47 76 663 31 2050 33 11 140 2934 45015 26 18 228 3733 18936 14 2223 204 45015 15 128 981 135 14 6610 26 13256 96 18 39850 15 182 9935 2463 17541 16 11678 8829 45017 11 7583 19 36 1460 138 27 10026 179 394 45015 70 24 10026 179 1645 21190 1985 15 1098 2216 45015 15 27 10026 179 677 213 18019 15 11841 45017 7583 22346 885 11 536 14 14518 128 4744 309 85 12026 28 487 1530 34 11 27728 15 6209 19 45015 15 41 2744 34 4132 14 10026 179 2041 45015 553 11 17894 45015 329 17 11 7583 3048 14 423 50 11 1206 14 24482 43561 16 13961 1572 45017 7583 858 15 213 340 2237 37 91 88 180 5046 17 11 11891 151 14 11 696 377 45015 118 10026 179 2508 2992 13983 34446 135 885 11 17541 14 4279 16 11 19493 2360 45015 17 2829 15 3432 16 228 1048 45015 17 2521 45015 15 17 11 8272 14 276 1282 15 11 966 100 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.032520 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.032724 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.041237 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:14.041477 140493874636672 run_squad.py:432] unique_id: 1000000014\n",
            "I0629 20:06:14.041585 140493874636672 run_squad.py:433] example_index: 14\n",
            "I0629 20:06:14.041657 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:14.041863 140493874636672 run_squad.py:436] tokens: [CLS] who as ##simil ##ted the roman language ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were fame ##d for thei ##r martial spirit and eventually for thei ##r christian piety , becoming exponent ##s of the catholic orthodoxy into which they assimilated . they adopted the gallo - romance language of the frankis ##h land they settled , thei ##r dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a cohesive and form ##ida ##ble principality in feudal tenure . the norman ##s are noted both for thei ##r culture , such as thei ##r unique romanesque architecture and musical traditions , and for thei ##r significant military accomplishments and innovations . norman adventurers founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the saracens and byzantine ##s , and an expedition on behalf of thei ##r duke , william the conqueror , led to the norman conquest of england at the battle of hasti ##ngs in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where thei ##r prince bo ##hem ##ond i founded the principality of antioch in the lev ##ant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\n",
            "I0629 20:06:14.042059 140493874636672 run_squad.py:438] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:18 30:18 31:19 32:20 33:20 34:21 35:22 36:22 37:23 38:24 39:24 40:25 41:26 42:27 43:28 44:29 45:30 46:30 47:31 48:32 49:32 50:33 51:34 52:34 53:35 54:36 55:37 56:38 57:39 58:40 59:41 60:42 61:42 62:43 63:44 64:45 65:46 66:46 67:46 68:47 69:48 70:49 71:50 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:57 82:58 83:59 84:59 85:60 86:60 87:61 88:62 89:63 90:63 91:64 92:65 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:71 101:72 102:73 103:74 104:75 105:76 106:77 107:78 108:79 109:80 110:80 111:81 112:82 113:83 114:84 115:84 116:85 117:86 118:87 119:87 120:88 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:100 134:100 135:101 136:102 137:103 138:104 139:104 140:105 141:106 142:106 143:107 144:108 145:109 146:110 147:111 148:111 149:112 150:112 151:113 152:114 153:115 154:115 155:116 156:117 157:118 158:119 159:120 160:121 161:121 162:122 163:123 164:124 165:124 166:125 167:126 168:127 169:128 170:129 171:129 172:130 173:131 174:132 175:133 176:134 177:135 178:136 179:137 180:138 181:139 182:140 183:141 184:141 185:142 186:143 187:144 188:145 189:146 190:147 191:148 192:148 193:148 194:149 195:150 196:151 197:152 198:153 199:154 200:155 201:155 202:156 203:156 204:157 205:158 206:159 207:159 208:160 209:161 210:162 211:163 212:164 213:165 214:166 215:167 216:168 217:169 218:170 219:171 220:171 221:172 222:173 223:173 224:173 225:174 226:175 227:176 228:177 229:178 230:179 231:180 232:181 233:182 234:183 235:184 236:185 237:186 238:187 239:188 240:189 241:190 242:191 243:192 244:192 245:193 246:194 247:194 248:195 249:196 250:196 251:196 252:197 253:198 254:199 255:200 256:201 257:202 258:203 259:204 260:205 261:205 262:205 263:206 264:207 265:208 266:209 267:210 268:211 269:212 270:212 271:213 272:214 273:214 274:215 275:216 276:217 277:218 278:219 279:220 280:221 281:222 282:223 283:224 284:225 285:225\n",
            "I0629 20:06:14.042237 140493874636672 run_squad.py:440] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True\n",
            "I0629 20:06:14.122907 140493874636672 run_squad.py:442] input_ids: 2 74 24 43370 1279 11 525 190 45024 3 11 7583 630 61 18 219 293 45015 858 15 213 1815 34 2223 414 15 220 11 696 377 45017 11 7583 19 53 1828 48 27 10026 179 14831 1795 15 728 27 10026 179 465 29183 45015 1652 11207 19 14 11 1201 10598 96 47 76 12604 45017 76 1676 11 34772 45016 8219 190 14 11 6962 137 341 76 1935 45015 10026 179 3017 1652 127 24 7583 45015 35078 32028 43 7583 140 45015 41 325 2052 190 45017 11 6998 14 6610 45015 47 76 663 31 2050 33 11 140 2934 45015 26 18 228 3733 18936 14 2223 204 45015 15 128 981 135 14 6610 26 13256 96 18 39850 15 182 9935 2463 17541 16 11678 8829 45017 11 7583 19 36 1460 138 27 10026 179 394 45015 70 24 10026 179 1645 21190 1985 15 1098 2216 45015 15 27 10026 179 677 213 18019 15 11841 45017 7583 22346 885 11 536 14 14518 128 4744 309 85 12026 28 487 1530 34 11 27728 15 6209 19 45015 15 41 2744 34 4132 14 10026 179 2041 45015 553 11 17894 45015 329 17 11 7583 3048 14 423 50 11 1206 14 24482 43561 16 13961 1572 45017 7583 858 15 213 340 2237 37 91 88 180 5046 17 11 11891 151 14 11 696 377 45015 118 10026 179 2508 2992 13983 34446 135 885 11 17541 14 4279 16 11 19493 2360 45015 17 2829 15 3432 16 228 1048 45015 17 2521 45015 15 17 11 8272 14 276 1282 15 11 966 100 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.123208 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.123400 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.132606 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:14.132868 140493874636672 run_squad.py:432] unique_id: 1000000015\n",
            "I0629 20:06:14.132965 140493874636672 run_squad.py:433] example_index: 15\n",
            "I0629 20:06:14.133042 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:14.133257 140493874636672 run_squad.py:436] tokens: [CLS] who rule ##d the country of normandy ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were fame ##d for thei ##r martial spirit and eventually for thei ##r christian piety , becoming exponent ##s of the catholic orthodoxy into which they assimilated . they adopted the gallo - romance language of the frankis ##h land they settled , thei ##r dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a cohesive and form ##ida ##ble principality in feudal tenure . the norman ##s are noted both for thei ##r culture , such as thei ##r unique romanesque architecture and musical traditions , and for thei ##r significant military accomplishments and innovations . norman adventurers founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the saracens and byzantine ##s , and an expedition on behalf of thei ##r duke , william the conqueror , led to the norman conquest of england at the battle of hasti ##ngs in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where thei ##r prince bo ##hem ##ond i founded the principality of antioch in the lev ##ant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\n",
            "I0629 20:06:14.133439 140493874636672 run_squad.py:438] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:18 30:18 31:19 32:20 33:20 34:21 35:22 36:22 37:23 38:24 39:24 40:25 41:26 42:27 43:28 44:29 45:30 46:30 47:31 48:32 49:32 50:33 51:34 52:34 53:35 54:36 55:37 56:38 57:39 58:40 59:41 60:42 61:42 62:43 63:44 64:45 65:46 66:46 67:46 68:47 69:48 70:49 71:50 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:57 82:58 83:59 84:59 85:60 86:60 87:61 88:62 89:63 90:63 91:64 92:65 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:71 101:72 102:73 103:74 104:75 105:76 106:77 107:78 108:79 109:80 110:80 111:81 112:82 113:83 114:84 115:84 116:85 117:86 118:87 119:87 120:88 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:100 134:100 135:101 136:102 137:103 138:104 139:104 140:105 141:106 142:106 143:107 144:108 145:109 146:110 147:111 148:111 149:112 150:112 151:113 152:114 153:115 154:115 155:116 156:117 157:118 158:119 159:120 160:121 161:121 162:122 163:123 164:124 165:124 166:125 167:126 168:127 169:128 170:129 171:129 172:130 173:131 174:132 175:133 176:134 177:135 178:136 179:137 180:138 181:139 182:140 183:141 184:141 185:142 186:143 187:144 188:145 189:146 190:147 191:148 192:148 193:148 194:149 195:150 196:151 197:152 198:153 199:154 200:155 201:155 202:156 203:156 204:157 205:158 206:159 207:159 208:160 209:161 210:162 211:163 212:164 213:165 214:166 215:167 216:168 217:169 218:170 219:171 220:171 221:172 222:173 223:173 224:173 225:174 226:175 227:176 228:177 229:178 230:179 231:180 232:181 233:182 234:183 235:184 236:185 237:186 238:187 239:188 240:189 241:190 242:191 243:192 244:192 245:193 246:194 247:194 248:195 249:196 250:196 251:196 252:197 253:198 254:199 255:200 256:201 257:202 258:203 259:204 260:205 261:205 262:205 263:206 264:207 265:208 266:209 267:210 268:211 269:212 270:212 271:213 272:214 273:214 274:215 275:216 276:217 277:218 278:219 279:220 280:221 281:222 282:223 283:224 284:225 285:225\n",
            "I0629 20:06:14.133636 140493874636672 run_squad.py:440] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True\n",
            "I0629 20:06:14.133846 140493874636672 run_squad.py:442] input_ids: 2 74 499 48 11 187 14 6610 45024 3 11 7583 630 61 18 219 293 45015 858 15 213 1815 34 2223 414 15 220 11 696 377 45017 11 7583 19 53 1828 48 27 10026 179 14831 1795 15 728 27 10026 179 465 29183 45015 1652 11207 19 14 11 1201 10598 96 47 76 12604 45017 76 1676 11 34772 45016 8219 190 14 11 6962 137 341 76 1935 45015 10026 179 3017 1652 127 24 7583 45015 35078 32028 43 7583 140 45015 41 325 2052 190 45017 11 6998 14 6610 45015 47 76 663 31 2050 33 11 140 2934 45015 26 18 228 3733 18936 14 2223 204 45015 15 128 981 135 14 6610 26 13256 96 18 39850 15 182 9935 2463 17541 16 11678 8829 45017 11 7583 19 36 1460 138 27 10026 179 394 45015 70 24 10026 179 1645 21190 1985 15 1098 2216 45015 15 27 10026 179 677 213 18019 15 11841 45017 7583 22346 885 11 536 14 14518 128 4744 309 85 12026 28 487 1530 34 11 27728 15 6209 19 45015 15 41 2744 34 4132 14 10026 179 2041 45015 553 11 17894 45015 329 17 11 7583 3048 14 423 50 11 1206 14 24482 43561 16 13961 1572 45017 7583 858 15 213 340 2237 37 91 88 180 5046 17 11 11891 151 14 11 696 377 45015 118 10026 179 2508 2992 13983 34446 135 885 11 17541 14 4279 16 11 19493 2360 45015 17 2829 15 3432 16 228 1048 45015 17 2521 45015 15 17 11 8272 14 276 1282 15 11 966 100 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.134023 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.134205 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.142765 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:14.143001 140493874636672 run_squad.py:432] unique_id: 1000000016\n",
            "I0629 20:06:14.143097 140493874636672 run_squad.py:433] example_index: 16\n",
            "I0629 20:06:14.143173 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:14.143376 140493874636672 run_squad.py:436] tokens: [CLS] what principality did william the conquer ##er found ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were fame ##d for thei ##r martial spirit and eventually for thei ##r christian piety , becoming exponent ##s of the catholic orthodoxy into which they assimilated . they adopted the gallo - romance language of the frankis ##h land they settled , thei ##r dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a cohesive and form ##ida ##ble principality in feudal tenure . the norman ##s are noted both for thei ##r culture , such as thei ##r unique romanesque architecture and musical traditions , and for thei ##r significant military accomplishments and innovations . norman adventurers founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the saracens and byzantine ##s , and an expedition on behalf of thei ##r duke , william the conqueror , led to the norman conquest of england at the battle of hasti ##ngs in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where thei ##r prince bo ##hem ##ond i founded the principality of antioch in the lev ##ant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\n",
            "I0629 20:06:14.143594 140493874636672 run_squad.py:438] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:18 31:18 32:19 33:20 34:20 35:21 36:22 37:22 38:23 39:24 40:24 41:25 42:26 43:27 44:28 45:29 46:30 47:30 48:31 49:32 50:32 51:33 52:34 53:34 54:35 55:36 56:37 57:38 58:39 59:40 60:41 61:42 62:42 63:43 64:44 65:45 66:46 67:46 68:46 69:47 70:48 71:49 72:50 73:50 74:51 75:52 76:53 77:53 78:54 79:54 80:55 81:56 82:57 83:58 84:59 85:59 86:60 87:60 88:61 89:62 90:63 91:63 92:64 93:65 94:66 95:67 96:67 97:68 98:69 99:70 100:71 101:71 102:72 103:73 104:74 105:75 106:76 107:77 108:78 109:79 110:80 111:80 112:81 113:82 114:83 115:84 116:84 117:85 118:86 119:87 120:87 121:88 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:100 135:100 136:101 137:102 138:103 139:104 140:104 141:105 142:106 143:106 144:107 145:108 146:109 147:110 148:111 149:111 150:112 151:112 152:113 153:114 154:115 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:121 163:122 164:123 165:124 166:124 167:125 168:126 169:127 170:128 171:129 172:129 173:130 174:131 175:132 176:133 177:134 178:135 179:136 180:137 181:138 182:139 183:140 184:141 185:141 186:142 187:143 188:144 189:145 190:146 191:147 192:148 193:148 194:148 195:149 196:150 197:151 198:152 199:153 200:154 201:155 202:155 203:156 204:156 205:157 206:158 207:159 208:159 209:160 210:161 211:162 212:163 213:164 214:165 215:166 216:167 217:168 218:169 219:170 220:171 221:171 222:172 223:173 224:173 225:173 226:174 227:175 228:176 229:177 230:178 231:179 232:180 233:181 234:182 235:183 236:184 237:185 238:186 239:187 240:188 241:189 242:190 243:191 244:192 245:192 246:193 247:194 248:194 249:195 250:196 251:196 252:196 253:197 254:198 255:199 256:200 257:201 258:202 259:203 260:204 261:205 262:205 263:205 264:206 265:207 266:208 267:209 268:210 269:211 270:212 271:212 272:213 273:214 274:214 275:215 276:216 277:217 278:218 279:219 280:220 281:221 282:222 283:223 284:224 285:225 286:225\n",
            "I0629 20:06:14.143782 140493874636672 run_squad.py:440] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True\n",
            "I0629 20:06:14.226519 140493874636672 run_squad.py:442] input_ids: 2 305 17541 278 553 11 12026 175 188 45024 3 11 7583 630 61 18 219 293 45015 858 15 213 1815 34 2223 414 15 220 11 696 377 45017 11 7583 19 53 1828 48 27 10026 179 14831 1795 15 728 27 10026 179 465 29183 45015 1652 11207 19 14 11 1201 10598 96 47 76 12604 45017 76 1676 11 34772 45016 8219 190 14 11 6962 137 341 76 1935 45015 10026 179 3017 1652 127 24 7583 45015 35078 32028 43 7583 140 45015 41 325 2052 190 45017 11 6998 14 6610 45015 47 76 663 31 2050 33 11 140 2934 45015 26 18 228 3733 18936 14 2223 204 45015 15 128 981 135 14 6610 26 13256 96 18 39850 15 182 9935 2463 17541 16 11678 8829 45017 11 7583 19 36 1460 138 27 10026 179 394 45015 70 24 10026 179 1645 21190 1985 15 1098 2216 45015 15 27 10026 179 677 213 18019 15 11841 45017 7583 22346 885 11 536 14 14518 128 4744 309 85 12026 28 487 1530 34 11 27728 15 6209 19 45015 15 41 2744 34 4132 14 10026 179 2041 45015 553 11 17894 45015 329 17 11 7583 3048 14 423 50 11 1206 14 24482 43561 16 13961 1572 45017 7583 858 15 213 340 2237 37 91 88 180 5046 17 11 11891 151 14 11 696 377 45015 118 10026 179 2508 2992 13983 34446 135 885 11 17541 14 4279 16 11 19493 2360 45015 17 2829 15 3432 16 228 1048 45015 17 2521 45015 15 17 11 8272 14 276 1282 15 11 966 100 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.226893 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.227128 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.232884 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:14.233181 140493874636672 run_squad.py:432] unique_id: 1000000017\n",
            "I0629 20:06:14.233313 140493874636672 run_squad.py:433] example_index: 17\n",
            "I0629 20:06:14.233412 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:14.233583 140493874636672 run_squad.py:436] tokens: [CLS] what is the original meaning of the word norman ? [SEP] the english name \" norman ##s \" comes from the french words norman ##s / norman ##z , plural of norman ##t , modern french norman ##d , which is itself borrowed from old low franconia ##n nor ##tman ##n \" north ##man \" or directly from old norse [UNK] , latiniz ##ed variously as nor ##tman ##nus , norman ##nus , or nord ##mann ##us ( recorded in medieval latin , 9 ##th century ) to mean \" norse ##man , viking \" . [SEP]\n",
            "I0629 20:06:14.233765 140493874636672 run_squad.py:438] token_to_orig_map: 12:0 13:1 14:2 15:3 16:3 17:3 18:3 19:4 20:5 21:6 22:7 23:8 24:9 25:9 26:9 27:9 28:9 29:9 30:10 31:11 32:12 33:12 34:12 35:13 36:14 37:15 38:15 39:15 40:16 41:17 42:18 43:19 44:20 45:21 46:22 47:23 48:23 49:24 50:24 51:24 52:25 53:25 54:25 55:25 56:26 57:27 58:28 59:29 60:30 61:31 62:31 63:32 64:32 65:33 66:34 67:35 68:35 69:35 70:35 71:36 72:36 73:36 74:37 75:38 76:38 77:38 78:39 79:39 80:40 81:41 82:42 83:42 84:43 85:43 86:44 87:44 88:45 89:46 90:47 91:47 92:47 93:47 94:48 95:48 96:48\n",
            "I0629 20:06:14.233921 140493874636672 run_squad.py:440] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True\n",
            "I0629 20:06:14.234184 140493874636672 run_squad.py:442] input_ids: 2 305 21 11 382 1101 14 11 455 7583 45024 3 11 335 170 45005 7583 19 45005 1823 37 11 140 518 7583 19 45018 7583 300 45015 5252 14 7583 121 45015 243 140 7583 48 45015 47 21 555 6321 37 400 488 18035 59 1715 35411 59 45005 276 767 45005 43 1020 37 400 3133 1 45015 44840 35 7758 24 1715 35411 13817 45015 7583 13817 45015 43 11932 5167 306 45011 947 16 2223 833 45015 1065 108 159 45012 17 2436 45005 3133 767 45015 7568 45005 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.234435 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.236788 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.240862 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:14.241092 140493874636672 run_squad.py:432] unique_id: 1000000018\n",
            "I0629 20:06:14.241174 140493874636672 run_squad.py:433] example_index: 18\n",
            "I0629 20:06:14.241245 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:14.241371 140493874636672 run_squad.py:436] tokens: [CLS] when was the latin version of the word norman first recorded ? [SEP] the english name \" norman ##s \" comes from the french words norman ##s / norman ##z , plural of norman ##t , modern french norman ##d , which is itself borrowed from old low franconia ##n nor ##tman ##n \" north ##man \" or directly from old norse [UNK] , latiniz ##ed variously as nor ##tman ##nus , norman ##nus , or nord ##mann ##us ( recorded in medieval latin , 9 ##th century ) to mean \" norse ##man , viking \" . [SEP]\n",
            "I0629 20:06:14.241496 140493874636672 run_squad.py:438] token_to_orig_map: 14:0 15:1 16:2 17:3 18:3 19:3 20:3 21:4 22:5 23:6 24:7 25:8 26:9 27:9 28:9 29:9 30:9 31:9 32:10 33:11 34:12 35:12 36:12 37:13 38:14 39:15 40:15 41:15 42:16 43:17 44:18 45:19 46:20 47:21 48:22 49:23 50:23 51:24 52:24 53:24 54:25 55:25 56:25 57:25 58:26 59:27 60:28 61:29 62:30 63:31 64:31 65:32 66:32 67:33 68:34 69:35 70:35 71:35 72:35 73:36 74:36 75:36 76:37 77:38 78:38 79:38 80:39 81:39 82:40 83:41 84:42 85:42 86:43 87:43 88:44 89:44 90:45 91:46 92:47 93:47 94:47 95:47 96:48 97:48 98:48\n",
            "I0629 20:06:14.241630 140493874636672 run_squad.py:440] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True\n",
            "I0629 20:06:14.241812 140493874636672 run_squad.py:442] input_ids: 2 94 26 11 833 484 14 11 455 7583 73 947 45024 3 11 335 170 45005 7583 19 45005 1823 37 11 140 518 7583 19 45018 7583 300 45015 5252 14 7583 121 45015 243 140 7583 48 45015 47 21 555 6321 37 400 488 18035 59 1715 35411 59 45005 276 767 45005 43 1020 37 400 3133 1 45015 44840 35 7758 24 1715 35411 13817 45015 7583 13817 45015 43 11932 5167 306 45011 947 16 2223 833 45015 1065 108 159 45012 17 2436 45005 3133 767 45015 7568 45005 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.242000 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.242190 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.245722 140493874636672 run_squad.py:431] *** Example ***\n",
            "I0629 20:06:14.245953 140493874636672 run_squad.py:432] unique_id: 1000000019\n",
            "I0629 20:06:14.246034 140493874636672 run_squad.py:433] example_index: 19\n",
            "I0629 20:06:14.246102 140493874636672 run_squad.py:434] doc_span_index: 0\n",
            "I0629 20:06:14.246223 140493874636672 run_squad.py:436] tokens: [CLS] what name comes from the english words norman ##s / norman ##z ? [SEP] the english name \" norman ##s \" comes from the french words norman ##s / norman ##z , plural of norman ##t , modern french norman ##d , which is itself borrowed from old low franconia ##n nor ##tman ##n \" north ##man \" or directly from old norse [UNK] , latiniz ##ed variously as nor ##tman ##nus , norman ##nus , or nord ##mann ##us ( recorded in medieval latin , 9 ##th century ) to mean \" norse ##man , viking \" . [SEP]\n",
            "I0629 20:06:14.246337 140493874636672 run_squad.py:438] token_to_orig_map: 15:0 16:1 17:2 18:3 19:3 20:3 21:3 22:4 23:5 24:6 25:7 26:8 27:9 28:9 29:9 30:9 31:9 32:9 33:10 34:11 35:12 36:12 37:12 38:13 39:14 40:15 41:15 42:15 43:16 44:17 45:18 46:19 47:20 48:21 49:22 50:23 51:23 52:24 53:24 54:24 55:25 56:25 57:25 58:25 59:26 60:27 61:28 62:29 63:30 64:31 65:31 66:32 67:32 68:33 69:34 70:35 71:35 72:35 73:35 74:36 75:36 76:36 77:37 78:38 79:38 80:38 81:39 82:39 83:40 84:41 85:42 86:42 87:43 88:43 89:44 90:44 91:45 92:46 93:47 94:47 95:47 96:47 97:48 98:48 99:48\n",
            "I0629 20:06:14.246450 140493874636672 run_squad.py:440] token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True\n",
            "I0629 20:06:14.246675 140493874636672 run_squad.py:442] input_ids: 2 305 170 1823 37 11 335 518 7583 19 45018 7583 300 45024 3 11 335 170 45005 7583 19 45005 1823 37 11 140 518 7583 19 45018 7583 300 45015 5252 14 7583 121 45015 243 140 7583 48 45015 47 21 555 6321 37 400 488 18035 59 1715 35411 59 45005 276 767 45005 43 1020 37 400 3133 1 45015 44840 35 7758 24 1715 35411 13817 45015 7583 13817 45015 43 11932 5167 306 45011 947 16 2223 833 45015 1065 108 159 45012 17 2436 45005 3133 767 45015 7568 45005 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.246873 140493874636672 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:06:14.247065 140493874636672 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 20:07:22.060994 140493874636672 run_squad.py:1240] ***** Running predictions *****\n",
            "I0629 20:07:22.061283 140493874636672 run_squad.py:1241]   Num orig examples = 11873\n",
            "I0629 20:07:22.061406 140493874636672 run_squad.py:1242]   Num split examples = 12280\n",
            "I0629 20:07:22.061514 140493874636672 run_squad.py:1243]   Batch size = 8\n",
            "I0629 20:07:22.806637 140493874636672 estimator.py:1145] Calling model_fn.\n",
            "I0629 20:07:23.016043 140493874636672 run_squad.py:598] *** Features ***\n",
            "I0629 20:07:23.016325 140493874636672 run_squad.py:600]   name = input_ids, shape = (1, 384)\n",
            "I0629 20:07:23.016449 140493874636672 run_squad.py:600]   name = input_mask, shape = (1, 384)\n",
            "I0629 20:07:23.016548 140493874636672 run_squad.py:600]   name = segment_ids, shape = (1, 384)\n",
            "I0629 20:07:23.016659 140493874636672 run_squad.py:600]   name = unique_ids, shape = (1,)\n",
            "I0629 20:07:27.421683 140493874636672 run_squad.py:634] **** Trainable Variables ****\n",
            "I0629 20:07:27.421962 140493874636672 run_squad.py:640]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422100 140493874636672 run_squad.py:640]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422196 140493874636672 run_squad.py:640]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422286 140493874636672 run_squad.py:640]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422375 140493874636672 run_squad.py:640]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422456 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422538 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422633 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422713 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422795 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422873 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.422948 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423025 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423099 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423174 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423247 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423326 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423406 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423485 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423576 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423660 140493874636672 run_squad.py:640]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423737 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423838 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423916 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.423994 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424069 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424146 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424221 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424297 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424373 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424447 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424521 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424617 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424694 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424777 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424852 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424925 140493874636672 run_squad.py:640]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.424998 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425073 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425153 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425231 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425306 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425382 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425455 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425532 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425621 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425695 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425776 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425852 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.425926 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426002 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426074 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426145 140493874636672 run_squad.py:640]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426218 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426294 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426368 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426444 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426517 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426608 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426682 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426757 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426836 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426907 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.426978 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427052 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427124 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427206 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427278 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427350 140493874636672 run_squad.py:640]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427421 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427494 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427583 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427666 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427739 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427822 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427895 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.427970 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428044 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428123 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428195 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428271 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428346 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428422 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428495 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428590 140493874636672 run_squad.py:640]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428665 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428742 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428822 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428896 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.428968 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429043 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429115 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429190 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429264 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429338 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429411 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429486 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429575 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429657 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429730 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429811 140493874636672 run_squad.py:640]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429884 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.429957 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430037 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430112 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430185 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430266 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430341 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430417 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430490 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430575 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430650 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430725 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430803 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430878 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.430949 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.431020 140493874636672 run_squad.py:640]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.431092 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.431167 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.431239 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.457674 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.457983 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.458131 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.458234 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.458343 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.458440 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.458533 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.458658 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.458756 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.458873 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.458976 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.459073 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.459164 140493874636672 run_squad.py:640]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.459257 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.459356 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.459451 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.459545 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.459671 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.459783 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.459883 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.459980 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.460077 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.460171 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.460265 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.460362 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.460460 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.460642 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.460768 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.460889 140493874636672 run_squad.py:640]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.461000 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.461112 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.461218 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.461330 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.461438 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.461548 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.461678 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.461801 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.461910 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.462015 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.462119 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.462244 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.462351 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.462459 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.462584 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.462694 140493874636672 run_squad.py:640]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.462817 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.462934 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.463039 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.463147 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.463257 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.463370 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.463474 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.463603 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.463714 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.463829 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.463932 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.464036 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.464142 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.464251 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.464355 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.464460 140493874636672 run_squad.py:640]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.464584 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.464697 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.464828 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.464943 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.465048 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.465156 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.465260 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.465370 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.465472 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.465597 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.465706 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.465827 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.465932 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.466043 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.466150 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.466251 140493874636672 run_squad.py:640]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.466354 140493874636672 run_squad.py:640]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.466465 140493874636672 run_squad.py:640]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "I0629 20:07:27.466588 140493874636672 run_squad.py:640]   name = cls/squad/output_weights:0, shape = (2, 768)\n",
            "I0629 20:07:27.466701 140493874636672 run_squad.py:640]   name = cls/squad/output_bias:0, shape = (2,)\n",
            "I0629 20:07:29.959370 140493874636672 estimator.py:1147] Done calling model_fn.\n",
            "I0629 20:07:29.964868 140493874636672 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 20:07:30.569847 140493874636672 monitored_session.py:240] Graph was finalized.\n",
            "W0629 20:07:30.570725 140493874636672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0629 20:07:30.722323 140493874636672 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/wordpiece/squad/model.ckpt-542\n",
            "I0629 20:08:27.320324 140493874636672 session_manager.py:500] Running local_init_op.\n",
            "I0629 20:08:27.520437 140493874636672 session_manager.py:502] Done running local_init_op.\n",
            "I0629 20:08:28.008804 140493874636672 tpu_estimator.py:557] Init TPU system\n",
            "I0629 20:08:36.082908 140493874636672 tpu_estimator.py:566] Initialized TPU in 8 seconds\n",
            "I0629 20:08:36.083994 140492660819712 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 20:08:36.084749 140492652427008 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 20:08:36.386400 140493874636672 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0629 20:08:36.589757 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:36.590110 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:36.590891 140492652427008 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 20:08:41.326701 140493874636672 run_squad.py:1259] Processing example: 0\n",
            "I0629 20:08:41.330072 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.330416 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.341856 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.342121 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.351352 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.351677 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.359179 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.359491 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.367478 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.367774 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.374687 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.374951 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.382771 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.383070 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.391713 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.392014 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.399924 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.400233 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.407917 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.408207 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.415952 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.416262 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.422838 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.423147 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.430757 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.431048 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.439207 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.439580 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.449971 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.450255 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.459695 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.459958 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.468203 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.468473 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.478336 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.478628 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.488221 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.488479 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.497879 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.498195 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.510372 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.510669 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.520084 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.520358 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.530982 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.531280 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.539216 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.539519 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.549382 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.549674 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.559609 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.559870 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.571178 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.571444 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.579969 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.580235 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.588900 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.589153 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.599479 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.599747 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.609140 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.609392 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.619812 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.620094 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.628386 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.628674 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.637394 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.637681 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.647376 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.647647 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.653960 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.654194 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.660694 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.660961 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.686100 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.686352 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.694463 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.694746 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.704745 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.705014 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.714370 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.714629 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.722924 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.723208 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.731904 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.732138 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.739094 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.739337 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.748009 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.748243 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.758198 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.758446 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.768841 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.769204 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.779619 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.779884 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.789776 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.790079 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.799097 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.799387 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.808240 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.808520 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.819079 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.819384 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.826290 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.826572 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.836398 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.836690 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.845403 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.845711 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.854360 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.854649 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.864639 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.864885 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.875159 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.875442 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.884068 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.884326 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.895053 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.895303 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.905879 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.906116 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.914439 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.914680 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.924256 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.924583 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.933144 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.933409 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.943255 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.943525 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.951908 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.952161 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.961279 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.961580 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.968670 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.968971 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.979679 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.979943 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.989275 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.989524 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:41.998667 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:41.998949 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.008220 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.008472 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.016819 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.017063 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.027968 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.028228 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.036117 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.036410 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.045392 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.045659 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.057047 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.057285 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.066929 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.067186 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.076039 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.076287 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.083165 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.083533 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.091143 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.091379 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.099113 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.099342 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.107612 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.107868 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.115674 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.115916 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.122448 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.122689 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.130798 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.131058 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.140451 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.140724 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.150067 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.150308 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.160149 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.160387 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.169080 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.169334 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.177052 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.177312 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.185708 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.185981 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.194946 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.195215 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.202775 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.203052 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.211421 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.211946 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.221171 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.221422 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.230815 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.231073 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.240030 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.240287 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.249007 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.249255 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.259679 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.259973 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.268111 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.268412 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.282730 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.283037 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.291159 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.291442 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.299837 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.300126 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.315944 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.316210 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.329888 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.330132 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.339617 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.339904 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.348947 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.349201 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.357790 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.358041 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.366840 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.367122 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.377752 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.378046 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.388904 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.389155 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.398131 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.398390 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.406946 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.407210 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.417473 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.417757 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.429862 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.430125 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.439354 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.439606 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.447791 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.448166 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.457743 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.458055 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.468214 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.468502 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.477497 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.479349 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.488678 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.488989 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.497909 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.498195 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.507683 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.507955 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.515237 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.515534 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.521585 140493874636672 run_squad.py:1259] Processing example: 1000\n",
            "I0629 20:08:42.525717 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.525948 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.534131 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.534383 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.543278 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.543604 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.551852 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.552170 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.559989 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.560298 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.569871 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.570188 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.577866 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.578184 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.587900 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.588201 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.596337 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.596632 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.606498 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.606844 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.615517 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.615848 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.624059 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.624365 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.635746 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.636175 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.645098 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.645403 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.654453 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.654834 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.663814 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.664158 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.677891 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.678270 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.690867 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.691207 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.700444 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.700810 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.711934 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.712300 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.721811 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.722119 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.730588 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.730883 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.742287 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.742623 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.750134 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.750441 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.761172 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.761519 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.772378 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.772734 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.783815 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.784120 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.792005 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.792263 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.802210 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.802536 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.809789 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.810105 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.818826 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.819130 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.826845 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.827125 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.834613 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.834874 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.843998 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.844279 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.854502 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.854794 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.863220 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.863486 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.873209 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.873486 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.881998 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.882297 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.891126 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.891423 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.901785 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.902105 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.910355 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.910715 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.920980 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.921294 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.930898 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.931217 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.939912 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.940222 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.947947 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.948259 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.957595 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.957907 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.966903 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.967214 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.984824 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.985120 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:42.996824 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:42.997123 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.006474 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.006808 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.017844 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.018160 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.030853 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.031174 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.043035 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.043365 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.054586 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.054912 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.064393 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.064738 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.074790 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.075101 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.087598 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.087949 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.101712 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.102038 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.114767 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.115134 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.126611 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.126942 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.139288 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.139627 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.150449 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.150835 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.167016 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.167421 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.190604 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.190973 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.205335 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.205729 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.221245 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.221629 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.232545 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.232918 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.244766 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.245112 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.257327 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.258013 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.276693 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.277019 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.300261 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.300930 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.314787 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.315119 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.326984 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.327296 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.339155 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.339494 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.350672 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.351001 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.376062 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.376400 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.391139 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.391457 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.403288 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.403644 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.416108 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.416436 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.430291 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.430652 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.446338 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.446730 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.459433 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.459835 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.472830 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.473238 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.486494 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.486853 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.496938 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.497262 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.510417 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.510848 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.522247 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.522654 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.535181 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.535549 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.547266 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.547653 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.565588 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.565877 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.582324 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.582616 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.599531 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.600023 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.612250 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.612615 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.624876 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.625217 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.635534 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.635903 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.653364 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.653951 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.665746 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.666018 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.676192 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.676478 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.687373 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.687657 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.699431 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.699708 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.714706 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.714965 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.727684 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.727954 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.739224 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.739494 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.756333 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.756600 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.768969 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.769225 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.780093 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.780477 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.792331 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.792674 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.803625 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.803900 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.815958 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.816203 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.827898 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.828167 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.841196 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.841452 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.856809 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.857065 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.872066 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.872328 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.891317 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.891585 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.903069 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.903293 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.917235 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.917484 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.927390 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.927686 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.940731 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.940946 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.951844 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.952097 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.967494 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.967779 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.978948 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.979231 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:43.990839 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:43.991123 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.009828 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.010210 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.023889 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.024170 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.036281 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.036669 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.045988 140493874636672 run_squad.py:1259] Processing example: 2000\n",
            "I0629 20:08:44.049827 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.050211 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.064627 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.064973 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.083096 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.083451 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.098759 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.099166 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.111422 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.111861 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.126713 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.127066 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.139734 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.140098 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.154635 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.154990 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.167480 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.167865 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.184589 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.184942 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.197271 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.197653 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.212968 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.213316 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.225911 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.226298 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.239743 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.240110 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.254137 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.254484 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.266205 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.266632 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.278087 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.278438 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.305763 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.306137 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.317935 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.318277 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.331869 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.332209 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.343905 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.344266 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.354044 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.354380 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.367739 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.368093 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.376873 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.377210 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.385872 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.386229 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.393715 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.394078 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.403329 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.403692 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.415156 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.415523 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.427409 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.428298 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.436324 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.436910 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.445955 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.446212 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.456992 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.457258 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.467414 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.467695 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.482147 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.482435 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.495922 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.496177 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.506502 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.506819 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.520330 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.520616 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.530451 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.530739 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.544234 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.544512 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.554355 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.554648 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.569705 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.569965 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.580151 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.580896 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.590703 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.591038 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.598124 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.598404 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.608818 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.609103 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.619995 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.620373 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.632834 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.633189 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.648153 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.648468 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.663114 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.663389 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.673455 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.673780 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.685169 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.685424 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.701472 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.701795 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.716325 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.716702 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.729124 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.729483 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.741491 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.741862 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.753427 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.753846 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.766099 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.766679 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.779307 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.779587 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.790400 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.790677 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.797007 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.797253 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.803963 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.804214 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.809974 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.810213 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.819732 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.820016 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.834388 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.834886 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.845349 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.845642 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.858855 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.859106 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.874403 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.874697 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.885480 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.885748 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.902219 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.902481 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.914967 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.915236 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.930348 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.930625 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.957118 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.957384 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.970199 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.970455 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.984296 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.984606 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:44.998187 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:44.998449 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.010928 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.011195 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.020389 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.020701 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.031358 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.031657 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.041465 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.041800 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.060684 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.060967 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.073094 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.073472 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.080170 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.080410 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.086096 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.086341 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.092274 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.092589 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.100209 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.100625 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.109393 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.109682 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.120542 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.121063 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.133758 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.134017 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.160711 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.160978 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.171025 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.171297 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.184053 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.184363 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.202037 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.202325 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.214191 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.214453 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.227491 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.228088 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.237778 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.238054 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.247666 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.248064 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.258448 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.258896 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.270984 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.271313 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.283814 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.284037 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.297159 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.297450 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.308734 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.309163 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.320904 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.321252 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.333822 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.334141 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.347362 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.347645 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.362824 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.363589 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.377681 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.377934 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.387612 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.387885 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.398813 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.399081 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.416276 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.416527 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.426283 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.426576 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.437195 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.437433 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.447871 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.448112 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.457165 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.457411 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.470905 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.471120 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.483150 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.483405 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.496182 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.496792 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.507315 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.507595 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.521675 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.521951 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.539666 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.540040 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.552486 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.552896 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.565457 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.565842 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.577400 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.577777 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.587604 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.587952 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.599072 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.599459 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.614463 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.614881 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.634294 140493874636672 run_squad.py:1259] Processing example: 3000\n",
            "I0629 20:08:45.638456 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.638851 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.650619 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.650978 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.662121 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.662476 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.674771 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.675139 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.685709 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.686058 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.697963 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.698324 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.708126 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.708461 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.718141 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.718477 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.730857 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.731163 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.774042 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.774309 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.788379 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.788665 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.798615 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.798906 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.808225 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.808590 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.820243 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.820635 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.832863 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.833163 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.844396 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.844657 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.856334 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.856608 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.866139 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.866392 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.881325 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.881590 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.892019 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.892249 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.902874 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.903106 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.910903 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.911122 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.917938 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.918169 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.923681 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.923916 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.929734 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.930170 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.937692 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.937941 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.943521 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.943773 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.949217 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.949456 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.956904 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.957146 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.963026 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.963256 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.969239 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.969461 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.974790 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.974997 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.981898 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.982167 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.987546 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.987822 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:45.994151 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:45.994420 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.000997 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.001242 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.007584 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.008136 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.014475 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.014736 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.025465 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.025761 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.031377 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.031626 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.038330 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.038598 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.045449 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.045712 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.052573 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.053105 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.061377 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.061905 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.074036 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.074367 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.085869 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.087844 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.101798 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.102086 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.114314 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.114763 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.130328 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.130764 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.149101 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.149430 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.163870 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.164183 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.179078 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.179399 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.192169 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.192489 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.210279 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.210644 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.221656 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.221972 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.233762 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.234072 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.247721 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.248040 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.260196 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.260586 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.285855 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.286182 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.300967 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.301272 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.314878 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.315479 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.329774 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.330127 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.352619 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.352923 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.369711 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.370012 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.383627 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.383936 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.397007 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.397343 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.408180 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.409100 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.426126 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.426469 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.439275 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.439681 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.452042 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.452419 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.464243 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.464507 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.477931 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.478156 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.490889 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.491625 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.505114 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.505442 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.521280 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.521871 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.541717 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.542078 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.558109 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.558349 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.571333 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.571551 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.585767 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.585971 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.599588 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.599805 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.609181 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.609379 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.619825 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.620052 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.630376 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.630613 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.641126 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.641340 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.672939 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.673220 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.685109 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.685365 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.694741 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.695401 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.702411 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.702658 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.709691 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.709942 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.716977 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.717220 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.723437 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.723700 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.729954 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.730181 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.737750 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.737985 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.744996 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.745248 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.752014 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.752245 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.760601 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.760843 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.770639 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.770879 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.780938 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.781224 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.793072 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.793322 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.804679 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.804918 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.814277 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.814511 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.824914 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.825173 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.834881 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.835113 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.854509 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.854778 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.871306 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.871647 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.890526 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.890793 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.903273 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.903527 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.913987 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.914596 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.926666 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.926915 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.940745 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.940994 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.951369 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.951612 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.961879 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.962116 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.973290 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.973541 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.983350 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.983621 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:46.993331 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:46.993620 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.004522 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.004812 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.023998 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.024250 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.036313 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.036572 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.046465 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.046721 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.059594 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.059937 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.069671 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.069932 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.080160 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.080421 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.089876 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.090132 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.099048 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.099349 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.108548 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.108869 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.116338 140493874636672 run_squad.py:1259] Processing example: 4000\n",
            "I0629 20:08:47.119832 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.120046 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.132069 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.132354 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.142232 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.142609 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.155063 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.155425 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.162109 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.162480 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.170302 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.171291 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.184962 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.185201 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.197731 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.198005 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.212413 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.212705 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.223632 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.223948 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.235280 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.235593 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.245897 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.246150 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.258993 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.259291 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.273343 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.273736 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.283452 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.283831 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.295436 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.295795 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.307524 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.307886 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.318211 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.318572 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.330131 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.330476 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.336053 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.336368 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.343444 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.343795 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.351942 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.352318 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.363226 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.363590 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.376819 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.377155 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.393791 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.394064 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.406358 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.406625 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.418549 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.419227 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.433821 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.434169 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.446066 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.446311 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.461960 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.462527 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.475181 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.475467 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.485039 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.485276 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.495185 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.495428 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.503106 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.503340 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.513226 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.513880 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.523003 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.523270 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.543304 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.543613 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.554511 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.554780 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.563629 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.563869 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.573914 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.574164 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.584483 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.584925 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.597710 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.597962 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.606878 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.607133 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.617758 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.618019 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.622976 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.623166 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.627952 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.628160 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.633853 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.634066 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.639111 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.639454 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.646253 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.646470 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.655084 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.655318 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.665591 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.665891 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.677279 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.677543 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.690679 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.690940 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.703372 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.703641 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.713977 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.714227 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.724885 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.725152 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.743658 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.743902 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.754088 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.754342 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.764595 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.764826 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.774749 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.775001 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.783888 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.784129 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.793601 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.793823 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.804738 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.804977 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.815038 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.815260 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.824091 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.824314 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.832238 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.832462 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.842129 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.842392 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.853096 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.853326 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.870028 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.870336 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.881782 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.882034 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.892866 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.893096 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.903780 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.904042 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.912175 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.912425 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.921254 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.921521 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.931159 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.931385 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.939333 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.939577 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.955017 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.955268 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.968339 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.968644 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.977477 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.977734 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.983037 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.983252 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.989312 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.989626 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:47.997128 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:47.997364 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.004195 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.004435 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.010671 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.010936 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.017039 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.017282 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.023978 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.024265 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.030166 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.030411 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.043135 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.043411 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.053006 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.053284 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.063293 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.063584 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.074354 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.074663 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.092057 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.092461 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.102968 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.103207 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.115526 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.116020 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.128125 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.128385 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.151726 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.151990 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.172351 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.172698 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.195333 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.195615 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.207046 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.207301 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.218339 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.218617 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.229879 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.230122 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.242678 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.242943 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.251900 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.252161 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.264020 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.264272 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.282710 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.282977 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.294637 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.294883 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.306848 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.307098 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.318749 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.319008 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.328957 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.329198 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.337888 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.338155 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.347580 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.347822 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.362401 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.362739 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.374989 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.375247 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.387051 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.387336 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.396896 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.397151 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.404631 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.404917 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.411530 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.411779 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.420781 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.421049 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.434924 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.435181 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.452426 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.452718 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.465816 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.466065 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.478042 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.478289 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.490603 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.490871 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.505681 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.505967 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.519361 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.519652 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.528720 140493874636672 run_squad.py:1259] Processing example: 5000\n",
            "I0629 20:08:48.532271 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.532514 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.544425 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.544732 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.554969 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.555847 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.566922 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.567210 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.581997 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.582324 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.594136 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.594408 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.605196 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.605438 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.614845 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.615107 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.625695 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.625967 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.635838 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.636241 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.643404 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.643670 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.649859 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.650293 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.657692 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.657977 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.667466 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.667742 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.697652 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.698016 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.712255 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.712597 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.731313 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.731606 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.743520 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.743928 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.755200 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.755455 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.768149 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.768440 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.776802 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.777046 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.787676 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.787944 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.815156 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.815422 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.827039 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.827304 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.841533 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.841814 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.853896 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.854157 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.863903 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.864170 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.874522 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.874999 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.885990 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.886317 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.899291 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.899714 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.911016 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.912050 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.925770 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.926067 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.939156 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.939437 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.953725 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.953974 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.965578 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.965857 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.978332 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.978701 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:48.990250 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:48.990605 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.003849 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.004173 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.017048 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.017328 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.027253 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.027529 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.039486 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.039759 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.051923 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.052233 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.063118 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.063404 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.075028 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.075319 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.085243 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.085511 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.092592 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.092884 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.101047 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.101346 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.114219 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.114492 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.134717 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.135173 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.148631 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.148916 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.168796 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.169087 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.193413 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.194174 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.215875 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.216175 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.229458 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.229749 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.241789 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.242065 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.254080 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.254347 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.265927 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.266196 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.276671 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.276948 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.291443 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.292327 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.302371 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.302685 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.313281 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.313621 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.327470 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.327783 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.342375 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.343223 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.354154 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.354486 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.365164 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.365465 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.378851 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.379292 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.394580 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.394887 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.405821 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.406188 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.417527 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.417922 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.429072 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.429444 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.447005 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.447378 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.458162 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.458518 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.468910 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.469281 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.481083 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.481481 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.493517 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.493916 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.506410 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.506798 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.521297 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.521732 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.535308 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.535707 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.549788 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.550182 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.569424 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.569813 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.580892 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.581225 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.595842 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.596185 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.612680 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.613029 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.625595 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.625991 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.638115 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.638417 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.650622 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.650872 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.663888 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.664139 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.676918 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.690976 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.702583 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.702899 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.714155 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.714546 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.729006 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.729846 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.743180 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.743544 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.755399 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.755800 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.771315 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.771721 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.782402 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.782810 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.794499 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.795103 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.854531 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.854879 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.870093 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.870753 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.886147 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.886457 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.902656 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.903013 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.916262 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.917096 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.935131 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.935419 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.950419 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.950740 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.967617 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.967906 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.983526 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.983827 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:49.996281 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:49.996549 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.008471 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.008771 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.018785 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.020520 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.039309 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.039848 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.050190 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.050470 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.069358 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.069844 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.079047 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.079409 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.089604 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.089885 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.097071 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.097400 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.105612 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.105893 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.115311 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.115660 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.126777 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.127124 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.140361 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.140899 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.153037 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.153388 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.164924 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.165280 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.179042 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.179409 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.194797 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.195281 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.207412 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.207870 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.218910 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.219261 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.228317 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.228711 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.240160 140493874636672 run_squad.py:1259] Processing example: 6000\n",
            "I0629 20:08:50.244133 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.244456 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.254246 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.254614 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.263850 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.264199 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.274406 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.274787 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.285438 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.285819 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.299313 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.299726 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.311021 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.311434 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.324532 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.324913 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.335618 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.335955 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.345090 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.345458 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.353850 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.354213 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.365115 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.365483 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.377810 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.378182 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.389227 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.389623 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.399781 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.400150 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.415526 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.415975 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.426926 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.427706 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.436786 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.437169 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.445708 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.446086 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.456243 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.456706 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.465913 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.466277 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.474823 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.475121 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.483500 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.483847 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.493519 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.493905 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.504102 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.504644 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.515256 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.515664 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.525252 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.525638 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.535984 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.536320 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.545719 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.546078 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.557877 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.558341 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.567730 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.568073 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.577127 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.577450 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.586738 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.587118 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.596607 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.597010 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.607184 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.607534 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.617153 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.617508 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.625974 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.626298 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.637545 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.637955 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.647051 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.647387 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.656942 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.657309 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.665974 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.666285 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.674152 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.674483 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.683066 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.683405 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.692546 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.692991 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.703076 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.703456 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.715203 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.715626 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.727116 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.727477 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.738027 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.738413 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.747478 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.747889 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.757707 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.758079 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.768438 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.768831 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.780531 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.780936 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.791264 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.791718 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.802730 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.803075 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.814894 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.815342 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.828163 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.828538 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.837423 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.837844 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.847060 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.847395 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.856302 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.856975 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.867199 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.867609 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.876973 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.877340 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.886350 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.886728 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.896121 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.896459 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.906084 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.906426 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.915025 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.915394 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.926732 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.927060 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.936060 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.936390 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.945414 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.945776 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.953827 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.954137 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.962465 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.962786 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.971272 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.971605 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.979912 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.980204 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.989258 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.989657 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:50.997790 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:50.998061 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.005967 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.006237 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.014117 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.014468 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.023049 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.023334 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.030703 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.031075 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.039487 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.039825 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.048711 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.049026 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.056865 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.057176 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.065140 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.065448 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.074184 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.074496 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.084150 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.084642 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.093895 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.094207 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.103529 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.103860 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.112909 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.113256 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.125261 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.125832 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.136409 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.136812 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.146657 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.147055 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.156629 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.156936 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.165900 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.166209 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.174372 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.174694 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.186445 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.186801 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.196202 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.196491 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.206655 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.206928 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.215853 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.216105 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.224391 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.224652 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.232371 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.232648 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.241274 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.241574 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.249953 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.250203 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.258419 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.258709 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.266859 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.267133 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.277058 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.277320 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.286974 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.287292 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.296929 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.297233 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.306382 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.306743 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.314489 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.314807 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.322865 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.323177 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.332769 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.333062 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.341617 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.342712 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.352540 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.352857 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.361178 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.361465 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.369110 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.369383 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.376468 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.376885 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.386504 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.386809 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.395197 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.395493 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.403638 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.403947 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.413688 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.414008 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.423077 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.423383 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.432003 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.432297 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.440358 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.440779 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.450240 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.450603 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.458508 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.458845 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.466787 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.467126 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.472428 140493874636672 run_squad.py:1259] Processing example: 7000\n",
            "I0629 20:08:51.476132 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.476421 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.484605 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.484931 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.492507 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.492879 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.503038 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.503359 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.512351 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.512731 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.521515 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.521872 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.536117 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.536503 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.546506 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.546855 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.555366 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.555718 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.564571 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.564887 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.575589 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.575912 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.583844 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.584185 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.592865 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.593194 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.602102 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.602460 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.610457 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.610782 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.618547 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.618860 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.627950 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.628434 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.637164 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.637447 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.645397 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.645738 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.653041 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.653336 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.662158 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.662481 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.671364 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.671722 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.679660 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.679982 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.688680 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.689017 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.696481 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.696819 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.706531 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.706886 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.716222 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.716532 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.724706 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.725026 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.733339 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.733675 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.749422 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.749768 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.757843 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.758138 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.766147 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.766448 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.774189 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.774631 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.783286 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.783633 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.791915 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.792196 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.799399 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.799723 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.806845 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.807106 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.814083 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.814430 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.824170 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.824504 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.833636 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.833933 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.841440 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.841896 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.849773 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.850044 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.857178 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.857434 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.864732 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.864976 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.876343 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.876660 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.885580 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.885851 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.893753 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.893988 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.903682 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.904000 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.912758 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.913057 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.922226 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.923520 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.935606 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.935896 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.944402 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.944702 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.952599 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.952841 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.960074 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.960268 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.968216 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.968471 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.975746 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.976001 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.983157 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.983425 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:51.990845 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:51.991117 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.000906 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.001210 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.008696 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.008957 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.016742 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.016972 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.024975 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.025225 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.035018 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.035285 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.047184 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.047528 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.056825 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.057135 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.065138 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.065426 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.073076 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.073363 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.082288 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.082645 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.090544 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.090879 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.098822 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.099157 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.107152 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.107464 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.115699 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.116012 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.126146 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.126457 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.138807 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.139094 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.147533 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.147808 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.157205 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.157475 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.166052 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.166301 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.176530 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.176897 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.186263 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.186610 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.195891 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.196198 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.205865 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.206230 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.216489 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.216948 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.227476 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.227829 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.237959 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.238231 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.249925 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.250177 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.258616 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.258853 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.267654 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.267901 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.275377 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.275650 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.286220 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.286478 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.294770 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.295020 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.303265 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.303585 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.311990 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.312246 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.322654 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.322901 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.330919 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.331225 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.338878 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.339110 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.346839 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.347100 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.357353 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.357641 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.365794 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.366027 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.372972 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.373219 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.380871 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.381133 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.388765 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.389106 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.396994 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.397287 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.406507 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.406810 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.415729 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.416009 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.424765 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.425053 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.434480 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.434773 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.442881 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.443165 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.452101 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.452373 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.461242 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.461488 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.469131 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.469401 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.476870 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.477118 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.485023 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.485278 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.493045 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.493285 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.501156 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.501417 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.510915 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.511160 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.519889 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.520135 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.527997 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.528245 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.536752 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.537027 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.549241 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.549545 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.558761 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.559037 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.568668 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.568954 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.583859 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.584331 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.594668 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.594946 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.607237 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.608618 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.618980 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.619267 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.624265 140493874636672 run_squad.py:1259] Processing example: 8000\n",
            "I0629 20:08:52.628342 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.628633 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.638309 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.638723 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.647227 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.648364 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.659739 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.659997 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.669330 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.669597 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.681020 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.681301 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.690706 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.690981 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.698855 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.699178 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.709812 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.710069 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.717421 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.717684 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.726033 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.726290 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.735515 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.735988 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.744726 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.744997 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.753995 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.754254 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.763209 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.763474 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.771201 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.771431 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.780423 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.780791 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.792236 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.792595 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.802331 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.802681 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.811746 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.812071 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.821538 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.821893 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.830487 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.830821 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.840305 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.840690 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.848882 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.849178 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.859486 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.859833 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.867667 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.867942 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.875119 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.875399 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.883193 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.883462 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.890792 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.891055 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.898711 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.898998 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.907016 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.907456 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.918104 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.918406 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.926417 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.926724 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.934022 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.934281 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.941912 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.942213 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.949917 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.950224 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.957908 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.958228 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.967272 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.967616 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.975955 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.976273 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.984361 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.984698 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:52.992223 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:52.992512 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.003155 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.005815 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.014715 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.015014 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.023270 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.023549 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.030669 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.030917 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.038137 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.038378 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.046792 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.047063 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.054545 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.054844 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.063854 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.064189 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.072544 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.072819 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.080513 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.080790 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.089020 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.089298 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.097280 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.097722 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.111288 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.111758 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.123849 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.124218 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.133978 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.134344 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.143893 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.144220 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.152231 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.152539 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.160236 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.160602 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.198501 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.198931 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.207485 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.207972 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.216642 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.216925 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.225400 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.225713 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.233866 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.234158 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.241324 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.241633 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.249369 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.249684 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.257236 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.257601 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.266642 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.266975 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.275352 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.275860 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.291765 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.292229 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.303772 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.304242 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.313768 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.314111 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.322638 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.322947 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.330824 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.331151 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.341786 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.342116 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.350748 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.351055 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.358937 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.359242 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.367048 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.367360 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.375957 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.376274 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.385179 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.385581 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.394037 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.394360 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.403652 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.404078 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.413066 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.413390 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.425319 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.425684 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.435214 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.435668 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.447764 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.448115 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.456968 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.457319 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.472625 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.472948 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.482096 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.482406 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.490322 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.490668 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.498773 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.499412 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.507262 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.507598 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.515857 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.516149 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.524079 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.524378 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.532430 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.532745 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.540693 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.540996 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.549310 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.549671 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.557251 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.557586 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.566772 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.567092 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.575795 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.576127 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.584545 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.584936 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.593803 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.594148 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.603778 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.604125 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.612715 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.613047 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.621189 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.621504 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.629876 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.630214 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.639991 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.640327 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.649720 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.650056 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.657956 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.658279 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.665764 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.666063 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.673308 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.673637 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.681275 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.681615 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.690733 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.691058 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.698731 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.699049 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.706039 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.706308 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.713724 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.714018 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.721048 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.721329 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.729226 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.729508 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.736703 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.736997 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.745100 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.745346 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.752793 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.753037 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.763900 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.764216 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.775127 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.775421 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.784650 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.785277 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.793581 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.793848 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.798720 140493874636672 run_squad.py:1259] Processing example: 9000\n",
            "I0629 20:08:53.802500 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.802784 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.811254 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.811509 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.819425 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.819689 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.827761 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.828006 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.836345 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.836616 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.845927 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.846200 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.854669 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.854926 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.863401 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.863681 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.871196 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.871428 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.879533 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.879774 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.887028 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.887250 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.896469 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.896718 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.905764 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.906018 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.913766 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.913995 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.922357 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.922620 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.932026 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.932258 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.939579 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.939817 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.946919 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.947158 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.954066 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.954285 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.962157 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.962385 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.973217 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.973692 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.981805 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.982030 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.988902 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.989108 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:53.996045 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:53.996241 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.003582 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.003777 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.010520 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.010759 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.017002 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.017199 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.025018 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.025269 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.032719 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.032966 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.040508 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.040774 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.048904 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.049148 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.058114 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.058359 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.066631 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.066865 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.077261 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.077516 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.087203 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.087455 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.096223 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.096878 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.106371 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.106713 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.116782 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.117079 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.125917 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.126226 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.135934 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.136230 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.148020 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.148353 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.158101 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.158433 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.168059 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.168377 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.179120 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.179424 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.187701 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.188010 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.197290 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.197535 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.212119 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.212370 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.220187 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.220605 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.229138 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.229413 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.237821 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.238082 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.246267 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.246518 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.254750 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.254996 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.263818 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.264076 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.273486 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.273760 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.283232 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.283482 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.292025 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.292285 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.300734 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.300975 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.309537 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.309871 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.319166 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.319447 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.329587 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.329932 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.338776 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.339140 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.348188 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.348450 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.357299 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.357581 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.366435 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.366727 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.375605 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.376020 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.385635 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.385951 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.394076 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.394341 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.402584 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.402834 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.411186 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.411432 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.420222 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.420539 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.428411 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.428681 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.436201 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.436452 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.445139 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.445381 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.453138 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.453368 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.461508 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.461777 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.473113 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.473367 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.484343 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.484628 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.493007 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.493386 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.501444 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.501705 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.510513 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.510787 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.519396 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.519668 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.528206 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.528449 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.536186 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.536433 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.545090 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.545778 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.554345 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.554702 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.562700 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.562944 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.570860 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.571109 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.579782 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.580019 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.588796 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.589062 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.596544 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.596820 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.604866 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.605134 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.614062 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.614341 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.622952 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.623206 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.631335 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.631616 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.640366 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.640771 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.649094 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.649358 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.657359 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.657649 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.666338 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.666612 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.675386 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.675675 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.687213 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.687523 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.697391 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.697705 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.705833 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.706087 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.714988 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.715251 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.724068 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.724335 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.733258 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.733539 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.741855 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.742115 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.750298 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.750611 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.758687 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.758947 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.766931 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.767190 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.775909 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.776169 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.784280 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.784542 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.792830 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.793106 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.802051 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.802339 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.811189 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.811453 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.820025 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.820306 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.828820 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.829131 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.837284 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.837615 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.846760 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.847047 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.856066 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.856343 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.865745 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.866037 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.875904 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.876191 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.889000 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.889293 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.898114 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.898409 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.907633 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.907907 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.917677 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.918006 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.923327 140493874636672 run_squad.py:1259] Processing example: 10000\n",
            "I0629 20:08:54.927828 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.928172 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.937965 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.938234 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.948107 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.948383 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.957512 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.957792 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.966803 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.967486 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.976677 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.976964 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.986485 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.986748 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:54.994344 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:54.994627 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.002658 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.002925 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.010317 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.010598 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.018478 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.018759 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.026468 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.026750 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.034040 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.034305 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.042690 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.042940 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.052292 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.052544 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.060975 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.061232 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.069449 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.069731 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.076962 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.077227 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.085925 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.086228 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.096976 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.097214 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.105005 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.105258 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.112971 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.113219 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.122089 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.122371 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.130460 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.130730 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.139050 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.139322 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.147112 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.147363 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.155367 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.155645 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.163889 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.164169 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.172203 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.172505 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.181163 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.181433 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.189770 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.190019 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.197680 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.197957 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.207549 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.207822 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.217223 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.217509 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.226721 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.227085 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.236861 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.237209 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.246115 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.246465 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.256218 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.256637 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.268611 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.268896 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.277195 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.277477 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.286373 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.286740 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.298704 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.298995 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.308262 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.308547 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.317504 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.317829 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.327692 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.327986 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.337165 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.337594 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.348875 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.349246 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.359122 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.359469 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.371537 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.371945 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.382043 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.382387 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.392672 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.394619 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.406451 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.406847 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.417806 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.418321 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.426942 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.427214 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.435967 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.436227 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.444159 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.444432 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.452706 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.453004 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.461284 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.461573 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.469233 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.469486 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.477784 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.478063 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.485830 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.486121 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.494778 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.495159 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.512439 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.512770 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.521734 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.522012 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.531219 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.531475 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.544775 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.545045 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.553153 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.553432 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.562134 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.562408 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.570682 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.570951 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.580020 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.580290 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.589027 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.589287 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.597002 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.597293 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.605590 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.605861 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.615678 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.615933 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.624743 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.625068 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.634257 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.634618 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.643069 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.643402 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.651386 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.651740 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.659644 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.659955 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.678333 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.678715 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.687099 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.687369 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.693819 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.694311 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.701115 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.701353 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.707885 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.708112 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.713364 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.713629 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.719001 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.719250 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.724416 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.724677 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.729985 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.730254 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.735545 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.735826 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.741273 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.741529 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.747056 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.747217 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.754966 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.755226 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.769155 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.769424 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.781593 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.781856 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.792453 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.792768 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.805915 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.806169 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.817040 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.817304 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.827078 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.827325 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.837835 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.838097 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.848262 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.848511 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.859549 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.860035 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.870898 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.871209 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.883448 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.883738 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.894263 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.894526 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.907126 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.907435 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.917803 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.918050 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.928606 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.928925 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.938150 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.938399 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.948354 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.948637 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.958371 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.958683 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.970367 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.970669 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.981283 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.981547 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:55.993020 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:55.993260 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.006576 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.006842 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.018301 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.018586 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.028989 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.029247 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.038760 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.039007 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.049350 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.049619 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.061423 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.061779 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.072226 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.072474 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.086182 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.086422 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.096547 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.096997 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.108391 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.108705 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.118881 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.119152 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.129506 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.129774 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.136800 140493874636672 run_squad.py:1259] Processing example: 11000\n",
            "I0629 20:08:56.140172 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.140452 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.151197 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.151465 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.162125 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.162404 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.172713 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.172987 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.183431 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.183719 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.194815 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.195087 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.209941 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.210231 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.221737 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.222025 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.232344 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.232640 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.243541 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.243824 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.253472 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.253758 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.264618 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.264908 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.276406 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.276703 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.288519 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.288800 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.298860 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.299961 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.311038 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.311296 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.321982 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.322229 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.333276 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.333549 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.345011 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.345330 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.356010 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.356313 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.368973 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.369271 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.379616 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.379928 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.390842 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.391130 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.440045 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.440316 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.452720 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.453004 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.463577 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.463864 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.473865 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.474159 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.484429 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.484721 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.495431 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.495699 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.506606 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.506887 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.516690 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.516952 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.527819 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.528115 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.538224 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.538496 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.549619 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.549924 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.563076 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.563404 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.577823 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.578155 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.589653 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.589893 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.599019 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.599267 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.610482 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.610756 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.621190 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.621448 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.631988 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.632277 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.642728 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.642989 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.653388 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.653687 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.665110 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.665403 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.675804 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.676100 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.687263 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.687586 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.697861 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.698328 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.710078 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.710574 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.722757 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.723021 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.733103 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.733358 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.743837 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.744105 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.754400 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.754706 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.769053 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.769321 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.779610 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.779997 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.790521 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.790789 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.801134 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.801402 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.812335 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.812605 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.823350 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.823636 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.834578 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.834830 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.845751 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.846022 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.856625 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.856991 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.867944 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.868209 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.880341 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.880631 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.891279 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.891606 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.902671 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.902959 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.916753 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.917167 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.930272 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.930547 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.940788 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.941060 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.951188 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.951486 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.961402 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.961675 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.974795 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.975464 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.988172 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.988498 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:56.997765 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:56.998070 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.008984 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.009675 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.019659 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.019975 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.029640 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.029947 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.041040 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.041368 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.051101 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.051430 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.062772 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.063508 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.075433 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.075722 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.090297 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.090599 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.102901 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.103227 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.114737 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.115011 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.125400 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.125679 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.136754 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.137006 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.147861 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.148128 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.158682 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.158976 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.169757 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.170038 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.179764 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.180124 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.191288 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.191527 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.203523 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.204455 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.214902 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.215163 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.226044 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.226328 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.237627 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.237935 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.248272 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.248549 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.259796 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.260081 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.269946 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.270212 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.281058 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.281296 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.291448 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.291750 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.301711 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.301993 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.314635 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.314922 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.325712 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.326092 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.337089 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.337340 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.347289 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.347728 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.357886 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.358148 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.367963 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.368211 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.377783 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.378063 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.389656 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.389890 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.403637 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.403891 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.415076 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.415349 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.428480 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.428811 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.438816 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.439051 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.448512 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.448809 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.458669 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.458929 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.469407 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.469675 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.480397 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.480710 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.490948 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.491186 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.501411 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.501683 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.512483 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.512767 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.523477 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.523771 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.534249 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.534507 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.544832 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.545101 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.555942 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.556195 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.568082 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.568324 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.578399 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.578678 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.586187 140493874636672 run_squad.py:1259] Processing example: 12000\n",
            "I0629 20:08:57.589653 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.589889 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.601634 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.601902 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.612659 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.612917 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.627854 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.628141 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.638810 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.639071 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.649310 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.649588 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.659127 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.659382 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.669722 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.670005 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.681025 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.681299 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.691230 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.691486 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.701601 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.701840 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.716059 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.716305 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.726188 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.726446 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.736889 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.737128 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.747710 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.747948 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.758472 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.758866 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.769171 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.769409 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.779502 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.779761 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.789973 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.790207 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.800898 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.801136 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.812089 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.812440 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.825210 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.825467 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.836878 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.837121 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.847387 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.847641 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.857855 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.858129 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.868517 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.868777 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.879307 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.879780 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.893121 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.893407 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.904336 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.904957 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.915870 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.916117 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.926953 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.927215 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.938130 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.938372 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.949054 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.949311 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.959643 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.959900 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.970793 140493874636672 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0629 20:08:57.971045 140493874636672 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0629 20:08:57.978129 140493874636672 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 20:08:57.978369 140493874636672 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 20:08:57.978594 140492660819712 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 20:08:57.978700 140492660819712 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 20:08:57.978840 140493874636672 error_handling.py:96] infeed marked as finished\n",
            "I0629 20:08:57.978981 140493874636672 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 20:08:57.979080 140493874636672 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 20:08:57.979215 140492652427008 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 20:08:57.979298 140492652427008 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 20:08:57.979408 140493874636672 error_handling.py:96] outfeed marked as finished\n",
            "I0629 20:08:57.979530 140493874636672 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 20:08:58.370923 140493874636672 error_handling.py:96] prediction_loop marked as finished\n",
            "I0629 20:08:58.371184 140493874636672 error_handling.py:96] prediction_loop marked as finished\n",
            "I0629 20:08:58.371353 140493874636672 run_squad.py:745] Writing predictions to: gs://gurebert/gureBERT/wordpiece/squad/predictions.json\n",
            "I0629 20:08:58.371432 140493874636672 run_squad.py:746] Writing nbest to: gs://gurebert/gureBERT/wordpiece/squad/nbest_predictions.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUcKt7Ehn6lV",
        "colab_type": "text"
      },
      "source": [
        "# gureBERT (japanese) erabiliz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqPX8zgt6lAO",
        "colab_type": "code",
        "outputId": "2b3f9ffa-60e5-4e7a-a7ed-3e01aecc88a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "#################################################################\n",
        "# gureBERT japanese\n",
        "\n",
        "!cd /content/\n",
        "\n",
        "!git clone --recursive https://github.com/zmwebdev/bert-japanese\n",
        "%cd bert-japanese\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert-japanese'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)   \u001b[K\rremote: Counting objects:  22% (2/9)   \u001b[K\rremote: Counting objects:  33% (3/9)   \u001b[K\rremote: Counting objects:  44% (4/9)   \u001b[K\rremote: Counting objects:  55% (5/9)   \u001b[K\rremote: Counting objects:  66% (6/9)   \u001b[K\rremote: Counting objects:  77% (7/9)   \u001b[K\rremote: Counting objects:  88% (8/9)   \u001b[K\rremote: Counting objects: 100% (9/9)   \u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects:  12% (1/8)   \u001b[K\rremote: Compressing objects:  25% (2/8)   \u001b[K\rremote: Compressing objects:  37% (3/8)   \u001b[K\rremote: Compressing objects:  50% (4/8)   \u001b[K\rremote: Compressing objects:  62% (5/8)   \u001b[K\rremote: Compressing objects:  75% (6/8)   \u001b[K\rremote: Compressing objects:  87% (7/8)   \u001b[K\rremote: Compressing objects: 100% (8/8)   \u001b[K\rremote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "Receiving objects:   0% (1/170)   \rReceiving objects:   1% (2/170)   \rReceiving objects:   2% (4/170)   \rReceiving objects:   3% (6/170)   \rReceiving objects:   4% (7/170)   \rReceiving objects:   5% (9/170)   \rReceiving objects:   6% (11/170)   \rReceiving objects:   7% (12/170)   \rReceiving objects:   8% (14/170)   \rReceiving objects:   9% (16/170)   \rReceiving objects:  10% (17/170)   \rReceiving objects:  11% (19/170)   \rReceiving objects:  12% (21/170)   \rReceiving objects:  13% (23/170)   \rReceiving objects:  14% (24/170)   \rReceiving objects:  15% (26/170)   \rReceiving objects:  16% (28/170)   \rReceiving objects:  17% (29/170)   \rReceiving objects:  18% (31/170)   \rReceiving objects:  19% (33/170)   \rReceiving objects:  20% (34/170)   \rReceiving objects:  21% (36/170)   \rReceiving objects:  22% (38/170)   \rReceiving objects:  23% (40/170)   \rReceiving objects:  24% (41/170)   \rReceiving objects:  25% (43/170)   \rReceiving objects:  26% (45/170)   \rReceiving objects:  27% (46/170)   \rReceiving objects:  28% (48/170)   \rReceiving objects:  29% (50/170)   \rReceiving objects:  30% (51/170)   \rReceiving objects:  31% (53/170)   \rReceiving objects:  32% (55/170)   \rReceiving objects:  33% (57/170)   \rReceiving objects:  34% (58/170)   \rReceiving objects:  35% (60/170)   \rReceiving objects:  36% (62/170)   \rReceiving objects:  37% (63/170)   \rReceiving objects:  38% (65/170)   \rReceiving objects:  39% (67/170)   \rReceiving objects:  40% (68/170)   \rReceiving objects:  41% (70/170)   \rReceiving objects:  42% (72/170)   \rReceiving objects:  43% (74/170)   \rReceiving objects:  44% (75/170)   \rReceiving objects:  45% (77/170)   \rReceiving objects:  46% (79/170)   \rReceiving objects:  47% (80/170)   \rReceiving objects:  48% (82/170)   \rReceiving objects:  49% (84/170)   \rReceiving objects:  50% (85/170)   \rReceiving objects:  51% (87/170)   \rReceiving objects:  52% (89/170)   \rReceiving objects:  53% (91/170)   \rReceiving objects:  54% (92/170)   \rReceiving objects:  55% (94/170)   \rReceiving objects:  56% (96/170)   \rReceiving objects:  57% (97/170)   \rReceiving objects:  58% (99/170)   \rReceiving objects:  59% (101/170)   \rReceiving objects:  60% (102/170)   \rReceiving objects:  61% (104/170)   \rReceiving objects:  62% (106/170)   \rReceiving objects:  63% (108/170)   \rReceiving objects:  64% (109/170)   \rReceiving objects:  65% (111/170)   \rReceiving objects:  66% (113/170)   \rReceiving objects:  67% (114/170)   \rReceiving objects:  68% (116/170)   \rReceiving objects:  69% (118/170)   \rReceiving objects:  70% (119/170)   \rReceiving objects:  71% (121/170)   \rReceiving objects:  72% (123/170)   \rReceiving objects:  73% (125/170)   \rReceiving objects:  74% (126/170)   \rReceiving objects:  75% (128/170)   \rReceiving objects:  76% (130/170)   \rReceiving objects:  77% (131/170)   \rReceiving objects:  78% (133/170)   \rReceiving objects:  79% (135/170)   \rReceiving objects:  80% (136/170)   \rReceiving objects:  81% (138/170)   \rReceiving objects:  82% (140/170)   \rReceiving objects:  83% (142/170)   \rReceiving objects:  84% (143/170)   \rremote: Total 170 (delta 1), reused 6 (delta 1), pack-reused 161\u001b[K\n",
            "Receiving objects:  85% (145/170)   \rReceiving objects:  86% (147/170)   \rReceiving objects:  87% (148/170)   \rReceiving objects:  88% (150/170)   \rReceiving objects:  89% (152/170)   \rReceiving objects:  90% (153/170)   \rReceiving objects:  91% (155/170)   \rReceiving objects:  92% (157/170)   \rReceiving objects:  93% (159/170)   \rReceiving objects:  94% (160/170)   \rReceiving objects:  95% (162/170)   \rReceiving objects:  96% (164/170)   \rReceiving objects:  97% (165/170)   \rReceiving objects:  98% (167/170)   \rReceiving objects:  99% (169/170)   \rReceiving objects: 100% (170/170)   \rReceiving objects: 100% (170/170), 261.38 KiB | 3.27 MiB/s, done.\n",
            "Resolving deltas:   0% (0/92)   \rResolving deltas:   5% (5/92)   \rResolving deltas:   9% (9/92)   \rResolving deltas:  17% (16/92)   \rResolving deltas:  20% (19/92)   \rResolving deltas:  21% (20/92)   \rResolving deltas:  22% (21/92)   \rResolving deltas:  23% (22/92)   \rResolving deltas:  43% (40/92)   \rResolving deltas:  45% (42/92)   \rResolving deltas:  51% (47/92)   \rResolving deltas:  54% (50/92)   \rResolving deltas:  83% (77/92)   \rResolving deltas:  86% (80/92)   \rResolving deltas:  91% (84/92)   \rResolving deltas:  94% (87/92)   \rResolving deltas: 100% (92/92)   \rResolving deltas: 100% (92/92), done.\n",
            "Submodule 'bert' (https://github.com/google-research/bert.git) registered for path 'bert'\n",
            "Cloning into '/content/gureBERT/bert-japanese/bert'...\n",
            "remote: Enumerating objects: 329, done.        \n",
            "remote: Total 329 (delta 0), reused 0 (delta 0), pack-reused 329        \n",
            "Receiving objects: 100% (329/329), 239.27 KiB | 3.52 MiB/s, done.\n",
            "Resolving deltas: 100% (188/188), done.\n",
            "Submodule path 'bert': checked out 'f39e881b169b9d53bea03d2d341b31707a6c052b'\n",
            "/content/gureBERT/bert-japanese\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d68MKTOjQxVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrhObi5n8xYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/data-download-and-extract.py\n",
        "!bash src/file-preprocessing.sh\n",
        "\n",
        "#!gsutil cp -r gs://gurebert/gureBERT/data/wiki/AA/wiki_00 data/wiki/AA/wiki_00\n",
        "\n",
        "# Oharra: esaldiak lerro bakarrean jarri behar dira\n",
        "#!cat data/wiki/AA/wiki_00"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m0o4LUCJ8ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wiki-eu.vocab eta wiki-eu.model sortzen ditu\n",
        "\n",
        "!pip install sentencepiece\n",
        "!python3 src/train-sentencepiece.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJFc3jAkOyHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp -r data/ gs://gurebert/gureBERT/\n",
        "!gsutil cp -r model/ gs://gurebert/gureBERT/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_qxPQAgP1sA",
        "colab_type": "code",
        "outputId": "358ed8f7-dd3c-4ef6-aee1-74edd53a49ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "!mkdir model\n",
        "!gsutil cp gs://gurebert/gureBERT/model/wiki-eu.vocab model\n",
        "!gsutil cp gs://gurebert/gureBERT/model/wiki-eu.model model\n",
        "!head -n 50 model/wiki-eu.vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>\t0\n",
            "<s>\t0\n",
            "</s>\t0\n",
            "[PAD]\t0\n",
            "[CLS]\t0\n",
            "[SEP]\t0\n",
            "[MASK]\t0\n",
            ",\t-2.87014\n",
            ".\t-2.98912\n",
            "▁eta\t-3.5406\n",
            "▁ziren\t-4.15683\n",
            "-\t-4.26943\n",
            "an\t-4.4068\n",
            "▁da\t-4.40907\n",
            "a\t-4.47601\n",
            "▁zen\t-4.51427\n",
            "ko\t-4.52925\n",
            "▁\t-4.67755\n",
            "▁zeuden\t-4.76555\n",
            "▁bat\t-4.84966\n",
            "▁\"\t-4.95145\n",
            "\"\t-4.95743\n",
            "▁1\t-5.04005\n",
            "ak\t-5.2203\n",
            "▁izan\t-5.32102\n",
            "▁zuen\t-5.34175\n",
            "en\t-5.36722\n",
            "ren\t-5.38606\n",
            ":\t-5.53342\n",
            "k\t-5.56319\n",
            "▁(\t-5.58916\n",
            "▁zituzten\t-5.58963\n",
            "▁zuten\t-5.59859\n",
            "▁etxek\t-5.66741\n",
            ")\t-5.66971\n",
            "▁du\t-5.70751\n",
            "▁zituen\t-5.72891\n",
            "▁enpresak\t-5.73378\n",
            "▁bere\t-5.7694\n",
            "▁2\t-5.78944\n",
            "▁dago\t-5.79301\n",
            "▁2009\t-5.8295\n",
            "▁ere\t-5.84878\n",
            "▁2007\t-5.87422\n",
            "n\t-5.87535\n",
            "▁pertsona\t-5.88342\n",
            "▁biztanle\t-5.93646\n",
            "aren\t-5.97756\n",
            "▁dira\t-5.97904\n",
            "▁ditu\t-6.0186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl457PDf8r-y",
        "colab_type": "code",
        "outputId": "320787c0-4f0e-4d29-f1af-3adecece4172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#https://github.com/zmwebdev/bert-japanese/blob/master/notebook/check-trained-tokenizer.ipynb\n",
        "!pip install sentencepiece\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "import tokenization_sentencepiece as tokenization\n",
        "\n",
        "text1 = \"Nere kotxea aitonaren etxe alboan dago\"\n",
        "text2 = \"Gorria da gure etxearen inguruan dagoen lorearen kolorea\"\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    model_file=\"model/wiki-eu.model\",\n",
        "    vocab_file=\"model/wiki-eu.vocab\",\n",
        "    do_lower_case=True)\n",
        "\n",
        "tokenizer.tokenize(text1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.82)\n",
            "Loaded a trained SentencePiece model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁nere', '▁kotxe', 'a', '▁aitona', 'ren', '▁etxe', '▁alboan', '▁dago']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ExyRkNqGpvi",
        "colab_type": "code",
        "outputId": "96fb8001-d24d-47ea-ae3b-dc80469fca0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "tokenizer.tokenize(text2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁gorria',\n",
              " '▁da',\n",
              " '▁gure',\n",
              " '▁etxearen',\n",
              " '▁inguruan',\n",
              " '▁dagoen',\n",
              " '▁lore',\n",
              " 'aren',\n",
              " '▁kolorea']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u9yUjU0HHwx",
        "colab_type": "code",
        "outputId": "e63d3131-22f9-47e5-8437-1527aac1446f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#do_lower_case=False jarrita tokenizazioa okerragoa da\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    model_file=\"model/wiki-eu.model\",\n",
        "    vocab_file=\"model/wiki-eu.vocab\",\n",
        "    do_lower_case=False)\n",
        "\n",
        "tokenizer.tokenize(text1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁', 'N', 'ere', '▁kotxe', 'a', '▁aitona', 'ren', '▁etxe', '▁alboan', '▁dago']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZMfqyxoHZnG",
        "colab_type": "code",
        "outputId": "d0cf6574-bcd8-4e35-e295-d0fee84fe64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "tokenizer.tokenize(text2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁',\n",
              " 'G',\n",
              " 'orri',\n",
              " 'a',\n",
              " '▁da',\n",
              " '▁gure',\n",
              " '▁etxearen',\n",
              " '▁inguruan',\n",
              " '▁dagoen',\n",
              " '▁lore',\n",
              " 'aren',\n",
              " '▁kolorea']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wosrOAdz2gQs",
        "colab_type": "code",
        "outputId": "d85a3585-d40c-43f9-930b-13475e0b2c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "# convert to WordPiece (kodean errorea dago). begiratu https://github.com/kwonmha/bert-vocab-builder/pull/4#issue-291306156\n",
        "\n",
        "!git clone https://github.com/kwonmha/bert-vocab-builder.git\n",
        "\n",
        "!python bert-vocab-builder/subword_builder.py \\\n",
        "--corpus_filepattern model/wiki-eu.vocab \\\n",
        "--output_filename model/wiki-eu-wordpiece.vocab\n",
        "#--min_count {minimum_subtoken_counts}\n",
        "\n",
        "# https://github.com/kwonmha/bert-vocab-builder/pull/4#issue-291306156"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'bert-vocab-builder' already exists and is not an empty directory.\n",
            "Traceback (most recent call last):\n",
            "  File \"bert-vocab-builder/subword_builder.py\", line 33, in <module>\n",
            "    import text_encoder\n",
            "  File \"/content/bert-japanese/bert-vocab-builder/text_encoder.py\", line 647\n",
            "    <<<<<<< HEAD\n",
            "     ^\n",
            "SyntaxError: invalid syntax\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBDprGbXRt4l",
        "colab_type": "code",
        "outputId": "1093ee9f-3fc7-48f5-96de-55fe03096bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "#Creating data for pretraining\n",
        "#Create .tfrecord files for pretraining. For longer sentence data, replace the value of max_seq_length with 512.\n",
        "!cat creating_data_for_pretraining.sh\n",
        "!bash creating_data_for_pretraining.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for DIR in $( find data/wiki/ -mindepth 1 -type d ); do \n",
            "  python3 src/create_pretraining_data.py \\\n",
            "    --input_file=${DIR}/all.txt \\\n",
            "    --output_file=${DIR}/all-maxseq128.tfrecord \\\n",
            "    --model_file=./model/wiki-eu.model \\\n",
            "    --vocab_file=./model/wiki-eu.vocab \\\n",
            "    --do_lower_case=True \\\n",
            "    --max_seq_length=128 \\\n",
            "    --max_predictions_per_seq=20 \\\n",
            "    --masked_lm_prob=0.15 \\\n",
            "    --random_seed=12345 \\\n",
            "    --dupe_factor=5\n",
            "done"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlBYBR1_G9Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy to gs\n",
        "!gsutil cp -r data gs://gurebert/gureBERT\n",
        "!gsutil cp -r model gs://gurebert/gureBERT\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29sDsZvBgFLj",
        "colab_type": "code",
        "outputId": "d69c0256-6298-492b-9d3e-b1fa8e086803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# Pre-Training\n",
        "# https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/pretraining.ipynb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "#Check TPU devices\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.16.115.170:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 5460104680638402674),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16051411686286296001),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1933247979704415977),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11046566402910946519),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13421296677076221120),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5352660506872180047),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7874427115731877787),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15986719012258325089),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7612787266547945911),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6745551487070946188),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14981037038192380193)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AzYwFXw8iJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DATA_GCS = 'gs://gurebert/gureBERT/data/wiki'\n",
        "TARGET_DIRS = [\n",
        "  'AA',\n",
        "  'AB',\n",
        "  'AC'\n",
        "]\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "#MAX_SEQ_LEN = 512\n",
        "\n",
        "\n",
        "INPUT_FILE = ','.join( [ '{}/{}/all-maxseq{}.tfrecord'.format(INPUT_DATA_GCS, elem, MAX_SEQ_LEN) for elem in TARGET_DIRS] )\n",
        "\n",
        "OUTPUT_GCS = 'gs://gurebert/gureBERT/model'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRRjRvl19NDV",
        "colab_type": "code",
        "outputId": "7135ff35-5f86-46c3-afe0-5f4e4312dddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Adding whole word masking aldaketa egin dute BERT-en. update egin behar da!!!\n",
        "# Begiratu https://github.com/google-research/bert/commit/0fce551b55caabcfba52c61e18f34b541aef186a\n",
        "\n",
        "!python src/run_pretraining.py \\\n",
        "  --input_file={INPUT_FILE} \\\n",
        "  --output_dir={OUTPUT_GCS} \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  --num_tpu_cores=8 \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length={MAX_SEQ_LEN} \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=1400000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0625 19:38:15.208214 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0625 19:38:15.209448 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:497: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0625 19:38:15.210220 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:412: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0625 19:38:15.210398 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:412: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0625 19:38:15.210564 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0625 19:38:15.211427 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:419: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0625 19:38:16.644103 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:423: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0625 19:38:17.434032 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:425: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0625 19:38:17.434276 140282338469760 run_pretraining.py:425] *** Input Files ***\n",
            "I0625 19:38:17.434366 140282338469760 run_pretraining.py:427]   gs://gurebert/gureBERT/data/wiki/AA/all-maxseq128.tfrecord\n",
            "I0625 19:38:17.434440 140282338469760 run_pretraining.py:427]   gs://gurebert/gureBERT/data/wiki/AB/all-maxseq128.tfrecord\n",
            "I0625 19:38:17.434512 140282338469760 run_pretraining.py:427]   gs://gurebert/gureBERT/data/wiki/AC/all-maxseq128.tfrecord\n",
            "W0625 19:38:18.462627 140282338469760 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0625 19:38:19.468373 140282338469760 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f95d11146a8>) includes params argument, but params are not passed to Estimator.\n",
            "I0625 19:38:19.469875 140282338469760 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.16.115.170:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f95d10b3588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.16.115.170:8470', '_evaluation_master': 'grpc://10.16.115.170:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f95dd593be0>}\n",
            "I0625 19:38:19.470217 140282338469760 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0625 19:38:19.470873 140282338469760 run_pretraining.py:464] ***** Running training *****\n",
            "I0625 19:38:19.470968 140282338469760 run_pretraining.py:465]   Batch size = 64\n",
            "I0625 19:38:23.258418 140282338469760 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.16.115.170:8470) for TPU system metadata.\n",
            "2019-06-25 19:38:23.259746: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0625 19:38:23.274165 140282338469760 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0625 19:38:23.274393 140282338469760 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0625 19:38:23.274487 140282338469760 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0625 19:38:23.274553 140282338469760 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0625 19:38:23.274617 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5460104680638402674)\n",
            "I0625 19:38:23.275380 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1933247979704415977)\n",
            "I0625 19:38:23.275457 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11046566402910946519)\n",
            "I0625 19:38:23.275525 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13421296677076221120)\n",
            "I0625 19:38:23.275590 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5352660506872180047)\n",
            "I0625 19:38:23.275657 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7874427115731877787)\n",
            "I0625 19:38:23.275721 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15986719012258325089)\n",
            "I0625 19:38:23.275784 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7612787266547945911)\n",
            "I0625 19:38:23.275844 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6745551487070946188)\n",
            "I0625 19:38:23.275904 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14981037038192380193)\n",
            "I0625 19:38:23.275966 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16051411686286296001)\n",
            "W0625 19:38:23.282766 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0625 19:38:23.299888 140282338469760 estimator.py:1145] Calling model_fn.\n",
            "W0625 19:38:23.300519 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:342: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0625 19:38:23.306573 140282338469760 deprecation.py:323] From src/run_pretraining.py:373: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0625 19:38:23.306786 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0625 19:38:23.335085 140282338469760 deprecation.py:323] From src/run_pretraining.py:390: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0625 19:38:23.335338 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0625 19:38:23.336897 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:398: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0625 19:38:23.342769 140282338469760 deprecation.py:323] From src/run_pretraining.py:405: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0625 19:38:23.426860 140282338469760 run_pretraining.py:122] *** Features ***\n",
            "I0625 19:38:23.427149 140282338469760 run_pretraining.py:124]   name = input_ids, shape = (8, 128)\n",
            "I0625 19:38:23.427274 140282338469760 run_pretraining.py:124]   name = input_mask, shape = (8, 128)\n",
            "I0625 19:38:23.427362 140282338469760 run_pretraining.py:124]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0625 19:38:23.427445 140282338469760 run_pretraining.py:124]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0625 19:38:23.427529 140282338469760 run_pretraining.py:124]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0625 19:38:23.427610 140282338469760 run_pretraining.py:124]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0625 19:38:23.427688 140282338469760 run_pretraining.py:124]   name = segment_ids, shape = (8, 128)\n",
            "W0625 19:38:23.427910 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:172: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0625 19:38:23.430187 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:411: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0625 19:38:23.466429 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:492: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0625 19:38:23.524997 140282338469760 deprecation.py:506] From /content/bert-japanese/src/../bert/modeling.py:359: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0625 19:38:23.546822 140282338469760 deprecation.py:323] From /content/bert-japanese/src/../bert/modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0625 19:38:27.546400 140282338469760 run_pretraining.py:172] **** Trainable Variables ****\n",
            "I0625 19:38:27.546652 140282338469760 run_pretraining.py:178]   name = bert/embeddings/word_embeddings:0, shape = (32000, 768)\n",
            "I0625 19:38:27.546783 140282338469760 run_pretraining.py:178]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0625 19:38:27.546877 140282338469760 run_pretraining.py:178]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0625 19:38:27.546964 140282338469760 run_pretraining.py:178]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.547065 140282338469760 run_pretraining.py:178]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.547148 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547240 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547317 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547397 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547473 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547553 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547628 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547708 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547784 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.547859 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.547933 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.548025 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.548103 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.548182 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548265 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.548341 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.548416 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548494 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548570 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548649 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548723 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548801 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548876 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548953 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.549039 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.549117 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.549198 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.549278 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.549353 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.549464 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.549555 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.549630 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.549705 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.549783 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.549859 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.549938 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550027 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.550108 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550183 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.550268 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550342 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.550415 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.550496 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.550579 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.550655 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.550734 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550811 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.550886 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.550960 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551053 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551129 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551213 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551289 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551367 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551442 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551520 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551597 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.551673 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.551748 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.551826 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.551902 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.551980 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552069 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.552144 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.552223 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552304 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552379 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552458 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552532 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552613 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552688 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552767 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552842 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.552917 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.552991 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.553102 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.553179 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.553265 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553341 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.553414 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.553490 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.553569 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553642 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.553722 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553798 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.553876 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553951 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.554041 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.554118 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.554199 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.554275 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.554352 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.554429 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.554506 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.554581 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.554655 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.554730 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.554809 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.554883 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.554962 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555051 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.555130 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555211 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.555290 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555366 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.555443 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.555517 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.555596 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.555672 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.555750 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555824 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.555898 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.555972 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.556064 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.556140 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.556225 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.556300 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.586935 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.587312 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.587467 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.587575 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.587675 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.587777 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.587887 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.587991 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.588119 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.588230 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.588329 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.588429 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.588537 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.588637 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.588743 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.588844 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.588951 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.589072 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.589181 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.589300 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.589403 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.589504 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.589615 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.589721 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.589831 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.589935 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.590058 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.590166 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590286 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.590391 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590503 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.590605 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590713 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.590820 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590930 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.591052 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.591159 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.591277 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.591384 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.591493 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.591601 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.591722 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.591833 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.591942 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592070 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592176 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592298 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592424 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592534 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592635 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592743 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592846 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.592947 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.593068 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.593182 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.593296 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.593404 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.593508 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.593610 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.593711 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.593819 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.593922 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.594047 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.594153 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.594272 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.594377 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.594484 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.594584 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.594686 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.594788 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.594895 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.594999 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.595129 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.595242 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.595343 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.595443 140282338469760 run_pretraining.py:178]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.595552 140282338469760 run_pretraining.py:178]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.595655 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.595763 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.595866 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.595969 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.596090 140282338469760 run_pretraining.py:178]   name = cls/predictions/output_bias:0, shape = (32000,)\n",
            "I0625 19:38:27.596205 140282338469760 run_pretraining.py:178]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0625 19:38:27.596317 140282338469760 run_pretraining.py:178]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0625 19:38:27.596510 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0625 19:38:27.598321 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0625 19:38:27.606181 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0625 19:38:32.398603 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0625 19:38:41.775387 140282338469760 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0625 19:38:42.164427 140282338469760 estimator.py:1147] Done calling model_fn.\n",
            "I0625 19:38:46.161324 140282338469760 tpu_estimator.py:499] TPU job name worker\n",
            "I0625 19:38:47.572438 140282338469760 monitored_session.py:240] Graph was finalized.\n",
            "W0625 19:38:47.744163 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0625 19:38:47.907891 140282338469760 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/model/model.ckpt-610000\n",
            "W0625 19:39:14.770554 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0625 19:39:17.352350 140282338469760 session_manager.py:500] Running local_init_op.\n",
            "I0625 19:39:17.997242 140282338469760 session_manager.py:502] Done running local_init_op.\n",
            "I0625 19:39:29.001356 140282338469760 basic_session_run_hooks.py:606] Saving checkpoints for 610000 into gs://gurebert/gureBERT/model/model.ckpt.\n",
            "W0625 19:39:56.630694 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0625 19:39:58.038233 140282338469760 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0625 19:39:58.039453 140282338469760 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-06-25 19:39:58.039894: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0625 19:39:58.044787 140282338469760 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0625 19:39:58.047351 140282338469760 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0625 19:39:58.051084 140282338469760 tpu_estimator.py:557] Init TPU system\n",
            "I0625 19:40:05.458847 140282338469760 tpu_estimator.py:566] Initialized TPU in 7 seconds\n",
            "I0625 19:40:05.459836 140281195702016 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0625 19:40:05.460223 140281178138368 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0625 19:40:06.142425 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:40:06.143549 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:40:40.743851 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0625 19:41:40.791741 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (0, 892)\n",
            "I0625 19:41:49.880066 140282338469760 basic_session_run_hooks.py:262] loss = 0.6504045, step = 611000\n",
            "I0625 19:41:49.883220 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:41:49.883571 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:42:40.830728 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (1, 646)\n",
            "I0625 19:43:05.396509 140282338469760 basic_session_run_hooks.py:260] loss = 0.50328124, step = 612000 (75.516 sec)\n",
            "I0625 19:43:05.398332 140282338469760 tpu_estimator.py:2159] global_step/sec: 13.2422\n",
            "I0625 19:43:05.399384 140282338469760 tpu_estimator.py:2160] examples/sec: 847.5\n",
            "I0625 19:43:05.400921 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:43:05.401206 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:43:40.897238 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (2, 509)\n",
            "I0625 19:44:14.648333 140282338469760 basic_session_run_hooks.py:260] loss = 1.0970724, step = 613000 (69.252 sec)\n",
            "I0625 19:44:14.650186 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4401\n",
            "I0625 19:44:14.650439 140282338469760 tpu_estimator.py:2160] examples/sec: 924.163\n",
            "I0625 19:44:15.609725 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:44:15.610541 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:44:40.963851 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (3, 359)\n",
            "I0625 19:45:24.749946 140282338469760 basic_session_run_hooks.py:260] loss = 1.6517268, step = 614000 (70.102 sec)\n",
            "I0625 19:45:24.751514 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.2651\n",
            "I0625 19:45:24.751736 140282338469760 tpu_estimator.py:2160] examples/sec: 912.964\n",
            "I0625 19:45:24.752844 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:45:24.753101 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:45:40.979294 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (4, 223)\n",
            "I0625 19:46:34.015049 140282338469760 basic_session_run_hooks.py:260] loss = 1.376927, step = 615000 (69.265 sec)\n",
            "I0625 19:46:34.016614 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4373\n",
            "I0625 19:46:34.016878 140282338469760 tpu_estimator.py:2160] examples/sec: 923.986\n",
            "I0625 19:46:34.874679 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:46:34.875390 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:46:40.998494 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (5, 73)\n",
            "I0625 19:47:41.048521 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (5, 965)\n",
            "I0625 19:47:44.127715 140282338469760 basic_session_run_hooks.py:260] loss = 1.4815391, step = 616000 (70.113 sec)\n",
            "I0625 19:47:44.129233 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.2628\n",
            "I0625 19:47:44.129462 140282338469760 tpu_estimator.py:2160] examples/sec: 912.818\n",
            "I0625 19:47:44.130714 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:47:44.130944 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:48:41.099297 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (6, 829)\n",
            "I0625 19:48:54.730155 140282338469760 basic_session_run_hooks.py:260] loss = 0.98817366, step = 617000 (70.602 sec)\n",
            "I0625 19:48:54.731993 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.1637\n",
            "I0625 19:48:54.732534 140282338469760 tpu_estimator.py:2160] examples/sec: 906.479\n",
            "I0625 19:48:54.733717 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:48:54.733918 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:49:41.108434 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (7, 671)\n",
            "I0625 19:50:03.977603 140282338469760 basic_session_run_hooks.py:260] loss = 1.0366956, step = 618000 (69.247 sec)\n",
            "I0625 19:50:03.979490 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.441\n",
            "I0625 19:50:03.979758 140282338469760 tpu_estimator.py:2160] examples/sec: 924.222\n",
            "I0625 19:50:03.981185 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:50:03.981428 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:50:41.152918 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (8, 534)\n",
            "I0625 19:51:13.211040 140282338469760 basic_session_run_hooks.py:260] loss = 0.9602766, step = 619000 (69.233 sec)\n",
            "I0625 19:51:13.215205 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4434\n",
            "I0625 19:51:13.215537 140282338469760 tpu_estimator.py:2160] examples/sec: 924.379\n",
            "I0625 19:51:14.192095 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:51:14.192481 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:51:41.192183 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (9, 383)\n",
            "I0625 19:52:23.419371 140282338469760 basic_session_run_hooks.py:606] Saving checkpoints for 620000 into gs://gurebert/gureBERT/model/model.ckpt.\n",
            "W0625 19:52:46.765204 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "I0625 19:52:51.234167 140282338469760 basic_session_run_hooks.py:260] loss = 0.8747845, step = 620000 (98.023 sec)\n",
            "I0625 19:52:51.235810 140282338469760 tpu_estimator.py:2159] global_step/sec: 10.2019\n",
            "I0625 19:52:51.236226 140282338469760 tpu_estimator.py:2160] examples/sec: 652.923\n",
            "I0625 19:52:51.237428 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:52:51.237631 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:52:52.456125 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (10, 0)\n",
            "I0625 19:53:52.494190 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (10, 892)\n",
            "I0625 19:54:00.463160 140282338469760 basic_session_run_hooks.py:260] loss = 1.1868722, step = 621000 (69.229 sec)\n",
            "I0625 19:54:00.464810 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4448\n",
            "I0625 19:54:02.102097 140282338469760 tpu_estimator.py:2160] examples/sec: 924.469\n",
            "I0625 19:54:02.104723 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:54:02.105079 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:54:52.541748 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (11, 731)\n",
            "I0625 19:55:11.311046 140282338469760 basic_session_run_hooks.py:260] loss = 0.7853336, step = 622000 (70.848 sec)\n",
            "I0625 19:55:11.313117 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.1147\n",
            "I0625 19:55:11.313430 140282338469760 tpu_estimator.py:2160] examples/sec: 903.339\n",
            "I0625 19:55:11.314776 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:55:11.315082 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:55:52.561795 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (12, 595)\n",
            "I0625 19:56:21.423934 140282338469760 basic_session_run_hooks.py:260] loss = 1.150667, step = 623000 (70.113 sec)\n",
            "I0625 19:56:21.425779 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.2628\n",
            "I0625 19:56:21.426093 140282338469760 tpu_estimator.py:2160] examples/sec: 912.816\n",
            "I0625 19:56:21.427527 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:56:21.427800 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MGl6QPM4B9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################\n",
        "##########################################\n",
        "\n",
        "# BERT-ekin konpatiblea den vocab sortzeko prozedura. wordpiece _ -> ##. \n",
        "# Honen abantaila BERT kodearen aldaketak eta run_squad.py, ... ezer aldatu gabe ibiliko dela da.\n",
        "# berez ez da wordpiece, sentencepiece baizik baina sintaktikoki konpatiblea\n",
        "# ikusi: https://colab.research.google.com/drive/1-uLyGTnz2K4gx3su6Qc00in9QCa4b5Kh#scrollTo=9S4CiOh3RzFW\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "559f9136-b061-48b5-fc01-bac5a987c93f",
        "id": "psBPQwy2y8UO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format('model/wiki-eu'))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 31999\n",
            "Sample tokens: ['.036', '▁ukuilu', 'lok', '▁ahaltsua', '▁batzuei', '▁erabili', '▁chapman', '▁tv', '▁pe', '▁polemika']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mia3KTjty8Vp",
        "colab": {}
      },
      "source": [
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "63bd9984-804d-47ea-dbff-1dc6aa131f45",
        "id": "9LbsNuney8V-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab\n",
        "\n",
        "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(32000 - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jxBd_J30y8WV",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c9f23d61-8211-4ac2-cef4-f963e7f8db16",
        "id": "OgRdyQpEy8Wh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from bert import tokenization\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(\"Nere kotxea aitonaren etxe alboan dago\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nere', 'kotxe', '##a', 'aitona', '##ren', 'etxe', 'alboan', 'dago']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bfd2e057-bd2b-4704-8cf8-ada627a37600",
        "id": "dAZRoG-Jy8W1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#!gsutil cp -r gs://gurebert/gureBERT/data/wiki/AA/wiki_00 wiki_00\n",
        "# ez du wiki-eu.model behar, vocab.txt-ekin nahikoa da\n",
        "\n",
        "!python bert/create_pretraining_data.py \\\n",
        "  --input_file=wiki_00 \\\n",
        "  --output_file=/tmp/tf_examples.tfrecord \\\n",
        "  --vocab_file=vocab.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=128 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=12345 \\\n",
        "  --dupe_factor=5\n",
        "\n",
        "#############################################################\n",
        "#############################################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf_examples.tfrecord  tmpcqwLhN\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}