# -*- coding: utf-8 -*-
"""gureBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kfRQTGpfmRdq4ps7QsJ6cM8ysD5PKuzA

#gureBERT
"""

# gureBERT

!git clone --recursive  https://github.com/zmwebdev/gureBERT
# %cd gureBERT

!pip install sentencepiece
!install -d spModels
# eu
!python src/sentence-split.py --config eu.config.ini --do_lower_case 
!python src/train-sentencepiece.py --config eu.config.ini

!head -n 100 spModels/eu.vocab

# en-eu
!python src/sentence-split.py --config en-eu.config.ini --do_lower_case 
!python src/train-sentencepiece.py --config en-eu.config.ini

!head -n 100 spModels/en-eu.vocab

import sys
sys.path.append("src")

import tokenization_sentencepiece as tokenization

tokenizer = tokenization.FullTokenizer(
    model_file="spModels/eu.model",
    vocab_file="spModels/eu.vocab",
    do_lower_case=True)

text1 = "Nere kotxea aitonaren etxe alboan dago, bere kolorea gorria da."

tokenizer.tokenize(text1)

tokenizer_en_eu = tokenization.FullTokenizer(
    model_file="spModels/en-eu.model",
    vocab_file="spModels/en-eu.vocab",
    do_lower_case=True)

text1 = "Nere kotxea aitonaren etxe alboan dago, bere kolorea gorria da."

tokenizer_en_eu.tokenize(text1)

text1 = "The Italian cities of Milan and Cortina d'Ampezzo are chosen as the joint hosts of the 2026 Winter Olympics and Winter Paralympics."

print("EN-EU: {}".format(len(tokenizer_en_eu.tokenize(text1))))
print("EU: {}".format(len(tokenizer.tokenize(text1))))

from google.colab import auth
auth.authenticate_user()

# Pre-Training
# https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/pretraining.ipynb

import tensorflow as tf

from google.colab import auth
auth.authenticate_user()

#Check TPU devices

import datetime
import json
import os
import pprint
import random
import string
import sys
import tensorflow as tf

assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'
TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']
print('TPU address is', TPU_ADDRESS)

with tf.Session(TPU_ADDRESS) as session:
  print('TPU devices:')
  pprint.pprint(session.list_devices())

  # Upload credentials to TPU.
  with open('/content/adc.json', 'r') as f:
    auth_info = json.load(f)
  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)
  # Now credentials are set for all future sessions on this TPU.

GS = 'gs://gurebert/gureBERT'
  
#!gsutil cp -r spModels $GS/
#!gsutil cp -r corpus $GS/

!python3 src/create_pretraining_data.py \
    --input_file=$GS/corpus/eu/2014wiki.eu.sent_splited \
    --output_file=$GS/pretraining.tf.data \
    --model_file=spModels/eu.model \
    --vocab_file=spModels/eu.vocab \
    --do_lower_case=True

#!install -d gureBERT

!python src/run_pretraining.py \
  --config_file eu.congif.ini \
  --input_file=$GS/pretraining.tf.data \
  --output_dir=$GS/eu.gureBERT \
  --do_train=True \
  --do_eval=True \
  --train_batch_size=64 \
  --max_seq_length=512 \
  --max_predictions_per_seq=20 \
  --num_train_steps=10000 \
  --num_warmup_steps=10000 \
  --save_checkpoints_steps=10000 \
  --learning_rate=1e-4 \
  --use_tpu=True \
  --tpu_name={TPU_ADDRESS} \
#  --num_train_steps=1000000 \

# pre training SQuAD

# https://github.com/google-research/bert#squad-20

#!git clone https://github.com/zmwebdev/bert.git
#%cd bert

!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json
!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json
# evaluation script: download file from a url that returns a save dialog box : https://superuser.com/questions/795265/download-file-from-a-url-that-returns-a-save-dialog-box#795269
!wget --content-disposition https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/

# sentenpiece tokenizer  erabili behar da!!

!python bert/run_squad.py \
  --vocab_file=./spModels/eu.vocab \
  --bert_config_file=bert_config.json \
  --do_lower_case=True \
  --do_train=True \
  --train_file=train-v2.0.json \
  --do_predict=True \
  --predict_file=dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=0.1 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://gurebert/gureBERT/squad/ \
  --use_tpu=True \
  --tpu_name=$TPU_NAME \
  --version_2_with_negative=True \
  #--init_checkpoint=$BERT_DIR \

"""## wordpiece erabiliz

- https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379
- https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt
"""

import random

def read_sentencepiece_vocab(filepath):
  voc = []
  with open(filepath, encoding='utf-8') as fi:
    for line in fi:
      voc.append(line.split("\t")[0])
  # skip the first <unk> token
  voc = voc[1:]
  return voc

snt_vocab = read_sentencepiece_vocab("{}.vocab".format('./spModels/eu'))
print("Learnt vocab size: {}".format(len(snt_vocab)))
print("Sample tokens: {}".format(random.sample(snt_vocab, 10)))

# hau errepasatu. adibidez @ ez dut ondo egiten... [UNK] agertzen da...
# adibidez, 

#import string
#def parse_sentencepiece_token(token):
#    if token.startswith("▁"):
#        return token[1:]
#    else:
#        if token in string.punctuation:
#            return token
#        else:
#            return "##" + token
          

def parse_sentencepiece_token(token):
    if token.startswith("▁"):
        return token[1:]
    else:
        return "##" + token

import string

bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))

ctrl_symbols = ["[PAD]","[UNK]","[CLS]","[SEP]","[MASK]"]
# puntuazio sinboloak gehitu 
ctrl_symbols_end = list(string.punctuation)
#ctrl_symbols = ["[UNK]"]
bert_vocab = ctrl_symbols + bert_vocab + ctrl_symbols_end

#bert_vocab += ["[UNUSED_{}]".format(i) for i in range(32000 - len(bert_vocab))]
print(len(bert_vocab))

VOC_FNAME = "vocab.txt"
with open(VOC_FNAME, "w") as fo:
  for token in bert_vocab:
    fo.write(token+"\n")

from bert import tokenization

bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)
bert_tokenizer.tokenize("Nere kotxea aitonaren etxe alboan dago!, eta bere kolorea gorria da. ni@ni.eus erabili ")

!python bert/create_pretraining_data.py \
  --input_file=$GS/corpus/eu/2014wiki.eu.sent_splited \
  --output_file=$GS/wordpiece/pretraining.tf.data \
  --vocab_file=vocab.txt \
  --do_lower_case=True \
  --max_seq_length=512 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=666 \
  --do_whole_word_mask=True \
  #--dupe_factor=5

'''
!python src/run_pretraining.py \
  --config_file eu.congif.ini \
  --input_file=$GS/pretraining.tf.data \
  --output_dir=$GS/eu.gureBERT \
  --do_train=True \
  --do_eval=True \
  --train_batch_size=64 \
  --max_seq_length=512 \
  --max_predictions_per_seq=20 \
  --num_train_steps=1000000 \
  --num_warmup_steps=10000 \
  --save_checkpoints_steps=10000 \
  --learning_rate=1e-4 \
  --use_tpu=True \
  --tpu_name={TPU_ADDRESS} \
'''

!python bert/run_pretraining.py \
  --input_file=$GS/wordpiece/pretraining.tf.data \ \
  --output_dir=$GS/wordpiece/model \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=bert_config.json \
  --train_batch_size=64 \
  --max_seq_length=512 \
  --max_predictions_per_seq=20 \
  --num_train_steps=10000 \
  --num_warmup_steps=10000 \
  --save_checkpoints_steps=10000 \
  --learning_rate=1e-4 \
  --use_tpu=True \
  --tpu_name={TPU_ADDRESS} \
  #--num_train_steps=1000000 \
  #--init_checkpoint=$GS/wordpiece/model \

"""### EN-EU"""

import random

def read_sentencepiece_vocab(filepath):
  voc = []
  with open(filepath, encoding='utf-8') as fi:
    for line in fi:
      voc.append(line.split("\t")[0])
  # skip the first <unk> token
  voc = voc[1:]
  return voc

snt_vocab = read_sentencepiece_vocab("{}.vocab".format('./spModels/en-eu'))
print("Learnt vocab size: {}".format(len(snt_vocab)))
print("Sample tokens: {}".format(random.sample(snt_vocab, 10)))

# hau errepasatu. adibidez @ ez dut ondo egiten... [UNK] agertzen da...
# adibidez, 

#import string
#def parse_sentencepiece_token(token):
#    if token.startswith("▁"):
#        return token[1:]
#    else:
#        if token in string.punctuation:
#            return token
#        else:
#            return "##" + token
          

def parse_sentencepiece_token(token):
    if token.startswith("▁"):
        return token[1:]
    else:
        return "##" + token

import string

bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))

ctrl_symbols = ["[PAD]","[UNK]","[CLS]","[SEP]","[MASK]"]
# puntuazio sinboloak gehitu 
ctrl_symbols_end = list(string.punctuation)
#ctrl_symbols = ["[UNK]"]
bert_vocab = ctrl_symbols + bert_vocab + ctrl_symbols_end

#bert_vocab += ["[UNUSED_{}]".format(i) for i in range(32000 - len(bert_vocab))]
print(len(bert_vocab))

VOC_FNAME = "vocab-en_eu.txt"
with open(VOC_FNAME, "w") as fo:
  for token in bert_vocab:
    fo.write(token+"\n")

from bert import tokenization

bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)
bert_tokenizer.tokenize("Nere kotxea aitonaren etxe alboan dago!, eta bere kolorea gorria da. ni@ni.eus erabili ")

FILES = "./corpus/en-eu/2014wiki.eu.sent_splited,./corpus/en-eu/2019wiki-10k.en.sent_splited"

!python bert/create_pretraining_data.py \
  --input_file={FILES} \
  --output_file=$GS/wordpiece/pretraining-en_eu.tf.data \
  --vocab_file=vocab-en_eu.txt \
  --do_lower_case=True \
  --max_seq_length=512 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=666 \
  --do_whole_word_mask=True \
  #--dupe_factor=5

'''
!python src/run_pretraining.py \
  --config_file eu.congif.ini \
  --input_file=$GS/pretraining.tf.data \
  --output_dir=$GS/eu.gureBERT \
  --do_train=True \
  --do_eval=True \
  --train_batch_size=64 \
  --max_seq_length=512 \
  --max_predictions_per_seq=20 \
  --num_train_steps=1000000 \
  --num_warmup_steps=10000 \
  --save_checkpoints_steps=10000 \
  --learning_rate=1e-4 \
  --use_tpu=True \
  --tpu_name={TPU_ADDRESS} \
'''

!python bert/run_pretraining.py \
  --input_file=$GS/wordpiece/pretraining-en_eu.tf.data \ \
  --output_dir=$GS/wordpiece/model-en_eu \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=bert_config.json \
  --train_batch_size=64 \
  --max_seq_length=512 \
  --max_predictions_per_seq=20 \
  --num_train_steps=10000 \
  --num_warmup_steps=10000 \
  --save_checkpoints_steps=10000 \
  --learning_rate=1e-4 \
  --use_tpu=True \
  --tpu_name={TPU_ADDRESS} \
  #--num_train_steps=1000000 \
  #--init_checkpoint=$GS/wordpiece/model \

# data English da!! baino kodea badabil 

!python bert/run_squad.py \
  --vocab_file=vocab-en_eu.txt \
  --bert_config_file=bert_config.json \
  --do_lower_case=True \
  --do_train=True \
  --train_file=train-v2.0.json \
  --do_predict=True \
  --predict_file=dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=0.1 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://gurebert/gureBERT/wordpiece/squad/ \
  --use_tpu=True \
  --tpu_name=$TPU_NAME \
  --version_2_with_negative=True \
  --init_checkpoint=$GS/wordpiece/model-en_eu \

"""# gureBERT (japanese) erabiliz"""

#################################################################
# gureBERT japanese

!cd /content/

!git clone --recursive https://github.com/zmwebdev/bert-japanese
# %cd bert-japanese

from google.colab import auth
auth.authenticate_user()

!python3 src/data-download-and-extract.py
!bash src/file-preprocessing.sh

#!gsutil cp -r gs://gurebert/gureBERT/data/wiki/AA/wiki_00 data/wiki/AA/wiki_00

# Oharra: esaldiak lerro bakarrean jarri behar dira
#!cat data/wiki/AA/wiki_00

# wiki-eu.vocab eta wiki-eu.model sortzen ditu

!pip install sentencepiece
!python3 src/train-sentencepiece.py

!gsutil cp -r data/ gs://gurebert/gureBERT/
!gsutil cp -r model/ gs://gurebert/gureBERT/

!mkdir model
!gsutil cp gs://gurebert/gureBERT/model/wiki-eu.vocab model
!gsutil cp gs://gurebert/gureBERT/model/wiki-eu.model model
!head -n 50 model/wiki-eu.vocab

#https://github.com/zmwebdev/bert-japanese/blob/master/notebook/check-trained-tokenizer.ipynb
!pip install sentencepiece

import sys
sys.path.append("src")

import tokenization_sentencepiece as tokenization

text1 = "Nere kotxea aitonaren etxe alboan dago"
text2 = "Gorria da gure etxearen inguruan dagoen lorearen kolorea"

tokenizer = tokenization.FullTokenizer(
    model_file="model/wiki-eu.model",
    vocab_file="model/wiki-eu.vocab",
    do_lower_case=True)

tokenizer.tokenize(text1)

tokenizer.tokenize(text2)

#do_lower_case=False jarrita tokenizazioa okerragoa da
tokenizer = tokenization.FullTokenizer(
    model_file="model/wiki-eu.model",
    vocab_file="model/wiki-eu.vocab",
    do_lower_case=False)

tokenizer.tokenize(text1)

tokenizer.tokenize(text2)

# convert to WordPiece (kodean errorea dago). begiratu https://github.com/kwonmha/bert-vocab-builder/pull/4#issue-291306156

!git clone https://github.com/kwonmha/bert-vocab-builder.git

!python bert-vocab-builder/subword_builder.py \
--corpus_filepattern model/wiki-eu.vocab \
--output_filename model/wiki-eu-wordpiece.vocab
#--min_count {minimum_subtoken_counts}

# https://github.com/kwonmha/bert-vocab-builder/pull/4#issue-291306156

#Creating data for pretraining
#Create .tfrecord files for pretraining. For longer sentence data, replace the value of max_seq_length with 512.
!cat creating_data_for_pretraining.sh
!bash creating_data_for_pretraining.sh

# copy to gs
!gsutil cp -r data gs://gurebert/gureBERT
!gsutil cp -r model gs://gurebert/gureBERT

# Pre-Training
# https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/pretraining.ipynb

import tensorflow as tf

from google.colab import auth
auth.authenticate_user()

#Check TPU devices

import datetime
import json
import os
import pprint
import random
import string
import sys
import tensorflow as tf

assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'
TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']
print('TPU address is', TPU_ADDRESS)

with tf.Session(TPU_ADDRESS) as session:
  print('TPU devices:')
  pprint.pprint(session.list_devices())

  # Upload credentials to TPU.
  with open('/content/adc.json', 'r') as f:
    auth_info = json.load(f)
  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)
  # Now credentials are set for all future sessions on this TPU.

INPUT_DATA_GCS = 'gs://gurebert/gureBERT/data/wiki'
TARGET_DIRS = [
  'AA',
  'AB',
  'AC'
]

MAX_SEQ_LEN = 128
#MAX_SEQ_LEN = 512


INPUT_FILE = ','.join( [ '{}/{}/all-maxseq{}.tfrecord'.format(INPUT_DATA_GCS, elem, MAX_SEQ_LEN) for elem in TARGET_DIRS] )

OUTPUT_GCS = 'gs://gurebert/gureBERT/model'

# Adding whole word masking aldaketa egin dute BERT-en. update egin behar da!!!
# Begiratu https://github.com/google-research/bert/commit/0fce551b55caabcfba52c61e18f34b541aef186a

!python src/run_pretraining.py \
  --input_file={INPUT_FILE} \
  --output_dir={OUTPUT_GCS} \
  --use_tpu=True \
  --tpu_name={TPU_ADDRESS} \
  --num_tpu_cores=8 \
  --do_train=True \
  --do_eval=True \
  --train_batch_size=64 \
  --max_seq_length={MAX_SEQ_LEN} \
  --max_predictions_per_seq=20 \
  --num_train_steps=1400000 \
  --num_warmup_steps=10000 \
  --save_checkpoints_steps=10000 \
  --learning_rate=1e-4

##########################################
##########################################

# BERT-ekin konpatiblea den vocab sortzeko prozedura. wordpiece _ -> ##. 
# Honen abantaila BERT kodearen aldaketak eta run_squad.py, ... ezer aldatu gabe ibiliko dela da.
# berez ez da wordpiece, sentencepiece baizik baina sintaktikoki konpatiblea
# ikusi: https://colab.research.google.com/drive/1-uLyGTnz2K4gx3su6Qc00in9QCa4b5Kh#scrollTo=9S4CiOh3RzFW

import random

def read_sentencepiece_vocab(filepath):
  voc = []
  with open(filepath, encoding='utf-8') as fi:
    for line in fi:
      voc.append(line.split("\t")[0])
  # skip the first <unk> token
  voc = voc[1:]
  return voc

snt_vocab = read_sentencepiece_vocab("{}.vocab".format('model/wiki-eu'))
print("Learnt vocab size: {}".format(len(snt_vocab)))
print("Sample tokens: {}".format(random.sample(snt_vocab, 10)))

def parse_sentencepiece_token(token):
    if token.startswith("▁"):
        return token[1:]
    else:
        return "##" + token

bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))

ctrl_symbols = ["[PAD]","[UNK]","[CLS]","[SEP]","[MASK]"]
bert_vocab = ctrl_symbols + bert_vocab

bert_vocab += ["[UNUSED_{}]".format(i) for i in range(32000 - len(bert_vocab))]
print(len(bert_vocab))

VOC_FNAME = "vocab.txt" #@param {type:"string"}

with open(VOC_FNAME, "w") as fo:
  for token in bert_vocab:
    fo.write(token+"\n")

from bert import tokenization

bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)
bert_tokenizer.tokenize("Nere kotxea aitonaren etxe alboan dago")

#!gsutil cp -r gs://gurebert/gureBERT/data/wiki/AA/wiki_00 wiki_00
# ez du wiki-eu.model behar, vocab.txt-ekin nahikoa da

!python bert/create_pretraining_data.py \
  --input_file=wiki_00 \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5

#############################################################
#############################################################