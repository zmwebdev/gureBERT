{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gureBERT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRIGPzfVoUlL",
        "colab_type": "text"
      },
      "source": [
        "#gureBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvKEb2yhhFc-",
        "colab_type": "code",
        "outputId": "4da98fd0-0a92-4dde-fc86-b06777173e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# gureBERT\n",
        "\n",
        "!git clone --recursive  https://github.com/zmwebdev/gureBERT\n",
        "%cd gureBERT\n",
        "\n",
        "!pip install sentencepiece\n",
        "!install -d spModels\n",
        "# eu\n",
        "!python src/sentence-split.py --config eu.config.ini --do_lower_case \n",
        "!python src/train-sentencepiece.py --config eu.config.ini\n",
        "\n",
        "!head -n 100 spModels/eu.vocab\n",
        "\n",
        "# en-eu\n",
        "!python src/sentence-split.py --config en-eu.config.ini --do_lower_case \n",
        "!python src/train-sentencepiece.py --config en-eu.config.ini\n",
        "\n",
        "!head -n 100 spModels/en-eu.vocab\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>\t0\n",
            "<s>\t0\n",
            "</s>\t0\n",
            "[PAD]\t0\n",
            "[CLS]\t0\n",
            "[SEP]\t0\n",
            "[MASK]\t0\n",
            "▁the\t-2.94385\n",
            ",\t-3.12044\n",
            ".\t-3.36831\n",
            "▁of\t-3.63212\n",
            "▁and\t-3.88588\n",
            "▁in\t-3.94686\n",
            "▁to\t-4.14213\n",
            "▁a\t-4.18988\n",
            "s\t-4.33794\n",
            "▁\t-4.42405\n",
            "▁is\t-4.83168\n",
            "▁\"\t-4.84885\n",
            "-\t-4.88602\n",
            "▁as\t-4.99159\n",
            "▁(\t-5.09098\n",
            "▁was\t-5.10418\n",
            "▁for\t-5.14361\n",
            "ing\t-5.15246\n",
            "▁that\t-5.17603\n",
            "'\t-5.23655\n",
            "▁by\t-5.25437\n",
            "\"\t-5.30471\n",
            "▁with\t-5.32029\n",
            "▁on\t-5.42771\n",
            "ed\t-5.49214\n",
            "▁are\t-5.6122\n",
            "▁from\t-5.65004\n",
            "▁*\t-5.65699\n",
            "▁it\t-5.74278\n",
            "▁eta\t-5.80773\n",
            "▁an\t-5.81847\n",
            "▁be\t-5.82443\n",
            "▁or\t-5.82872\n",
            "▁his\t-5.8639\n",
            ")\t-5.90229\n",
            "a\t-5.94598\n",
            "▁which\t-5.96859\n",
            "d\t-5.97233\n",
            ":\t-5.98608\n",
            "▁at\t-6.00148\n",
            "▁this\t-6.02449\n",
            "▁-\t-6.0418\n",
            "▁were\t-6.06326\n",
            "▁he\t-6.0713\n",
            "▁have\t-6.13196\n",
            "ly\t-6.20551\n",
            "▁has\t-6.24478\n",
            "▁not\t-6.24927\n",
            "n\t-6.27705\n",
            "▁also\t-6.32563\n",
            "▁had\t-6.43683\n",
            "▁but\t-6.45083\n",
            "▁one\t-6.47153\n",
            ";\t-6.48428\n",
            "▁been\t-6.48897\n",
            "e\t-6.49357\n",
            "▁its\t-6.50068\n",
            "ko\t-6.51327\n",
            "▁other\t-6.5402\n",
            "▁such\t-6.5726\n",
            "\",\t-6.6277\n",
            "k\t-6.63656\n",
            "▁first\t-6.65626\n",
            "▁who\t-6.68015\n",
            "▁some\t-6.68221\n",
            "▁they\t-6.69216\n",
            ").\t-6.70626\n",
            "▁most\t-6.7164\n",
            "▁can\t-6.73573\n",
            "),\t-6.75053\n",
            "▁more\t-6.75593\n",
            "ir\t-6.78541\n",
            "ren\t-6.80497\n",
            "▁all\t-6.84294\n",
            "▁after\t-6.84312\n",
            "y\t-6.85688\n",
            "▁may\t-6.85927\n",
            "▁new\t-6.8618\n",
            "▁there\t-6.86747\n",
            "▁used\t-6.8804\n",
            "▁these\t-6.92874\n",
            "▁than\t-6.92982\n",
            "\".\t-6.93084\n",
            "▁when\t-6.94215\n",
            "the\t-6.95202\n",
            "▁into\t-6.95211\n",
            "▁two\t-6.96751\n",
            "▁many\t-6.96969\n",
            "o\t-7.01476\n",
            "▁islands\t-7.01901\n",
            "▁use\t-7.03485\n",
            "▁time\t-7.043\n",
            "en\t-7.07726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk8_HLFhpLYA",
        "colab_type": "code",
        "outputId": "cad6905b-44c9-4a6e-f5da-170a1933131e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "import tokenization_sentencepiece as tokenization\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    model_file=\"spModels/eu.model\",\n",
        "    vocab_file=\"spModels/eu.vocab\",\n",
        "    do_lower_case=True)\n",
        "\n",
        "text1 = \"Nere kotxea aitonaren etxe alboan dago, bere kolorea gorria da.\"\n",
        "\n",
        "tokenizer.tokenize(text1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded a trained SentencePiece model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁ne',\n",
              " 're',\n",
              " '▁',\n",
              " 'ko',\n",
              " 'txea',\n",
              " '▁ai',\n",
              " 'ton',\n",
              " 'aren',\n",
              " '▁etxe',\n",
              " '▁alb',\n",
              " 'o',\n",
              " 'an',\n",
              " '▁dago',\n",
              " ',',\n",
              " '▁bere',\n",
              " '▁kolore',\n",
              " 'a',\n",
              " '▁gorria',\n",
              " '▁da',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9BPqkOUMHiz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "544d1afd-af8b-491f-d0e8-e47fd485adf7"
      },
      "source": [
        "tokenizer_en_eu = tokenization.FullTokenizer(\n",
        "    model_file=\"spModels/en-eu.model\",\n",
        "    vocab_file=\"spModels/en-eu.vocab\",\n",
        "    do_lower_case=True)\n",
        "\n",
        "text1 = \"Nere kotxea aitonaren etxe alboan dago, bere kolorea gorria da.\"\n",
        "\n",
        "tokenizer_en_eu.tokenize(text1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded a trained SentencePiece model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁ne',\n",
              " 're',\n",
              " '▁ko',\n",
              " 'txea',\n",
              " '▁a',\n",
              " 'it',\n",
              " 'on',\n",
              " 'aren',\n",
              " '▁etxe',\n",
              " '▁',\n",
              " 'albo',\n",
              " 'an',\n",
              " '▁dago',\n",
              " ',',\n",
              " '▁bere',\n",
              " '▁kolore',\n",
              " 'a',\n",
              " '▁gorria',\n",
              " '▁da',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojp8yYLUMUzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d64d8867-8b65-40fd-9d61-d59002940170"
      },
      "source": [
        "text1 = \"The Italian cities of Milan and Cortina d'Ampezzo are chosen as the joint hosts of the 2026 Winter Olympics and Winter Paralympics.\"\n",
        "\n",
        "print(\"EN-EU: {}\".format(len(tokenizer_en_eu.tokenize(text1))))\n",
        "print(\"EU: {}\".format(len(tokenizer.tokenize(text1))))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EN-EU: 31\n",
            "EU: 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLvpxWmnofck",
        "colab_type": "code",
        "outputId": "17d91dc9-287e-4862-e65c-900a2a066aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0629 17:11:49.993598 140062171948928 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNdhng5UqP6R",
        "colab_type": "code",
        "outputId": "3d141164-98c9-4877-d5d3-4b67e5d16a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# Pre-Training\n",
        "# https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/pretraining.ipynb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "#Check TPU devices\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.2.81.2:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 5201560055092340784),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6671482654539601268),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15235922008977912019),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16587921834020422802),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 18249457006028921626),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9769923856279949422),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14707561106018069366),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11546342222793515035),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15225485262720068302),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 18107303912184056565),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12015888753985987776)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c76u8zhRog86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GS = 'gs://gurebert/gureBERT'\n",
        "  \n",
        "#!gsutil cp -r spModels $GS/\n",
        "#!gsutil cp -r corpus $GS/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlujbeOnmPC_",
        "colab_type": "code",
        "outputId": "1ae350c1-ba94-4f1f-ef3b-43560383057b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 src/create_pretraining_data.py \\\n",
        "    --input_file=$GS/corpus/eu/2014wiki.eu.sent_splited \\\n",
        "    --output_file=$GS/pretraining.tf.data \\\n",
        "    --model_file=spModels/eu.model \\\n",
        "    --vocab_file=spModels/eu.vocab \\\n",
        "    --do_lower_case=True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 14:23:14.996464 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:458: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 14:23:14.997114 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:424: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 14:23:14.997257 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:424: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "Loaded a trained SentencePiece model.\n",
            "W0629 14:23:15.023280 140356343957376 deprecation_wrapper.py:119] From /content/gureBERT/src/tokenization_sentencepiece.py:115: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 14:23:15.085362 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:432: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 14:23:16.296864 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:434: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 14:23:16.297110 140356343957376 create_pretraining_data.py:434] *** Reading from input files ***\n",
            "I0629 14:23:16.297206 140356343957376 create_pretraining_data.py:436]   gs://gurebert/gureBERT/corpus/eu/2014wiki.eu.sent_splited\n",
            "I0629 14:23:26.074661 140356343957376 create_pretraining_data.py:445] *** Writing to output files ***\n",
            "I0629 14:23:26.074900 140356343957376 create_pretraining_data.py:447]   gs://gurebert/gureBERT/pretraining.tf.data\n",
            "W0629 14:23:26.075123 140356343957376 deprecation_wrapper.py:119] From src/create_pretraining_data.py:91: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0629 14:23:26.076873 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.077040 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁helbide ▁hau [MASK] [SEP] ▁su bject [SEP]\n",
            "I0629 14:23:26.077317 140356343957376 create_pretraining_data.py:151] input_ids: 4 1270 91 6 5 398 6526 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.077567 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.077784 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.077873 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.077956 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 1622 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.078046 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.078120 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.079127 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.079411 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁norvegia ko ▁nobel ▁komite aren ▁bilera ▁gela ▁bakea ren ▁nobel ▁saria ▁( norvegieraz ▁eta ▁suedieraz : ▁) ▁alfred ▁nobel ek ▁sortu riko ▁5 ▁nobel ▁sari etako ▁bat ▁da . [MASK] ▁horretan ▁bakea ren ▁alde , ▁nazio en ▁arteko ▁elkartasuna ren ▁alde ▁eta ▁ger ren ▁aurka ▁lana ▁egin ▁duen ▁norbait i ▁ematen ▁zaio . ▁beste ▁sari ek ▁ez [MASK] ▁norvegia ko ▁parlamentu ko ▁bost ▁kide en ▁komisio ▁batek ▁erabakitze n ▁du ▁nori ▁eman . ▁saria ▁sortu ▁zen [MASK] ▁suedia ▁eta ▁norvegia ▁herrialde [MASK] ▁ziren ▁eta ▁horregatik ▁bana tzerakoan ▁norvegia ri ▁eman ▁zitzaion ▁aukeratze ko ▁eskubidea . ▁saria ▁oslo n ▁banatzen ▁da ▁abendua ren ▁10 ean , ▁eta ▁stockholmen ▁ematen ▁ez ▁den ▁nobel ▁sari ▁bakarra ▁da . ▁bakea ren ▁nobel ▁saria ren ▁domina ▁gustav ▁vi geland ▁norvegiar ▁eskultorea [MASK] ▁diseinatu ▁zuen . ▁2011 [MASK] [MASK] ▁:201 4 ▁mal ala ▁y ousa f zai ▁eta [MASK] ▁mito ▁sa ty art hi . ▁:201 3 ▁arma ▁kimikoak ▁debekatze ko ▁erakundea ▁:201 2 ▁europar ▁batasuna ▁:201 1 ▁ellen [MASK] [MASK] [MASK] , ▁ leyma h ▁g bowe e ▁eta ▁t awa kul ▁karm an ▁2001 -2010 ▁:201 0 ▁li u ▁xiao bo ▁:200 9 ▁bar ack ▁obama ▁:200 8 ▁mart ti ▁a h tisa ari ▁:200 7 ▁al ▁gore ▁eta ▁klima ▁aldaketari ▁buruzko ▁gobernu ▁arteko ▁taldea ▁:200 6 ▁muhammad ▁y unus ▁eta ▁gram e en ▁bankua . ▁:200 5 ▁energia ▁atomiko rako ▁nazioarteko ▁agentzia ▁eta ▁mo ham ed ▁elbar ade j ▁:200 4 ▁w angari ▁maathai ▁:200 3 ▁shir in ▁e badi ▁:200 2 ▁jim my ▁carter ▁:200 1 ▁nazio ▁batu en ▁erakundea , ▁k ofi ▁annan ▁1991 - 2000 ▁:200 0 ▁kim [MASK] [MASK] ▁j ung ▁:199 9 ▁muga rik ▁gabeko ▁mediku ak ▁:199 8 ▁john ▁hum e , ▁david ▁tri mble ▁:199 7 ▁pertson en ▁aurkako ▁min ak ▁galaraz teko [MASK] ▁kanpain a ▁( ic bl ), ▁jo dy ▁williams ▁:199 6 ▁carlo s ▁filip e ▁ ximenes ▁belo , ▁jo sé ▁ramos - horta ▁:199 5 ▁joseph ▁rot bla t , ▁pug wash ▁con ferences ▁on ▁sci ence ▁and ▁world ▁aff air s ▁:199 4 ▁y asser ▁arafat , ▁shi mon ▁per es , ▁y itz h ak ▁rabin ▁:19 93 ▁n elson [MASK] ▁nevill ▁frederi k ▁will em ▁de ▁k lerk [SEP] ▁jarraipen ▁zerrenda tik ▁eskatutako ak ▁ezabatzen ... [SEP]\n",
            "I0629 14:23:26.079677 140356343957376 create_pretraining_data.py:151] input_ids: 4 1718 13 115 7536 23 3089 10193 659 17 115 285 15 10790 10 5312 22 1949 1227 115 38 94 990 429 115 1075 226 36 24 7 6 593 659 17 249 8 402 21 212 8708 17 249 10 728 17 390 1105 59 130 8861 46 356 660 7 57 1075 38 32 6 1718 13 2287 13 779 1343 21 7293 136 5784 18 65 8893 135 7 285 94 20 6 2028 10 1718 351 6 44 10 3752 4362 2243 1718 79 135 908 6144 13 8781 7 285 5412 18 2147 24 1397 17 467 104 8 10 7235 356 32 117 115 1075 690 24 7 659 17 115 285 17 5935 2497 569 9958 4363 1742 6 5470 26 7 311 6 6 993 300 2067 1333 579 13474 309 3605 10 6 8277 453 4354 2033 3441 7 993 179 2178 1908 3495 13 910 993 113 369 491 993 111 3785 6 6 6 8 14 5441 33 275 11477 40 10 361 4594 5765 10415 30 443 4461 993 210 1175 45 8831 2646 321 167 883 2330 4996 321 169 6304 749 73 33 13400 149 321 237 162 13202 10 798 9540 623 737 212 6142 321 246 7202 579 14322 10 9203 40 21 6322 7 321 215 2623 4808 338 461 7814 10 684 3461 657 9052 4013 27 321 300 315 6167 4908 321 179 5690 207 84 7010 321 113 957 1033 2118 321 111 402 498 21 910 8 87 9936 5525 713 25 6370 321 210 2382 6 6 191 6037 622 167 1119 48 939 1384 19 622 169 112 2286 40 8 406 2265 12353 622 237 2371 21 268 2410 19 11988 1173 6 3764 12 15 6549 10469 105 241 1722 2007 622 246 1168 43 1497 40 14 8600 13295 8 241 1552 7824 25 11353 622 215 560 8852 4492 114 8 9604 12392 2228 7749 2081 8944 6225 1106 2110 8060 6729 43 622 300 579 2240 2932 8 3472 4682 1019 195 8 579 2224 33 19 9522 72 1583 559 3205 6 7679 12938 16 3184 1958 75 87 9736 5 811 174 55 5327 19 8071 372 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.079903 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.080115 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.080209 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 30 58 77 82 126 127 131 132 133 143 144 164 165 166 167 269 270 298 362 363\n",
            "I0629 14:23:26.080295 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 175 171 2208 489 1742 16 311 6130 1157 3530 13479 3072 25 7428 8 24 40 461 3866 8\n",
            "I0629 14:23:26.080395 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.080477 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.081332 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.081463 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁ordu [MASK] [SEP] ▁1597 [SEP]\n",
            "I0629 14:23:26.081689 140356343957376 create_pretraining_data.py:151] input_ids: 4 1017 6 5 5186 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.081910 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.082123 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.177016 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.177336 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 2803 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.177518 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.177643 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.179102 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.179255 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁2014 ). [SEP] ▁otsaila ren [MASK] [SEP]\n",
            "I0629 14:23:26.179545 140356343957376 create_pretraining_data.py:151] input_ids: 4 199 29 5 154 17 6 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.179779 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.179994 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.180085 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.180168 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 550 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.180265 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.180341 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.181242 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.181365 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁* ▁se haska ▁egutegi ko ▁izend egia : ▁ar aitz , ▁ laxari / l azaro [MASK] [MASK] [MASK] [MASK] [SEP] ▁erabiltzaile ari ▁e - posta ▁bidali [SEP]\n",
            "I0629 14:23:26.181617 140356343957376 create_pretraining_data.py:151] input_ids: 4 9 108 166 86 13 172 157 22 519 3636 8 14 7611 77 201 8763 6 6 6 6 5 403 149 84 25 875 500 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.181838 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.182051 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.182141 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 17 18 19 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.182230 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 10 3271 9314 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.182317 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.182406 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.183287 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.183591 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁longpage warn ing [SEP] ▁hiztegia ▁hiztegia ▁hizkuntza ▁asko tatik ▁dator . ▁zenbait ▁hitz [MASK] ▁europa z ▁kanpo ko ▁hizkuntzetatik [MASK] ▁dira , ▁adibidez ▁japonieratik , ▁nazioarteko ak [MASK] [MASK] ▁baina ▁ele ▁gehienak ▁hizkuntza ▁erromantze etatik ▁( gehienbat ▁latinetik ▁eta ▁frantsesetik ) ▁datoz , ▁alemanetik ▁eta ▁ingelesetik . ▁* [MASK] ▁latino etatik : ▁* * ▁latinetik : ▁a bio , ▁sed , ▁tam en , ▁okul o , ▁a kvo ▁* * ▁frantsesetik : ▁di man ĉ o , ▁fermi , ▁ ĉ e , ▁fra pi , ▁ ĉ e valo , ▁b utiko ▁* * ▁italiera tik : ▁ ĉ ielo , ▁f ari , ▁vo ĉ o ▁* * ▁portugesa tik : ▁sa ŭ dado ▁* * ▁beste : ▁faci la , ▁fer o , ▁tra , ▁verd a ▁* ▁hizkuntza ▁germaniarre tatik : ▁* * ▁alemanetik : ▁balda ŭ , ▁bed a ŭ ri , ▁ha ŭ to , ▁jaro , [MASK] ▁* * ▁ingelesetik : ▁bi rdo , ▁m itingo , ▁spit e , ▁sun o , ▁ ŝ arko , ▁te amo ▁* * ▁beste : [MASK] [MASK] [MASK] ▁fi ŝ o , ▁fr em da , ▁gr undo , ▁hal ti , ▁hast i , ▁h undo , ▁of ta , ▁s omero , ▁ ŝ ipo , ▁vi ntro ▁* ▁hizkuntza ▁eslabiarr etatik : ▁* * ▁poloniera tik : ▁cel o , ▁ ĉ u , ▁k rado , ▁ luti , ▁mo ŝ to ▁* * ▁errusiera tik : ▁bar ak ti , ▁serp o , ▁vos to ▁* * ▁beste : ▁klo po di , ▁krom , ▁pr ava ▁* ▁beste ▁indoeuropar ▁hizkuntzetatik : ▁* * ▁ grekotik : ▁hepato , ▁kaj , ▁biologi o , ▁politiko ▁* * ▁lituaniera tik : ▁du , ▁ju , ▁tu j ▁* * ▁san s k ri to tik : [MASK] [MASK] [MASK] ▁gardner [MASK] [MASK] ▁pa do ▁* ▁hizkuntza ▁fin o - ug rikoetatik : ▁* * ▁l aponieratik : ▁sam ea , ▁bo aco , ▁jo j ko ▁* * ▁finlandiera tik : ▁li rli , ▁sa ŭ no ▁* * ▁hungariera tik : ▁c inci , ▁ ĉ ako , ▁ ĉ ardo , ▁ ĉ arda ŝ o , ▁ ĉ uro ▁* ▁semit ar ▁hizkuntzetatik : ▁* * ▁hebreera tik : [MASK] [MASK] ▁* * ▁arabiera tik : ▁ka dio , ▁kaid o , ▁a ŭ ▁* ▁beste [MASK] [MASK] ▁japonieratik : ▁cun amo , ▁ha ŝ io , ▁ha j ko , ▁ uta o , ▁zen o ▁* * ▁txinera tik : ▁to ŭ fu o ▁* * ▁euskara tik : ▁e ŭ ska ▁erreferentzia k ▁kanpo ▁loturak ▁* ▁* ▁* ▁* [MASK] ▁* ▁* ▁* ▁* ▁kategoria : esperantoa [SEP]\n",
            "I0629 14:23:26.183861 140356343957376 create_pretraining_data.py:151] input_ids: 4 8661 2568 330 5 4849 4849 119 280 2090 2429 7 757 456 6 425 41 242 13 5206 6 37 8 562 7946 8 461 19 6 6 88 6148 603 119 6822 416 15 11791 5213 10 5247 28 4175 8 7709 10 8416 7 9 6 14518 416 22 9 64 5213 22 73 3431 8 10781 8 10582 21 8 8699 35 8 73 11056 9 64 5247 22 343 499 0 35 8 5033 8 14 0 40 8 2036 3389 8 14 0 40 12985 8 128 11768 9 64 2142 55 22 14 0 10735 8 313 149 8 2726 0 35 9 64 2705 55 22 453 0 12975 9 64 57 22 8480 333 8 6402 35 8 1338 8 8919 12 9 119 9412 2090 22 9 64 7709 22 10007 0 8 10704 12 0 79 8 342 0 383 8 13269 8 6 9 64 8416 22 95 6779 8 134 12640 8 9054 40 8 4087 35 8 14 0 6560 8 962 4126 9 64 57 22 6 6 6 1189 0 35 8 2682 1958 553 8 6455 8651 8 1993 749 8 2798 46 8 147 8651 8 238 101 8 137 3376 8 14 0 5877 8 569 14288 9 119 7141 416 22 9 64 6948 55 22 1978 35 8 14 0 45 8 87 9793 8 14 9582 8 684 0 383 9 64 3577 55 22 883 19 749 8 8732 35 8 6342 383 9 64 57 22 6647 1967 841 8 5978 8 1784 11222 9 57 12014 5206 22 9 64 14 5527 22 7798 8 9241 8 2709 35 8 2655 9 64 7857 55 22 65 8 2366 8 3248 27 9 64 433 43 16 79 383 55 22 6 6 6 7486 6 6 464 729 9 119 1714 35 25 11345 12634 22 9 64 331 12234 22 1778 943 8 1790 6611 8 241 27 13 9 64 7972 55 22 1175 9695 8 453 0 535 9 64 5119 55 22 209 13953 8 14 0 452 8 14 0 1461 8 14 0 13908 0 35 8 14 0 3545 9 8788 316 5206 22 9 64 7926 55 22 6 6 9 64 2489 55 22 954 2380 8 4291 35 8 73 0 9 57 6 6 7946 22 5831 4126 8 342 0 522 8 342 27 13 8 14 6011 35 8 20 35 9 64 5317 55 22 1366 0 8278 35 9 64 229 55 22 84 0 2764 542 16 242 488 9 9 9 9 6 9 9 9 9 78 22 11732 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.184086 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.184309 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.280735 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 14 20 28 29 49 155 182 183 184 307 308 309 310 311 312 383 384 400 401 446\n",
            "I0629 14:23:26.280960 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 257 125 1525 8 119 3911 14 9709 8 128 8865 8 7534 35 8 954 10883 9 64 9\n",
            "I0629 14:23:26.281090 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.281188 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.282682 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.282880 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁gertaerak ▁arte ▁eta ▁kultura ▁zientzia ▁gaztee ▁teknologia ▁kirolak [SEP] ▁sitesupport [SEP]\n",
            "I0629 14:23:26.283317 140356343957376 create_pretraining_data.py:151] input_ids: 4 68 47 10 54 50 14559 67 70 5 12146 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.283725 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.284065 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.286158 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.286304 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 68 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.286466 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.287182 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.289069 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.289453 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁urtarrila ren ▁15 a ▁gregoriotar ▁egutegi aren ▁urteko ▁hamabos garren ▁eguna ▁da . ▁35 0 ▁egun ▁falt a ▁dira ▁urtea ▁amaitzeko , ▁35 1 ▁egun ▁bis urteetan . ▁gertaerak [SEP] ▁euskal ▁herria ▁* ▁1979 ▁- [MASK] , ▁francisc o ▁mota ▁calv o ▁go ardia ▁zibila ▁erail ▁zuen ▁donostian . ▁mundua ▁* ▁2006 ▁- ▁michel le ▁bache let , ▁txile ko ▁presidente ▁aukeratu ▁zuten . ▁* ▁2009 [MASK] ▁israelek ▁hamase ko ▁barne ▁ministro ▁hiltzea z ▁gain , ▁n ben ▁gune ▁bat , ▁erie txe ▁bat ▁eta ▁pr entsa ▁egoitza ▁bat ▁bonba rdatu ▁zituen . ▁ekonomia ▁* ▁2006 ▁- ▁b b va ▁banku ak ▁bere ▁2005 . eko ▁emaitza k ▁aurkeztu ▁zituen , ▁380 6 ▁milioi ▁euro ko ▁irabaziekin . ▁arte ▁eta ▁kultura ▁* ▁1937 ▁- ▁the ▁plo ugh ▁and ▁the ▁star s ▁(\" go ldea ▁eta ▁izarrak \") ▁filma ▁estreinat u ▁zen , ▁john ▁ford ek ▁zuzendu ▁eta ▁barbar a ▁stan wy ck , ▁prest on ▁foster ▁eta ▁barry ▁fitzgerald ▁aktoree k ▁antzeztu [MASK] [MASK] . ▁* ▁1953 ▁- ▁k night s ▁of ▁the ▁ round ▁table ▁(\" mahai ▁borob ile ko ▁zaldun ak \") ▁filma ▁estreinat u ▁zen , ▁richard ▁tho rpek ▁zuzendu ▁eta ▁robert ▁taylor , ▁ava ▁gardner ▁eta [MASK] ▁ferr er ▁aktoree k ▁antzeztu ▁zuten . ▁* ▁1974 ▁- ▁ha pp y ▁da ys ▁(\" egun ▁ala iak \") ▁telesaila ▁aeb ko ▁ab c ▁telebista ▁katea n ▁aire tatzen ▁hasi ▁zen . ▁zientzia ▁eta ▁teknologia ▁* ▁2001 ▁- ▁wikipedia ren ▁lehen ▁bertsioa , ▁ingelesezkoa , [MASK] ▁jarri ▁zen . ▁kirolak ▁jaiotza k ▁euskal ▁herria ▁* ▁1931 ▁- ▁ro ger ▁idi art , ▁apaiz ▁eta ▁euskal ▁idazlea . ▁( h . ▁2009 ). ▁mundua ▁* ▁1891 ▁- ▁iv or ▁nov ello , ▁galestar ▁konposatzailea , ▁abeslaria ▁eta ▁aktorea ▁( h . ▁1951 ). ▁* ▁1965 ▁- ▁james ▁n esbit t , ▁ipar irlandar ▁aktorea . ▁heriotzak [MASK] ▁herria ▁* [MASK] ▁- ▁imanol ▁ceci aga ▁arizaga ▁euskal ▁futbolaria . ▁( j . ▁1929) . ▁mundua ▁* ▁1519 ▁- ▁vasco ▁n ú ñez ▁de ▁bal boa ▁espainiar [MASK] ▁eta ▁konkistatzailea . ▁( j . ▁1 475) . ▁* ▁1919 ▁- ▁rosa [MASK] [MASK] [MASK] ▁iraultzaile ▁eta ▁teoria lari ▁alemaniarra ▁( j . ▁1870 ). ▁* ▁1919 ▁- ▁karl ▁lieb k necht , ▁alemaniar ▁sozialista [MASK] j [MASK] ▁1871) . ▁* ▁2012 ▁- ▁ manuel ▁frag a , ▁galiziar ▁politikari ▁ultraeskuindar ▁eta ▁frankista ▁( j . ▁1912) . [MASK] ▁eta ▁urteurrena k ▁* ▁se haska ▁egutegi ko ▁izend egia : [MASK] [MASK] ▁eta ▁maur o . [SEP]\n",
            "I0629 14:23:26.289876 140356343957376 create_pretraining_data.py:151] input_ids: 4 236 17 240 12 165 86 23 107 2512 273 145 24 7 754 210 52 159 12 37 143 164 8 754 111 52 288 335 7 68 5 34 51 9 326 11 6 8 1074 35 485 9423 35 492 1018 575 473 26 1666 7 53 9 323 11 2168 282 9673 4005 8 1327 13 324 507 39 7 9 373 6 4541 8970 13 468 484 9472 41 460 8 559 10509 1736 36 8 5687 3216 36 10 1784 13371 2904 36 804 3214 61 7 614 9 323 11 128 528 2394 4340 19 49 346 7 74 745 16 918 61 8 5868 246 703 996 13 5414 7 47 10 54 9 1530 11 103 6457 6097 1106 103 5639 43 231 56 13391 10 5252 206 120 133 45 20 8 112 3343 38 129 10 3228 12 1683 12727 1730 8 6191 180 2969 10 2134 3666 319 16 146 6 6 7 9 1052 11 87 8484 43 238 103 14 13040 7028 231 4731 8227 4776 13 3834 19 206 120 133 45 20 8 693 2324 13960 129 10 197 1838 8 6014 7486 10 6 8960 139 319 16 146 39 7 9 1305 11 342 5781 153 24 10383 231 6701 1213 2049 206 1836 341 13 1076 302 1230 2538 18 4330 5809 92 20 7 50 10 67 9 443 11 291 17 69 1918 8 6523 8 6 348 20 7 70 82 16 34 51 9 689 11 633 847 4819 2033 8 1997 10 34 93 7 15 33 7 373 29 53 9 1263 11 838 901 6319 2012 8 4769 2459 8 385 10 109 15 33 7 922 29 9 760 11 258 559 11113 114 8 251 6875 109 7 66 6 51 9 6 11 2100 3803 1691 7532 34 399 7 15 27 7 3311 7 53 9 8121 11 5071 559 0 8118 75 1676 13828 395 6 10 3341 7 15 27 7 214 6375 7 9 723 11 3088 6 6 6 1856 10 368 1298 869 15 27 7 4064 29 9 723 11 929 7662 16 7836 8 576 1307 6 27 6 9464 7 9 308 11 14 554 6481 12 8 3654 265 4941 10 5938 15 27 7 2050 7 6 10 163 16 9 108 166 86 13 172 157 22 6 6 10 8386 35 7 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.290147 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.290369 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.290493 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 36 37 67 163 164 165 201 248 308 311 337 351 352 353 374 375 376 397 409 410\n",
            "I0629 14:23:26.290646 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 441 8 11 322 48 7 2184 3902 34 332 1390 7862 7809 8 15 27 7 168 596 1854\n",
            "I0629 14:23:26.290795 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.290894 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 14:23:26.291864 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.291985 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁ezb . [SEP] [MASK] [SEP]\n",
            "I0629 14:23:26.292261 140356343957376 create_pretraining_data.py:151] input_ids: 4 5740 7 5 6 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.292608 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.292955 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.293084 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.293188 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 7971 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.293298 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.293406 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.294872 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.295220 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁martxoa ren [MASK] [MASK] ▁gregoriotar ▁egutegi aren ▁urteko ▁hirurogei garren ▁eguna ▁da , ▁61 . a ▁bis urteetan . ▁305 ▁egun ▁falt a ▁dira ▁urtea ▁amaitzeko . ▁gertaerak ▁euskal ▁herria ▁* ▁1976 ▁- [MASK] ▁e milio ▁g ezala ▁aran buru ▁autobus etako ▁in spe ktorea ▁hil [MASK] ▁lezo n . ▁* ▁2005 ▁- ▁2 ▁hildako ▁bilbao ▁ex hibit ion ▁centre en ▁egon dako ▁lan ▁istripuan . ▁* ▁2009 ▁- ▁eus ko ▁legebiltzarre rako ▁hauteskundeak . ▁eajk ▁irabazi ▁zituen ▁30 ▁es er leku rekin , ▁baina ▁ez ▁zion ▁lehendakaritza ▁ziur tatu ▁ibarretxe ri . ▁* ▁2011 ▁- ▁bilbo ▁eta ▁galdakao n ▁eta rekin ▁zerikusi a ▁zutelakoan ▁hainbat ▁pertsona ▁atxilotu ▁zituzten . ▁mundua ▁* ▁1936 ▁- ▁ ll uí s ▁company s ▁bartzelona ra ▁itzuli ▁zen [MASK] [MASK] ▁kargu ▁egiteko . ▁* ▁1958 ▁- ▁ful ge ncio ▁batista k ▁u ko ▁egin ▁zion ▁kuba n ▁batasun ▁nazionale ko ▁gobernu ▁bat ▁osatzeko ▁apezpiku tza ▁katoliko ak ▁egin ▁zion ▁de iari . ▁* ▁1992 ▁- ▁bosnia - her zego vin ak ▁independentzia ▁lortu ▁zuen . ▁* ▁2011 ▁- [MASK] [MASK] [MASK] ▁b h atti ▁gutxiengo en ▁ministro ▁pakistandar ra ▁erail ▁zuten . ▁arte ▁eta ▁kultura ▁* ▁1991 ▁- ▁the ▁door s ▁filma ▁estreinat u ▁zen , ▁olive r ▁stone k [MASK] ▁eta ▁val ▁kil mer , ▁me g ▁ryan , ▁k yle ▁macl ach lan , ▁frank ▁wha ley , ▁k evin ▁d illon , ▁kath leen ▁quin lan , ▁bi lly ▁id ol ▁eta ▁josh ▁ev ans ▁aktoree k [MASK] ▁zuten . ▁zientzia ▁eta [MASK] ▁kirolak ▁jaiotza k ▁euskal ▁herria ▁* ▁1884 ▁- ▁angel [MASK] [MASK] ▁ole aga ▁txistularia . ▁( h . ▁1972) ▁* ▁1899 ▁- ▁aingeru ▁irigara y ▁euskal ▁idazle . [MASK] [MASK] [MASK] [MASK] ▁* ▁1915 ▁- ▁anjel ▁go enaga ▁euskal ▁idazlea . ▁( h . ▁1974) ▁* ▁1943 ▁- ▁jose ▁angel ▁iri bar ▁euskal ▁futbolari ▁ohia ▁eta ▁euskal ▁futbol ▁selekzio aren ▁entrenatzailea . ▁* ▁1978 [MASK] ▁mari sol ▁gil ▁zinemagilea . [SEP] ▁datu - basea ▁blokeatu ▁egin ▁da [SEP]\n",
            "I0629 14:23:26.297648 140356343957376 create_pretraining_data.py:151] input_ids: 4 510 17 6 6 165 86 23 107 2982 273 145 24 8 8426 7 12 288 335 7 8730 52 159 12 37 143 164 7 68 34 51 9 508 11 6 84 2787 275 11081 9085 2378 7656 226 497 6328 5486 76 6 9269 18 7 9 346 11 418 140 1346 1961 5058 1206 7386 21 272 945 178 4061 7 9 373 11 2750 13 9050 338 595 7 6201 250 61 426 1161 139 6785 161 8 88 32 243 3776 2651 2624 2958 79 7 9 311 11 959 10 7372 18 10 161 1470 12 2995 305 263 591 118 7 53 9 1135 11 14 3435 13438 43 5005 43 2907 71 897 20 6 6 1810 472 7 9 1164 11 5790 1612 6315 5992 16 1480 13 59 243 790 18 676 1805 13 737 36 4004 2844 619 1314 19 59 243 75 10325 7 9 917 11 10099 25 1793 13238 3436 19 347 440 26 7 9 311 11 6 6 6 128 33 6289 5055 21 484 5643 71 473 39 7 47 10 54 9 713 11 103 8185 43 120 133 45 20 8 3468 89 5676 16 6 10 3416 13868 1406 8 1108 299 3843 8 87 4280 8762 6071 1956 8 874 8951 768 8 87 3277 194 5325 8 5699 9597 4376 1956 8 95 1759 4697 2201 10 5521 6384 4688 319 16 6 39 7 50 10 6 70 82 16 34 51 9 2292 11 1376 6 6 8937 1691 7245 7 15 33 7 4479 9 2494 11 7470 8254 153 34 85 7 6 6 6 6 9 1734 11 7687 492 13453 34 93 7 15 33 7 10264 9 704 11 192 1376 6110 1428 34 725 777 10 34 476 4287 23 2533 7 9 781 6 829 14100 2253 3093 7 5 336 25 705 948 59 24 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.297954 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.298188 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.298286 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 3 4 34 47 125 126 175 176 177 207 223 247 252 262 263 281 282 283 284 317\n",
            "I0629 14:23:26.298373 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 214 12 441 26 7459 23 3304 528 12830 129 874 146 67 11429 2612 15 33 7 8499 11\n",
            "I0629 14:23:26.298478 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.298552 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.299551 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.299873 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁otsaila ren ▁25 a ▁gregoriotar ▁egutegi aren ▁urteko ▁berrogei ta ▁hamasei ren ▁eguna ▁da . ▁30 9 ▁egun ▁falt a ▁dira ▁urtea ▁amaitzeko , ▁31 0 ▁egun ▁bis urteetan . ▁gertaerak ▁euskal ▁herria ▁* ▁11 19 ▁- ▁tutera ko ▁errendit ze ▁agiria . ▁ordura ▁arte ▁musulmana ▁zen ▁hiria , ▁nafarroako ▁erregea ren ▁esku tan ▁ger atu ▁zen . ▁* ▁1984 ▁- ▁franko ti ratzaile ▁batek ▁eugenio ▁gut i é rrez ▁salazar ▁\" ti gre \" ▁etakidea ▁hil ▁zuen ▁ida uze - mendi n . ▁* ▁2005 ▁- ▁ger n ikako ▁arbola ▁berria ▁landatu ▁zuten ▁ger n ikako ▁batzar ▁etxean , ▁aurrekoa ▁2004 ean ▁i hartu ▁baitzen . ▁mundua ▁* ▁1570 ▁- ▁pio ▁v . a ▁aita ▁santua k ▁elisabet ▁i . a ▁es ko m ikatu ▁zuen . ▁* ▁1921 ▁- [MASK] ▁georgia ▁okupatu ▁zuen . ▁* ▁1932 ▁- ▁adolf ▁hitler ▁hiritar ▁alemaniar ▁bihurtu ▁zen , ▁natura lizazio ▁bidez . ▁* ▁1945 ▁- ▁turkiak ▁gerra ▁dekl aratu [MASK] ▁alemania ri , ▁bigarren ▁mundu ▁gerra ren ▁baitan . ▁* ▁1947 ▁- ▁prusia ko ▁estatua ▁desegin ▁zen . ▁* ▁1948 [MASK] ▁txekoslovakia ko ▁gobernua ▁txekoslovakia ko ▁alderdi ▁komunistaren ▁menpe ▁gelditu ▁zen . ▁* ▁1954 ▁- ▁ga mal ▁ab del ▁n asser ▁egipto ko [MASK] ▁bihurtu ▁zen . ▁* ▁1956 ▁- ▁adierazpen [MASK] ▁buruzagi ▁nik ita ▁khr us txe ve k ▁i os if ▁stalin en ▁pertsona reki ko ▁gur tza ▁kritika tu ▁zuen , ▁eta ▁des esta lin izazioa ▁abiaraz i ▁zuen . ▁* ▁1983 ▁- ▁bale ar ▁uharteeta ko ▁autonomi a ▁estatu tua ▁indarre an ▁sartu ▁zen . ▁* ▁2011 ▁- ▁irlanda ko ▁errepublika ko ▁hauteskunde ▁orokorretan , ▁end a ▁ken ny k ▁gidatuta ko ▁fine ▁gael ▁alderdiak ▁gorakada ▁nabarmena ▁egin ▁zuen , ▁eta ▁gobernu ko ▁fi anna ▁f á il ▁alderdiak , ▁aldiz , ▁inoiz ko ▁porrot ik [MASK] ▁izan ▁zuen . ▁* ▁2012 ▁- ▁iñ aki ▁urdan garin en ▁aurkako ▁epaiketa ▁hasi ▁zen ▁mallorca n . ▁arte ▁eta ▁kultura [MASK] ▁1965 ▁- ▁lor d ▁jim ▁filma ▁estreinat u ▁zen , ▁richard ▁brooks ek ▁zuzendu ▁eta ▁peter ▁o ' to ole , ▁james ▁ma son , ▁cur d ▁j ür gens 1642) ▁eli ▁wallac h ▁aktoree k ▁antzeztu [MASK] [MASK] [MASK] ▁* ▁2004 ▁- ▁the ▁pass ion ▁of ▁the ▁christ ▁(\" kristo ren ▁pas ioa \") ▁filma ▁estreinat u ▁zen , ▁mel ▁gibson ek ▁zuzendu ▁eta ▁jim ▁cav ie zel , ▁ monica ▁bell ucci ▁eta ▁maia ▁morg en stern ▁aktoree k ▁antzeztu ▁zutela rik . ▁zientzia ▁eta ▁teknologia ▁kirolak ▁jaiotza k ▁euskal ▁herria ▁* [MASK] ▁- ▁tx ema ▁vitoria ▁txir ibi ton , ▁euskal ▁pa il azoa ▁eta ▁matematika ▁irakaslea . ▁mundua ▁* ▁ 1778 ▁- ▁jose ▁de ▁san ▁martin ▁militar ▁eta ▁independentist a ▁argentinarra . ▁( h . ▁1850 ) ▁* ▁1875 ▁- ▁enri co ▁caruso ▁tenore ▁italiarra . ▁( h . ▁1921) ▁* ▁1890 ▁- ▁vi atx eslav [MASK] ▁landaredia [MASK] [MASK] ▁sobietarra . ▁( h . ▁1986) ▁* ▁1943 ▁- ▁george ▁harrison ▁musikaria [MASK] ▁* ▁1953 ▁- ▁jose ▁maria ▁aznar [MASK] ▁espainiarra . [SEP] ▁aldaketak [SEP]\n",
            "I0629 14:23:26.300141 140356343957376 create_pretraining_data.py:151] input_ids: 4 154 17 352 12 165 86 23 107 712 101 6076 17 145 24 7 426 167 52 159 12 37 143 164 8 477 210 52 288 335 7 68 34 51 9 408 1881 11 5275 13 5730 1245 5364 7 1688 47 8433 20 639 8 581 527 17 610 170 728 176 20 7 9 1199 11 2714 749 9998 136 2478 6032 46 944 9033 7598 63 749 6988 60 2087 76 26 8803 7019 25 1610 18 7 9 346 11 728 18 4230 4365 404 10441 39 728 18 4230 3579 3131 8 9250 594 104 121 6949 2549 7 53 9 5226 11 1914 387 7 12 270 936 16 2097 121 7 12 1161 13 360 5805 26 7 9 1549 11 6 5666 2119 26 7 9 762 11 1228 4422 10222 576 306 20 8 1196 14018 493 7 9 831 11 4632 234 1999 1905 6 276 79 8 337 132 234 17 1711 7 9 842 11 2587 13 1707 1350 20 7 9 567 6 3684 13 1399 3684 13 397 3309 3469 5323 20 7 9 995 11 1202 5956 1076 1950 559 2240 866 13 6 306 20 7 9 912 11 5178 6 1942 680 1539 13185 196 3216 1576 16 121 664 6995 5070 21 263 13417 13 1994 619 6096 97 26 8 10 1830 14050 1215 9692 3992 46 26 7 9 848 11 6034 316 3170 13 2558 12 81 2210 983 30 366 20 7 9 311 11 1117 13 219 13 2080 8066 8 9475 12 3174 577 16 8733 13 10237 12739 4195 5249 1777 59 26 8 10 737 13 1189 11975 313 2401 679 4195 8 203 8 1475 13 1682 186 6 31 26 7 9 308 11 2136 2665 5426 6378 21 268 1875 92 20 7045 18 7 47 10 54 6 760 11 2387 228 957 120 133 45 20 8 693 5014 38 129 10 632 224 400 383 10074 8 258 596 548 8 2753 228 191 9549 5931 6506 1324 3838 33 319 16 146 6 6 6 9 594 11 103 9968 1206 238 103 4402 231 6932 17 3961 2766 206 120 133 45 20 8 2184 5043 38 129 10 957 9350 1400 3229 8 14 5517 6632 5934 10 4294 13281 21 9090 319 16 146 322 48 7 50 10 67 70 82 16 34 51 9 6 11 1739 4643 9017 6448 13360 968 8 34 464 679 14001 10 819 3154 7 53 9 14 3245 11 192 75 433 345 1311 10 3024 12 2154 7 15 33 7 9289 28 9 3117 11 6341 772 7425 5268 532 7 15 33 7 2018 9 2829 11 569 10627 9923 6 3827 6 6 5415 7 15 33 7 3355 9 704 11 367 3794 649 6 9 1052 11 192 417 7637 6 1042 7 5 608 5 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.300370 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.300603 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.300695 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 132 158 179 202 209 210 299 321 352 359 360 361 416 472 473 474 475 487 488 495\n",
            "I0629 14:23:26.300787 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 4430 243 11 1942 10666 13 512 9 10 322 48 7 704 684 5970 2051 265 649 7 265\n",
            "I0629 14:23:26.300872 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.300944 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.301816 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.301933 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁\" . $1\" [MASK] ▁da ▁gomenda tutako [MASK] ▁fitxategi ▁formatua . [SEP] ▁searchresults head [SEP]\n",
            "I0629 14:23:26.302165 140356343957376 create_pretraining_data.py:151] input_ids: 4 63 7 9557 6 24 9771 1907 6 937 3880 7 5 7830 11705 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.302401 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.302620 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.398494 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 4 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.398756 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 32 1453 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.399047 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.399208 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.400994 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.401296 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁gero ▁etorri ▁zen ▁h aren ▁ordez koak , ▁viena ko ▁eskolak ▁(1770-18 30) , ▁sinfonia [MASK] ▁ka mara ▁musika ren ▁hainbat ▁forma ▁bideratu ▁zituen . ▁garai ▁hartako ▁musika ▁ sorkuntza ko ▁ahalmen ▁handia ren [MASK] [MASK] ▁bikain ▁ditugu ▁ha ydn , ▁mozart [MASK] ▁beethoven . ▁erromantizismo ko ▁musika n [MASK] [MASK] ▁subjektib ismoa , ▁poesia ▁eta ▁irudi mena ▁ditugu . ▁schubert ekin ▁hasi ▁zen ▁aldi ▁hark ▁wagner ▁eta ▁lis zt ▁izan go ▁zituen ▁goren ean , ▁eta ▁korronte ▁nazionalista ▁erromantikoa k ▁iriste an ▁amaitu ▁zen . ▁xx . ▁mendean , ▁musika ren ▁joera ▁ohiko ▁arau ▁formala k ▁eta ▁konposizio aren ak ▁hauste a ▁izan ▁da , ▁at onalitatea ▁eta ▁dodekafonismo a ▁bezala ko ▁joe ren ▁bidez . ▁estatu ▁batuetan ▁jazz a ▁sortzea rekin ▁eta ▁aurrerapen ▁teknologiko ekin , ▁musika ▁konkretu a ▁eta ▁elektronikoa ▁etorri [MASK] [MASK] ▁1950 eko ▁hamarkadan ▁berrikuntz a ▁handiak ▁izan ▁ziren , ▁eta ▁disko ▁jo gailu aren ▁eta ▁irrati aren ▁zabal kundeak ▁maisu ▁zahar ▁eta ▁konpositore ▁berrien ▁obra k ▁hedatu ▁zituzten ▁mundu ▁osoan . ▁musika ren [MASK] ▁kontinentek a ▁amerika ko ▁musika ▁amerika ko ▁musikari 2 ▁lehen ▁azterketa k ▁xix . ▁mendea ren ▁bukaeran ▁hasi ▁ziren ; ▁ordura ko ▁europako ▁musika ren ▁eragin ▁handia ▁zuen [MASK] en ▁musikak . ▁dena [MASK] [MASK] ▁arkeolog ia ▁aztarn egi etan ▁aurkitu riko ▁tresnen , ▁xvi . ▁eta ▁xvii . ▁mende etako ▁misiolari en ▁kronik en ▁eta ▁batez ▁ere ▁beste ▁biztanle ekin ▁nahastu ▁ez ▁diren ▁indiar ▁talde [MASK] ▁gaur ▁egun go ▁musika ren [MASK] [MASK] ▁aztertu ▁ahal ▁izan ▁da ▁bertako ▁musika ren ▁bilakaera . ▁europarra k ▁heldu ▁aurretik ▁india rrek ▁ez ▁zuten ▁ezagutzen ▁musika ▁notazio rako ▁sistema rik , ▁musika ▁tresne k ▁lagun tzen rei ▁kantua ri , ▁eta [MASK] ▁eta ▁dantza ▁batera ▁egiten [MASK] ▁beti . ▁sort aldeko ▁musika ▁hindu en ▁musikak ▁ved a ▁testu en ▁edo ▁testu ▁sakratu en ▁irakur k etan ▁du ▁jatorria . ▁india ko ▁musikak ▁erabateko ▁aldaketa ▁izan ▁zuen ▁k . a . ▁vii . ▁mendean , ▁budismoa ren ▁eta ▁helen iar ▁kultura ren ▁eraginez ; ▁hori etatik ▁sortu ▁zen ▁indiar ▁esti lo ▁klasikoa , ▁rag a ▁izenekoa . ▁txinako ▁musika , ▁han ▁dinastia ren ▁garaian ▁( k . a . ▁20 6 ▁- ▁k . o . ▁220 ), ▁errit uzkoa ▁zen ; ▁laur ogeita ▁hamar ▁musikari z ▁osatutako ▁orkestr ek ▁jo tzen ▁zuten . [SEP] ▁orri ▁hone n ▁eztabaida ▁orria ▁ere ▁mugitu ▁da . [SEP]\n",
            "I0629 14:23:26.401612 140356343957376 create_pretraining_data.py:151] input_ids: 4 181 896 20 147 23 1150 628 8 8953 13 13869 7058 6398 8 7513 6 954 2758 158 17 305 1171 5429 61 7 339 2170 158 14 8337 13 4762 260 17 6 6 5090 1009 342 8582 8 3725 6 3665 7 6053 13 158 18 6 6 11542 1043 8 2519 10 1453 3276 1009 7 4919 193 92 20 1212 3329 3713 10 5914 8547 31 56 61 8894 104 8 10 7182 5130 8137 16 6094 30 667 20 7 877 7 325 8 158 17 1648 2344 1421 5590 16 10 4322 23 19 10772 12 31 24 8 999 12897 10 4884 12 171 13 2388 17 493 7 81 1649 4055 12 938 161 10 3039 8825 193 8 158 3018 12 10 1339 896 6 6 644 74 2537 4011 12 388 31 44 8 10 8847 241 3179 23 10 2315 23 932 4862 1732 2864 10 2446 3404 1495 16 1463 118 132 934 7 158 17 6 9219 12 262 13 158 262 13 845 113 69 3069 16 706 7 411 17 2544 92 44 58 1688 13 235 158 17 301 260 26 6 21 2772 7 482 6 6 7011 182 4617 2052 83 223 990 8509 8 1367 7 10 976 7 312 226 2116 21 6427 21 10 409 42 57 213 193 8939 32 152 3210 233 6 253 52 56 158 17 6 6 1930 787 31 24 980 158 17 2452 7 4833 16 1118 1646 370 2678 32 39 969 158 7823 338 423 48 8 158 4727 16 503 100 10782 4332 79 8 10 6 10 6566 296 222 6 1459 7 4567 3260 158 6116 21 2772 9159 12 1090 21 62 1090 7765 21 10858 16 83 65 1528 7 370 13 2772 3457 389 31 26 87 7 12 7 821 7 325 8 3717 17 10 4520 1437 54 17 1247 58 124 416 94 20 3210 5743 739 1744 8 5849 12 3171 7 1395 158 8 1082 3534 17 2208 15 16 7 12 7 462 246 11 87 7 35 7 13456 105 10536 13271 20 58 9166 11049 1062 845 41 1887 10738 38 241 100 39 7 5 442 1087 18 375 914 42 2487 24 7 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.401857 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.402079 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.402172 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 16 35 36 43 50 51 135 136 170 179 199 200 204 205 237 243 244 274 279 284\n",
            "I0629 14:23:26.402266 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 10 7004 6453 10 7659 11343 44 7 142 623 980 21 117 8 1520 493 8 533 158 44\n",
            "I0629 14:23:26.402358 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:23:26.402449 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.403336 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.403466 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁* [MASK] [MASK] [SEP] ▁kanpo ▁loturak ▁* ▁erreferentzia k [SEP]\n",
            "I0629 14:23:26.403706 140356343957376 create_pretraining_data.py:151] input_ids: 4 9 6 6 5 242 488 9 542 16 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.403926 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.404144 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.404249 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.404333 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 115 285 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.404442 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.404522 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 0\n",
            "I0629 14:23:26.405406 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.405529 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁155 1 [SEP] [MASK] [SEP]\n",
            "I0629 14:23:26.405759 140356343957376 create_pretraining_data.py:151] input_ids: 4 4854 111 5 6 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.405980 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.406194 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.406291 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.503809 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 1641 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.504071 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.504188 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.505302 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.505477 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁links here [SEP] ▁next n [SEP]\n",
            "I0629 14:23:26.505749 140356343957376 create_pretraining_data.py:151] input_ids: 4 9655 5629 5 8064 18 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.505972 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.506184 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.506279 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.506360 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.506467 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.506541 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.507830 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.507946 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁saioa ▁hasi ▁behar ▁neurri [MASK] ▁aldatzeko . [SEP] ▁2003 [SEP]\n",
            "I0629 14:23:26.508175 140356343957376 create_pretraining_data.py:151] input_ids: 4 525 92 102 2649 6 3071 7 5 1044 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.508431 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.508660 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.508751 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.508836 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 650 2098 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.508923 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.508998 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.509860 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.509972 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁orrialde ▁bisitatu en ak [SEP] [MASK] [MASK] ▁mendea ren ▁taula [SEP]\n",
            "I0629 14:23:26.510198 140356343957376 create_pretraining_data.py:151] input_ids: 4 185 9143 21 19 5 6 6 411 17 668 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.510438 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.510658 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.510747 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 6 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.510830 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 706 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.510918 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.510991 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.511833 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.511941 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁not logg edin [SEP] ▁sphe ading [SEP]\n",
            "I0629 14:23:26.512171 140356343957376 create_pretraining_data.py:151] input_ids: 4 11046 8038 8361 5 5944 10507 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.512411 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.512624 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.603824 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.604072 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.604237 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.604367 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:26.605872 140356343957376 create_pretraining_data.py:139] *** Example ***\n",
            "I0629 14:23:26.606020 140356343957376 create_pretraining_data.py:141] tokens: [CLS] ▁un watchthis page [SEP] ▁revert page [SEP]\n",
            "I0629 14:23:26.606286 140356343957376 create_pretraining_data.py:151] input_ids: 4 1210 11678 558 5 5428 558 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.606531 140356343957376 create_pretraining_data.py:151] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.606758 140356343957376 create_pretraining_data.py:151] segment_ids: 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.606849 140356343957376 create_pretraining_data.py:151] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.606933 140356343957376 create_pretraining_data.py:151] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:23:26.607022 140356343957376 create_pretraining_data.py:151] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:23:26.607096 140356343957376 create_pretraining_data.py:151] next_sentence_labels: 1\n",
            "I0629 14:23:42.175292 140356343957376 create_pretraining_data.py:156] Wrote 20651 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWLKYF-Hmkb2",
        "colab_type": "code",
        "outputId": "ab40b681-91ce-4289-dc3f-8f299e64832a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!install -d gureBERT\n",
        "\n",
        "!python src/run_pretraining.py \\\n",
        "  --config_file eu.congif.ini \\\n",
        "  --input_file=$GS/pretraining.tf.data \\\n",
        "  --output_dir=$GS/eu.gureBERT \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=10000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "#  --num_train_steps=1000000 \\\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 12:19:57.805771 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 12:19:57.808184 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:498: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 12:19:57.808862 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:413: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 12:19:57.809011 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:413: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 12:19:57.809142 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 12:19:57.809830 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:420: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0629 12:19:59.198010 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:424: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 12:19:59.352377 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:426: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 12:19:59.352607 139887738599296 run_pretraining.py:426] *** Input Files ***\n",
            "I0629 12:19:59.352693 139887738599296 run_pretraining.py:428]   gs://gurebert/gureBERT/pretraining.tf.data\n",
            "W0629 12:20:00.362826 139887738599296 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 12:20:01.368843 139887738599296 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f39f0dc2c80>) includes params argument, but params are not passed to Estimator.\n",
            "I0629 12:20:01.370452 139887738599296 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/eu.gureBERT', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.10.160.194:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f39fd2432e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.10.160.194:8470', '_evaluation_master': 'grpc://10.10.160.194:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f39fd24ffd0>}\n",
            "I0629 12:20:01.370841 139887738599296 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0629 12:20:01.371613 139887738599296 run_pretraining.py:465] ***** Running training *****\n",
            "I0629 12:20:01.371721 139887738599296 run_pretraining.py:466]   Batch size = 64\n",
            "I0629 12:20:03.858192 139887738599296 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.10.160.194:8470) for TPU system metadata.\n",
            "2019-06-29 12:20:03.859898: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 12:20:03.881084 139887738599296 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0629 12:20:03.881365 139887738599296 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0629 12:20:03.881476 139887738599296 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0629 12:20:03.881558 139887738599296 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0629 12:20:03.881658 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5315554842738034793)\n",
            "I0629 12:20:03.882470 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 12012256443778481517)\n",
            "I0629 12:20:03.882557 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4561923752582128113)\n",
            "I0629 12:20:03.882643 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12806759333023036689)\n",
            "I0629 12:20:03.882716 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6527302475489399346)\n",
            "I0629 12:20:03.882784 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2817395607489107592)\n",
            "I0629 12:20:03.882852 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 4079461139510367575)\n",
            "I0629 12:20:03.882913 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 380354458380127376)\n",
            "I0629 12:20:03.882972 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6785183797655118652)\n",
            "I0629 12:20:03.883038 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12102718425508620686)\n",
            "I0629 12:20:03.883100 139887738599296 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5403939343175850055)\n",
            "W0629 12:20:03.889518 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0629 12:20:03.903289 139887738599296 estimator.py:1145] Calling model_fn.\n",
            "W0629 12:20:03.903932 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:343: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0629 12:20:03.910025 139887738599296 deprecation.py:323] From src/run_pretraining.py:374: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0629 12:20:03.910292 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0629 12:20:03.938652 139887738599296 deprecation.py:323] From src/run_pretraining.py:391: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0629 12:20:03.938914 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0629 12:20:03.940707 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:399: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0629 12:20:03.946725 139887738599296 deprecation.py:323] From src/run_pretraining.py:406: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0629 12:20:04.038428 139887738599296 run_pretraining.py:123] *** Features ***\n",
            "I0629 12:20:04.038700 139887738599296 run_pretraining.py:125]   name = input_ids, shape = (8, 512)\n",
            "I0629 12:20:04.038825 139887738599296 run_pretraining.py:125]   name = input_mask, shape = (8, 512)\n",
            "I0629 12:20:04.038925 139887738599296 run_pretraining.py:125]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0629 12:20:04.039020 139887738599296 run_pretraining.py:125]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0629 12:20:04.039129 139887738599296 run_pretraining.py:125]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0629 12:20:04.039216 139887738599296 run_pretraining.py:125]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0629 12:20:04.039453 139887738599296 run_pretraining.py:125]   name = segment_ids, shape = (8, 512)\n",
            "W0629 12:20:04.039717 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:172: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0629 12:20:04.042016 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:411: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0629 12:20:04.078561 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/modeling.py:492: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0629 12:20:04.254380 139887738599296 deprecation.py:506] From /content/gureBERT/src/../bert/modeling.py:359: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0629 12:20:04.276711 139887738599296 deprecation.py:323] From /content/gureBERT/src/../bert/modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0629 12:20:08.228111 139887738599296 run_pretraining.py:173] **** Trainable Variables ****\n",
            "I0629 12:20:08.228388 139887738599296 run_pretraining.py:179]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768)\n",
            "I0629 12:20:08.228539 139887738599296 run_pretraining.py:179]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0629 12:20:08.228651 139887738599296 run_pretraining.py:179]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0629 12:20:08.228754 139887738599296 run_pretraining.py:179]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.228858 139887738599296 run_pretraining.py:179]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.228946 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.229029 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.229107 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.229186 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.229276 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.229355 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.229429 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.229506 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.229581 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.229661 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.229735 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.229812 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.229886 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.229964 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.230037 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.230112 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.230185 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.230275 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.230348 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.230425 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.230499 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.230576 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.230655 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.230733 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.230807 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.230880 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.230955 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.231032 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.231106 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.231186 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.231273 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.231348 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.231422 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.231499 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.231573 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.231657 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.231730 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.231807 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.231881 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.231956 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.232029 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.232102 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.232176 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.232265 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.232342 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.232419 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.232493 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.232567 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.232648 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.232724 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.232797 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.232873 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.232946 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.233022 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.233094 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.233171 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.233260 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.233338 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.233413 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.233491 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.233566 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.233651 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.233726 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.233798 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.233871 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.233949 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.234022 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.234098 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.234173 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.234265 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.234342 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.234420 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.234493 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.234565 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.234647 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.234723 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.234797 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.234874 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.234949 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.235021 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.235094 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.235171 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.235257 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.235335 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.235409 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.235485 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.235560 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.235643 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.235716 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.235788 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.235861 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.235938 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.236013 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.236089 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.236163 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.236251 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.236326 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.236403 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.236477 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.236553 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.236633 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.236710 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.236783 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.236861 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.236935 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.237007 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.237080 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.237158 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.237244 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.237323 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.237398 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.237470 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.237542 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.237626 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.237699 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.237776 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.237851 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.293427 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.293770 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.293917 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.294020 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.294120 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.294220 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.294351 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.294456 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.294564 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.294680 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.294785 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.294890 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.294999 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.295101 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.295207 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.295329 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.295445 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.295546 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.295665 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.295770 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.295880 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.295981 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.296090 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.296195 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.296319 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.296425 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.296538 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.296651 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.296759 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.296862 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.296971 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.297072 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.297194 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.297319 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.297428 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.297529 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.297642 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.297747 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.297856 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.297959 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.298066 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.298168 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.298286 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.298393 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.298519 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.298631 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.298739 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.298857 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.298964 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.299066 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.299173 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.299407 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.299533 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.299648 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.299758 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.299860 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.299967 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.300068 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.300170 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.300286 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.300397 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 12:20:08.300500 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.300617 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 12:20:08.300720 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.300826 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 12:20:08.300930 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.301035 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.301136 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.301265 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.301373 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 12:20:08.301481 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 12:20:08.301583 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 12:20:08.301703 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.301804 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.301902 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.302002 139887738599296 run_pretraining.py:179]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.302110 139887738599296 run_pretraining.py:179]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.302213 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0629 12:20:08.302351 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0629 12:20:08.302456 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 12:20:08.302557 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 12:20:08.302678 139887738599296 run_pretraining.py:179]   name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0629 12:20:08.302782 139887738599296 run_pretraining.py:179]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0629 12:20:08.302889 139887738599296 run_pretraining.py:179]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0629 12:20:08.303078 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0629 12:20:08.304887 139887738599296 deprecation_wrapper.py:119] From /content/gureBERT/src/../bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0629 12:20:08.312771 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0629 12:20:12.988123 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0629 12:20:22.340407 139887738599296 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0629 12:20:22.723875 139887738599296 estimator.py:1147] Done calling model_fn.\n",
            "I0629 12:20:26.487624 139887738599296 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 12:20:27.913854 139887738599296 monitored_session.py:240] Graph was finalized.\n",
            "W0629 12:20:28.055055 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0629 12:20:28.229808 139887738599296 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/eu.gureBERT/model.ckpt-0\n",
            "W0629 12:20:56.797613 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0629 12:20:58.530132 139887738599296 session_manager.py:500] Running local_init_op.\n",
            "I0629 12:20:59.630610 139887738599296 session_manager.py:502] Done running local_init_op.\n",
            "I0629 12:21:11.542770 139887738599296 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://gurebert/gureBERT/eu.gureBERT/model.ckpt.\n",
            "W0629 12:21:42.218657 139887738599296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0629 12:21:44.502938 139887738599296 util.py:98] Initialized dataset iterators in 1 seconds\n",
            "I0629 12:21:44.504098 139887738599296 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-06-29 12:21:44.504481: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 12:21:44.510127 139887738599296 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0629 12:21:44.512408 139887738599296 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0629 12:21:44.516939 139887738599296 tpu_estimator.py:557] Init TPU system\n",
            "I0629 12:21:56.298977 139887738599296 tpu_estimator.py:566] Initialized TPU in 11 seconds\n",
            "I0629 12:21:56.299902 139886600554240 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 12:21:56.300327 139886574077696 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 12:21:57.523996 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:21:57.525045 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:22:33.871097 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 12:23:33.945027 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (0, 237)\n",
            "I0629 12:24:34.020742 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (0, 474)\n",
            "I0629 12:25:34.096276 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (0, 711)\n",
            "I0629 12:26:34.174379 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (0, 948)\n",
            "I0629 12:26:49.075427 139887738599296 basic_session_run_hooks.py:262] loss = 6.937065, step = 1000\n",
            "I0629 12:26:49.077914 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:26:49.078168 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:27:34.265166 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (1, 140)\n",
            "I0629 12:28:34.341797 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (1, 377)\n",
            "I0629 12:29:34.418697 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (1, 614)\n",
            "I0629 12:30:34.495788 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (1, 851)\n",
            "I0629 12:31:12.829446 139887738599296 basic_session_run_hooks.py:260] loss = 7.0191164, step = 2000 (263.754 sec)\n",
            "I0629 12:31:12.831134 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.79141\n",
            "I0629 12:31:12.832075 139887738599296 tpu_estimator.py:2160] examples/sec: 242.65\n",
            "I0629 12:31:13.653395 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:31:13.653923 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:31:34.502065 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (2, 77)\n",
            "I0629 12:32:34.580137 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (2, 314)\n",
            "I0629 12:33:34.656750 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (2, 551)\n",
            "I0629 12:34:34.734082 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (2, 788)\n",
            "I0629 12:35:28.993658 139887738599296 basic_session_run_hooks.py:260] loss = 6.616879, step = 3000 (256.164 sec)\n",
            "I0629 12:35:28.994962 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.90375\n",
            "I0629 12:35:28.995130 139887738599296 tpu_estimator.py:2160] examples/sec: 249.84\n",
            "I0629 12:35:30.022647 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:35:30.023137 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:35:34.954321 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (3, 14)\n",
            "I0629 12:36:35.031526 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (3, 251)\n",
            "I0629 12:37:35.106886 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (3, 488)\n",
            "I0629 12:38:35.182509 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (3, 725)\n",
            "I0629 12:39:35.258756 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (3, 962)\n",
            "I0629 12:39:46.361016 139887738599296 basic_session_run_hooks.py:260] loss = 5.333578, step = 4000 (257.367 sec)\n",
            "I0629 12:39:46.362700 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.88549\n",
            "I0629 12:39:46.363071 139887738599296 tpu_estimator.py:2160] examples/sec: 248.671\n",
            "I0629 12:39:46.364535 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:39:46.364724 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:40:35.332487 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (4, 188)\n",
            "I0629 12:41:35.412887 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (4, 425)\n",
            "I0629 12:42:35.493352 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (4, 662)\n",
            "I0629 12:43:35.574719 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (4, 899)\n",
            "I0629 12:44:01.747989 139887738599296 basic_session_run_hooks.py:260] loss = 3.9896374, step = 5000 (255.387 sec)\n",
            "I0629 12:44:01.749690 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.91563\n",
            "I0629 12:44:01.749899 139887738599296 tpu_estimator.py:2160] examples/sec: 250.6\n",
            "I0629 12:44:02.695559 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:44:02.696082 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:44:35.732580 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (5, 125)\n",
            "I0629 12:45:35.810064 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (5, 362)\n",
            "I0629 12:46:35.888416 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (5, 599)\n",
            "I0629 12:47:35.966619 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (5, 836)\n",
            "I0629 12:48:18.042087 139887738599296 basic_session_run_hooks.py:260] loss = 1.4737389, step = 6000 (256.294 sec)\n",
            "I0629 12:48:18.043930 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.90177\n",
            "I0629 12:48:18.044143 139887738599296 tpu_estimator.py:2160] examples/sec: 249.713\n",
            "I0629 12:48:18.903822 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:48:18.904337 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:48:36.199195 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (6, 63)\n",
            "I0629 12:49:36.276869 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (6, 300)\n",
            "I0629 12:50:36.352848 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (6, 537)\n",
            "I0629 12:51:36.429468 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (6, 774)\n",
            "I0629 12:52:35.242589 139887738599296 basic_session_run_hooks.py:260] loss = 3.5686815, step = 7000 (257.200 sec)\n",
            "I0629 12:52:35.244260 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.88802\n",
            "I0629 12:52:35.244764 139887738599296 tpu_estimator.py:2160] examples/sec: 248.833\n",
            "I0629 12:52:35.245944 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:52:35.246130 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:52:36.624135 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (7, 0)\n",
            "I0629 12:53:36.701828 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (7, 237)\n",
            "I0629 12:54:36.778469 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (7, 474)\n",
            "I0629 12:55:36.854178 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (7, 711)\n",
            "I0629 12:56:36.932058 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (7, 948)\n",
            "I0629 12:56:50.684499 139887738599296 basic_session_run_hooks.py:260] loss = 0.7635531, step = 8000 (255.442 sec)\n",
            "I0629 12:56:50.686144 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.91478\n",
            "I0629 12:56:50.686382 139887738599296 tpu_estimator.py:2160] examples/sec: 250.546\n",
            "I0629 12:56:51.666585 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 12:56:51.667099 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 12:57:37.106355 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (8, 174)\n",
            "I0629 12:58:37.182964 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (8, 411)\n",
            "I0629 12:59:37.259992 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (8, 648)\n",
            "I0629 13:00:37.340672 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (8, 885)\n",
            "I0629 13:01:07.060887 139887738599296 basic_session_run_hooks.py:260] loss = 1.0859506, step = 9000 (256.376 sec)\n",
            "I0629 13:01:07.062442 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.90052\n",
            "I0629 13:01:07.062622 139887738599296 tpu_estimator.py:2160] examples/sec: 249.633\n",
            "I0629 13:01:08.013406 139887738599296 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 13:01:08.013932 139887738599296 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 13:01:37.543004 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (9, 111)\n",
            "I0629 13:02:37.618957 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (9, 348)\n",
            "I0629 13:03:37.695111 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (9, 585)\n",
            "I0629 13:04:37.771129 139886574077696 tpu_estimator.py:275] Outfeed finished for iteration (9, 822)\n",
            "I0629 13:05:23.505219 139887738599296 basic_session_run_hooks.py:606] Saving checkpoints for 10000 into gs://gurebert/gureBERT/eu.gureBERT/model.ckpt.\n",
            "I0629 13:05:51.317413 139887738599296 basic_session_run_hooks.py:260] loss = 0.3301283, step = 10000 (284.257 sec)\n",
            "I0629 13:05:51.318880 139887738599296 tpu_estimator.py:2159] global_step/sec: 3.51795\n",
            "I0629 13:05:51.319253 139887738599296 tpu_estimator.py:2160] examples/sec: 225.149\n",
            "I0629 13:05:52.334653 139887738599296 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 13:05:52.334912 139887738599296 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 13:05:52.335096 139886600554240 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 13:05:52.335194 139886600554240 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 13:05:52.335385 139887738599296 error_handling.py:96] infeed marked as finished\n",
            "I0629 13:05:52.335493 139887738599296 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 13:05:52.335581 139887738599296 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 13:05:52.335736 139886574077696 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 13:05:52.335826 139886574077696 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 13:05:52.336000 139887738599296 error_handling.py:96] outfeed marked as finished\n",
            "I0629 13:05:52.336095 139887738599296 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 13:05:54.203137 139887738599296 estimator.py:368] Loss for final step: 0.3301283.\n",
            "I0629 13:05:54.204193 139887738599296 error_handling.py:96] training_loop marked as finished\n",
            "I0629 13:05:54.204368 139887738599296 run_pretraining.py:475] ***** Running evaluation *****\n",
            "I0629 13:05:54.204454 139887738599296 run_pretraining.py:476]   Batch size = 8\n",
            "I0629 13:05:54.992729 139887738599296 estimator.py:1145] Calling model_fn.\n",
            "I0629 13:05:55.086533 139887738599296 run_pretraining.py:123] *** Features ***\n",
            "I0629 13:05:55.086806 139887738599296 run_pretraining.py:125]   name = input_ids, shape = (1, 512)\n",
            "I0629 13:05:55.086912 139887738599296 run_pretraining.py:125]   name = input_mask, shape = (1, 512)\n",
            "I0629 13:05:55.086997 139887738599296 run_pretraining.py:125]   name = masked_lm_ids, shape = (1, 20)\n",
            "I0629 13:05:55.087078 139887738599296 run_pretraining.py:125]   name = masked_lm_positions, shape = (1, 20)\n",
            "I0629 13:05:55.087162 139887738599296 run_pretraining.py:125]   name = masked_lm_weights, shape = (1, 20)\n",
            "I0629 13:05:55.087257 139887738599296 run_pretraining.py:125]   name = next_sentence_labels, shape = (1, 1)\n",
            "I0629 13:05:55.087345 139887738599296 run_pretraining.py:125]   name = segment_ids, shape = (1, 512)\n",
            "I0629 13:05:58.685091 139887738599296 run_pretraining.py:173] **** Trainable Variables ****\n",
            "I0629 13:05:58.685368 139887738599296 run_pretraining.py:179]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768)\n",
            "I0629 13:05:58.685495 139887738599296 run_pretraining.py:179]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0629 13:05:58.685587 139887738599296 run_pretraining.py:179]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0629 13:05:58.685676 139887738599296 run_pretraining.py:179]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.685760 139887738599296 run_pretraining.py:179]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.685857 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.685945 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.686023 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.686101 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.686184 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.686277 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.686352 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.686429 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.686502 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.686575 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.686647 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.686725 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.686799 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.686875 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.686948 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.687021 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.687093 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.687175 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.687262 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.687341 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.687414 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.687490 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.687563 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.687639 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.687714 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.687787 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.687859 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.687935 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.688009 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.688085 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.688166 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.688254 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.688329 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.688407 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.688481 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.688558 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.688633 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.688719 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.688802 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.688884 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.688958 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.689031 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.689105 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.689188 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.689276 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.689356 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.689429 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.689501 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.689575 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.689652 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.689726 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.689803 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.689877 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.689954 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.690029 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.690104 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.690183 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.690268 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.690341 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.690418 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.690491 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.690567 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.690640 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.690713 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.690787 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.690863 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.690936 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.691012 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.691085 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.691165 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.691251 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.691330 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.691402 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.691493 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.691568 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.691646 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.691719 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.691796 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.691870 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.691943 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.692017 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.692094 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.692174 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.692264 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.692339 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.692416 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.692488 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.692564 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.692636 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.692709 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.692781 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.692858 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.692931 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.693007 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.693081 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.693159 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.693247 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.693325 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.693397 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.693474 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.693549 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.693624 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.693699 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.693775 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.693847 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.693919 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.693991 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.694066 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.694144 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.694222 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.694311 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.694385 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.694456 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.694532 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.694605 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.694682 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.694755 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.694831 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.716998 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.717361 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.717492 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.717593 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.717690 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.717789 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.717888 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.717998 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.718101 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.718213 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.718336 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.718449 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.718555 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.718664 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.718768 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.718877 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.718980 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.719089 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.719201 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.719326 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.719455 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.719572 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.719678 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.719787 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.719892 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.719994 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.720093 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.720207 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.720335 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.720445 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.720547 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.720664 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.720773 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.720880 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.720983 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.721083 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.721196 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.721326 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.721433 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.721549 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.721667 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.721777 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.721881 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.721991 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.722093 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.722213 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.722341 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.722450 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.722549 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.722659 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.722764 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.722864 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.722963 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.723069 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.723181 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.723308 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.723413 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.723515 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.723617 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.723723 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 13:05:58.723824 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.723933 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 13:05:58.724034 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.724148 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 13:05:58.724271 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.724387 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.724491 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.724592 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.724695 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 13:05:58.724803 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 13:05:58.724905 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 13:05:58.725011 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.725116 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.725227 139887738599296 run_pretraining.py:179]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.725349 139887738599296 run_pretraining.py:179]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.725459 139887738599296 run_pretraining.py:179]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.725564 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0629 13:05:58.725668 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0629 13:05:58.725769 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 13:05:58.725869 139887738599296 run_pretraining.py:179]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 13:05:58.725970 139887738599296 run_pretraining.py:179]   name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0629 13:05:58.726069 139887738599296 run_pretraining.py:179]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0629 13:05:58.726184 139887738599296 run_pretraining.py:179]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0629 13:05:59.283804 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:204: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
            "\n",
            "W0629 13:05:59.300651 139887738599296 deprecation_wrapper.py:119] From src/run_pretraining.py:208: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "I0629 13:06:00.356783 139887738599296 estimator.py:1147] Done calling model_fn.\n",
            "I0629 13:06:00.375307 139887738599296 evaluation.py:255] Starting evaluation at 2019-06-29T13:06:00Z\n",
            "I0629 13:06:00.375581 139887738599296 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 13:06:00.901163 139887738599296 monitored_session.py:240] Graph was finalized.\n",
            "I0629 13:06:01.065716 139887738599296 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/eu.gureBERT/model.ckpt-10000\n",
            "I0629 13:06:24.608201 139887738599296 session_manager.py:500] Running local_init_op.\n",
            "I0629 13:06:24.831046 139887738599296 session_manager.py:502] Done running local_init_op.\n",
            "I0629 13:06:25.307175 139887738599296 tpu_estimator.py:557] Init TPU system\n",
            "I0629 13:06:33.942093 139887738599296 tpu_estimator.py:566] Initialized TPU in 8 seconds\n",
            "I0629 13:06:33.942871 139886756013824 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 13:06:33.943264 139886747621120 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 13:06:34.174344 139887738599296 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0629 13:06:34.364402 139887738599296 tpu_estimator.py:590] Enqueue next (100) batch(es) of data to infeed.\n",
            "I0629 13:06:34.364741 139887738599296 tpu_estimator.py:594] Dequeue next (100) batch(es) of data from outfeed.\n",
            "I0629 13:06:41.150196 139886747621120 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 13:06:43.320188 139887738599296 evaluation.py:167] Evaluation [100/100]\n",
            "I0629 13:06:43.320549 139887738599296 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 13:06:43.320647 139887738599296 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 13:06:43.320840 139886756013824 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 13:06:43.320957 139886756013824 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 13:06:43.321102 139887738599296 error_handling.py:96] infeed marked as finished\n",
            "I0629 13:06:43.321229 139887738599296 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 13:06:43.321324 139887738599296 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 13:06:44.063313 139886747621120 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 13:06:44.063584 139886747621120 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 13:06:44.063806 139887738599296 error_handling.py:96] outfeed marked as finished\n",
            "I0629 13:06:44.064006 139887738599296 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 13:06:44.664921 139887738599296 evaluation.py:275] Finished evaluation at 2019-06-29-13:06:44\n",
            "I0629 13:06:44.665283 139887738599296 estimator.py:2039] Saving dict for global step 10000: global_step = 10000, loss = 0.64757514, masked_lm_accuracy = 0.933996, masked_lm_loss = 0.41447976, next_sentence_accuracy = 1.0, next_sentence_loss = 0.0019768863\n",
            "I0629 13:06:49.262079 139887738599296 estimator.py:2099] Saving 'checkpoint_path' summary for global step 10000: gs://gurebert/gureBERT/eu.gureBERT/model.ckpt-10000\n",
            "I0629 13:06:50.198595 139887738599296 error_handling.py:96] evaluation_loop marked as finished\n",
            "I0629 13:06:50.198965 139887738599296 run_pretraining.py:489] ***** Eval results *****\n",
            "I0629 13:06:50.199102 139887738599296 run_pretraining.py:491]   global_step = 10000\n",
            "I0629 13:06:50.199516 139887738599296 run_pretraining.py:491]   loss = 0.64757514\n",
            "I0629 13:06:50.199642 139887738599296 run_pretraining.py:491]   masked_lm_accuracy = 0.933996\n",
            "I0629 13:06:50.199761 139887738599296 run_pretraining.py:491]   masked_lm_loss = 0.41447976\n",
            "I0629 13:06:50.199855 139887738599296 run_pretraining.py:491]   next_sentence_accuracy = 1.0\n",
            "I0629 13:06:50.199947 139887738599296 run_pretraining.py:491]   next_sentence_loss = 0.0019768863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxSQy-Gu47Bq",
        "colab_type": "code",
        "outputId": "8a2cb427-c92a-41df-d963-d5fc1062855a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "# pre training SQuAD\n",
        "\n",
        "# https://github.com/google-research/bert#squad-20\n",
        "\n",
        "#!git clone https://github.com/zmwebdev/bert.git\n",
        "#%cd bert\n",
        "\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
        "# evaluation script: download file from a url that returns a save dialog box : https://superuser.com/questions/795265/download-file-from-a-url-that-returns-a-save-dialog-box#795269\n",
        "!wget --content-disposition https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-29 14:24:12--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.109.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M   175MB/s    in 0.2s    \n",
            "\n",
            "2019-06-29 14:24:13 (175 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n",
            "--2019-06-29 14:24:14--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.109.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "dev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-06-29 14:24:14 (48.6 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n",
            "--2019-06-29 14:24:15--  https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
            "Resolving worksheets.codalab.org (worksheets.codalab.org)... 40.71.231.153\n",
            "Connecting to worksheets.codalab.org (worksheets.codalab.org)|40.71.231.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/x-python]\n",
            "Saving to: ‘evaluate-v2.0.py’\n",
            "\n",
            "evaluate-v2.0.py        [ <=>                ]  10.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-06-29 14:24:16 (188 MB/s) - ‘evaluate-v2.0.py’ saved [10547]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA0oUJsbBTaW",
        "colab_type": "code",
        "outputId": "376f0da0-e5a5-454a-c81e-7bcefdeeac93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# sentenpiece tokenizer  erabili behar da!!\n",
        "\n",
        "!python bert/run_squad.py \\\n",
        "  --vocab_file=./spModels/eu.vocab \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --do_lower_case=True \\\n",
        "  --do_train=True \\\n",
        "  --train_file=train-v2.0.json \\\n",
        "  --do_predict=True \\\n",
        "  --predict_file=dev-v2.0.json \\\n",
        "  --train_batch_size=24 \\\n",
        "  --learning_rate=3e-5 \\\n",
        "  --num_train_epochs=0.1 \\\n",
        "  --max_seq_length=384 \\\n",
        "  --doc_stride=128 \\\n",
        "  --output_dir=gs://gurebert/gureBERT/squad/ \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name=$TPU_NAME \\\n",
        "  --version_2_with_negative=True \\\n",
        "  #--init_checkpoint=$BERT_DIR \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 14:24:22.660527 139909234767744 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 14:24:22.662883 139909234767744 deprecation_wrapper.py:119] From bert/run_squad.py:1283: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 14:24:22.663583 139909234767744 deprecation_wrapper.py:119] From bert/run_squad.py:1127: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 14:24:22.663740 139909234767744 deprecation_wrapper.py:119] From bert/run_squad.py:1127: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 14:24:22.663872 139909234767744 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 14:24:22.664730 139909234767744 deprecation_wrapper.py:119] From bert/run_squad.py:1133: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0629 14:24:24.906227 139909234767744 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 14:24:25.911592 139909234767744 deprecation_wrapper.py:119] From bert/run_squad.py:229: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 14:24:37.744079 139909234767744 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f3ef1d5fbf8>) includes params argument, but params are not passed to Estimator.\n",
            "I0629 14:24:37.745940 139909234767744 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/squad/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.36.212.66:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3ef1d65320>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.36.212.66:8470', '_evaluation_master': 'grpc://10.36.212.66:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f3efe682f60>}\n",
            "I0629 14:24:37.746278 139909234767744 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "W0629 14:24:37.747064 139909234767744 deprecation_wrapper.py:119] From bert/run_squad.py:1065: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"bert/run_squad.py\", line 1283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"bert/run_squad.py\", line 1200, in main\n",
            "    output_fn=train_writer.process_feature)\n",
            "  File \"bert/run_squad.py\", line 391, in convert_examples_to_features\n",
            "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
            "  File \"/content/gureBERT/bert/tokenization.py\", line 179, in convert_tokens_to_ids\n",
            "    return convert_by_vocab(self.vocab, tokens)\n",
            "  File \"/content/gureBERT/bert/tokenization.py\", line 140, in convert_by_vocab\n",
            "    output.append(vocab[item])\n",
            "KeyError: '[CLS]'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lablZa6fsh51",
        "colab_type": "text"
      },
      "source": [
        "## wordpiece erabiliz\n",
        "\n",
        "- https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379\n",
        "- https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2XwxN-Z4a3m",
        "colab_type": "code",
        "outputId": "b708fe4d-623a-41e9-b591-a66b85bdeeb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format('./spModels/eu'))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 14705\n",
            "Sample tokens: ['emakumezko', '▁erla', 'olof', '▁40', 'ulara', '▁abiarazle', '▁uploadlogpage', '▁belle', '▁arazoen', '▁aipamena']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-YOATBGsg02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hau errepasatu. adibidez @ ez dut ondo egiten... [UNK] agertzen da...\n",
        "# adibidez, \n",
        "\n",
        "#import string\n",
        "#def parse_sentencepiece_token(token):\n",
        "#    if token.startswith(\"▁\"):\n",
        "#        return token[1:]\n",
        "#    else:\n",
        "#        if token in string.punctuation:\n",
        "#            return token\n",
        "#        else:\n",
        "#            return \"##\" + token\n",
        "          \n",
        "\n",
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYEtWr0D39Rf",
        "colab_type": "code",
        "outputId": "2dec5177-3053-4434-e716-fb56c2886ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import string\n",
        "\n",
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "# puntuazio sinboloak gehitu \n",
        "ctrl_symbols_end = list(string.punctuation)\n",
        "#ctrl_symbols = [\"[UNK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab + ctrl_symbols_end\n",
        "\n",
        "#bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(32000 - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiGAWsMw4CLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\"\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLLaP8_04uY4",
        "colab_type": "code",
        "outputId": "8b2c297d-1016-41a0-835a-540e6c1b71cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "from bert import tokenization\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(\"Nere kotxea aitonaren etxe alboan dago!, eta bere kolorea gorria da. ni@ni.eus erabili \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ne',\n",
              " '##re',\n",
              " 'ko',\n",
              " '##txea',\n",
              " 'ait',\n",
              " '##ona',\n",
              " '##ren',\n",
              " 'etxe',\n",
              " 'alb',\n",
              " '##oan',\n",
              " 'dago',\n",
              " '!',\n",
              " ',',\n",
              " 'eta',\n",
              " 'bere',\n",
              " 'kolore',\n",
              " '##a',\n",
              " 'gorria',\n",
              " 'da',\n",
              " '.',\n",
              " 'ni',\n",
              " '@',\n",
              " 'ni',\n",
              " '.',\n",
              " 'eus',\n",
              " 'erabili']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn54GJq95Uf7",
        "colab_type": "code",
        "outputId": "1ef864b3-7b3b-4c77-cff7-d9b52e5ac679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python bert/create_pretraining_data.py \\\n",
        "  --input_file=$GS/corpus/eu/2014wiki.eu.sent_splited \\\n",
        "  --output_file=$GS/wordpiece/pretraining.tf.data \\\n",
        "  --vocab_file=vocab.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=666 \\\n",
        "  --do_whole_word_mask=True \\\n",
        "  #--dupe_factor=5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 14:27:51.331921 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 14:27:51.332679 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 14:27:51.332850 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 14:27:51.332988 140698090002304 deprecation_wrapper.py:119] From /content/gureBERT/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 14:27:51.387063 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 14:27:52.573521 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 14:27:52.573760 140698090002304 create_pretraining_data.py:446] *** Reading from input files ***\n",
            "I0629 14:27:52.573846 140698090002304 create_pretraining_data.py:448]   gs://gurebert/gureBERT/corpus/eu/2014wiki.eu.sent_splited\n",
            "I0629 14:28:04.601530 140698090002304 create_pretraining_data.py:457] *** Writing to output files ***\n",
            "I0629 14:28:04.601782 140698090002304 create_pretraining_data.py:459]   gs://gurebert/gureBERT/wordpiece/pretraining.tf.data\n",
            "W0629 14:28:04.601961 140698090002304 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0629 14:28:04.603364 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.603535 140698090002304 create_pretraining_data.py:151] tokens: [CLS] [MASK] [SEP] disambiguations [SEP]\n",
            "I0629 14:28:04.603769 140698090002304 create_pretraining_data.py:161] input_ids: 2 4 3 4887 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.603982 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.604194 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.604281 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.604363 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 496 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.604471 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.604547 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.605411 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.605550 140698090002304 create_pretraining_data.py:151] tokens: [CLS] bai , datu - basea blokeatu nahi dut . [SEP] kirolak [MASK] [MASK] heriotzak * jamaika [MASK] 1 ##a - mart ##zel ##o ii . a aita santua [MASK] [MASK] [MASK] hilabete baino gutxiago eman ostean . * maiatz ##aren 25 [MASK] enrike ii . [MASK] , nafarroako erregea . - - - - agintari ##ak [SEP]\n",
            "I0629 14:28:04.605771 140698090002304 create_pretraining_data.py:161] input_ids: 2 431 14721 340 14722 9946 952 135 324 14723 3 74 4 4 70 14719 7652 4 218 16 14722 6308 3233 39 271 14723 77 274 940 4 4 4 1233 192 1666 139 804 14723 14719 318 27 356 4 5495 271 14723 4 14721 585 531 14723 14722 14722 14722 14722 94 23 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.605977 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.606178 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.606264 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 12 13 16 17 29 30 31 42 46 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.606343 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 86 20 318 27 14721 3204 22 234 77 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.606447 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.606524 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.607523 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.607639 140698090002304 create_pretraining_data.py:151] tokens: [CLS] colum ##ns [SEP] ezin izan zaio \" [MASK] 1 \" fitxategia ##ri \" $ 2 \" [MASK] berria [MASK] . [SEP]\n",
            "I0629 14:28:04.607862 140698090002304 create_pretraining_data.py:161] input_ids: 2 3772 2827 3 367 35 664 14711 4 218 14711 1212 83 14711 14713 422 14711 4 408 4 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.608070 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.608278 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.608363 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 8 17 19 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.608461 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 14713 655 139 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.608549 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.608622 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.609474 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.609719 140698090002304 create_pretraining_data.py:151] tokens: [CLS] musika , bitarteko ##tzat soinua eta isiltasuna darabilt ##zan artea [MASK] . musika sortze ##n duen pertsona ##ri musikagile deitze ##n zaio ; musika jotze ##n edo abest ##en duenari , musikari ; eta musika aztertzen duenari , musik ##ologo . historia musika soinu - konbina ##zio edo - [MASK] da ; soinua ##k giza organo fon ##atzaileek ( ah ##ot ##s musika ) edota horretarako diseinatu ##riko musika tresne ##k sortua ##k ( musika instrumentala ) dira , zibilizazio batetik bestera aldatzen diren printzipio teknikoe ##n arabera landu ##ak [MASK] antolatua ##k . herri musika ##ren jatorria historiaurrea ##n du ##go . segur aski , hasiera batean erlijioa ##ri [MASK] fax lotur ##ik izan ##go zen , eta egin ##kizun estetikoa baino [MASK] [MASK] ##ko - sin ##boli ##koa izan ##go zuen . musika [MASK] hori talde etniko [MASK] kultura ##l guztiek dute , gehienbat ahozko tradizio ##z transmitit ##zen da , eta [MASK] garapena ##k eta eboluzioa ##k bide desberdina ##k jarraitu dituzte zibilizazio bakoitzean , baina , betiere , teknikoki nahi ##z instrumental ##ki sinplea izaten da , eta gai jakin baten inguruan bar ##ia ##zioak inp ##ro ##bis ##atzeko aukera ematen [MASK] . musika horren barruan sartze ##n dira , hasi tribu primiti ##bo ##en abesti ##etatik eta [MASK] egung ##o jazz ##eraino [MASK] herrialde desberdineta ##ko folklor ##ea [MASK] [MASK] duten manifestazio musika ##l guztieta ##tik igarot ##a . mendebaldean , musika jaso ##aren garapena grezian eta erroman hasi zen , eta handik europa osora zabaldu zen , bi bide erabiliz : musika prof ##anoa eta erlijiosoa . azken hori , kristautasuna ##ren heda ##pena ##ri zuzenean lotua . gregoriar kantua vi [MASK] mendean sortu zen . jarraian etorri zen , vii . mendean , polifoni ##a ( grekot ##ik dator hitza , eta « ah ##ot ##s asko [UNK] esan nahi du ) . pentag ##rama edo paper pau ##tatuan musika idazteko modua xi . mendean asmatu zen . [SEP] martxoa ##ren 2 [SEP]\n",
            "I0629 14:28:04.658286 140698090002304 create_pretraining_data.py:161] input_ids: 2 162 14721 5387 368 11044 14 7069 9173 3956 6427 4 14723 162 1190 22 134 267 83 1423 1451 22 664 14726 162 10470 22 66 13817 25 5601 14721 849 14726 14 162 487 5601 14721 5923 5562 14723 146 162 3200 14722 12807 937 66 14722 4 28 14726 11044 20 1076 5933 5874 12469 14717 14074 1535 47 162 14718 874 1130 5474 994 162 4731 20 9133 20 14717 162 8188 14718 41 14721 1620 1513 4289 1864 156 3972 13449 22 212 1647 23 4 10920 20 14723 243 162 21 1532 5814 22 69 60 14723 10924 5485 14721 610 127 3310 83 4 11140 14310 190 35 60 24 14721 14 63 1762 7548 192 4 4 17 14722 1275 11769 786 35 60 30 14723 162 4 128 237 4790 4 58 205 924 120 14721 2525 2826 3436 45 4995 220 28 14721 14 4 3332 20 14 1964 20 837 3363 20 968 282 1620 3823 14721 92 14721 3763 14721 7725 135 45 11968 440 11138 983 28 14721 14 283 670 177 675 887 186 6741 6391 745 5808 2082 967 360 4 14723 162 428 1239 5728 22 41 14721 96 1352 7335 2650 25 6995 420 14 4 6557 39 4059 12600 4 355 10609 17 7504 947 4 4 236 1851 162 205 1613 59 10155 16 14723 1068 14721 162 1147 27 3332 4069 14 2189 96 24 14721 14 2687 429 9688 462 24 14721 99 837 818 14725 162 10013 14118 14 3354 14723 160 128 14721 2202 21 4791 10883 83 1884 4047 14723 9607 4336 573 4 329 98 24 14723 1242 900 24 14721 825 14723 329 14721 6807 16 14717 14145 190 2433 1085 14721 14 737 14074 1535 47 284 1 270 135 69 14718 14723 8763 10346 66 3922 3467 13283 162 5428 6771 1490 14723 329 991 24 14723 3 514 21 422 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.658822 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.659170 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.659320 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 11 50 91 111 112 124 125 126 136 140 145 155 196 213 218 224 225 278 283 287\n",
            "I0629 14:28:04.659469 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 28 5356 14 164 10817 6342 5963 17 489 14 14721 53 69 257 14721 681 104 14723 1242 825\n",
            "I0629 14:28:04.659601 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:28:04.659718 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.660927 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.661057 140698090002304 create_pretraining_data.py:151] tokens: [CLS] erakutsi azken $ [MASK] aldaketak [MASK] 2 egun ##etan . [SEP] image ##page [SEP]\n",
            "I0629 14:28:04.661310 140698090002304 create_pretraining_data.py:161] input_ids: 2 701 160 14713 4 612 4 422 56 87 14723 3 2554 562 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.661542 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.661763 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.661859 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 4 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.661940 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 218 14713 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.662024 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.662094 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.662963 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.663069 140698090002304 create_pretraining_data.py:151] tokens: [CLS] image ##revert ##ed [SEP] delete ##comment [SEP]\n",
            "I0629 14:28:04.663285 140698090002304 create_pretraining_data.py:161] input_ids: 2 2554 11857 661 3 3358 4718 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.663508 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.663717 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.663802 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.663882 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.663965 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.664037 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.664889 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.664994 140698090002304 create_pretraining_data.py:151] tokens: [CLS] head ##line [MASK] [SEP] text ##match ##es [SEP]\n",
            "I0629 14:28:04.665207 140698090002304 create_pretraining_data.py:161] input_ids: 2 4011 2311 4 3 8907 2538 199 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.665446 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.665664 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.665754 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.665832 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 1277 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.665914 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.665985 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.666849 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.666951 140698090002304 create_pretraining_data.py:151] tokens: [CLS] ipb ##lock ##list [SEP] ara ##katu [SEP]\n",
            "I0629 14:28:04.667162 140698090002304 create_pretraining_data.py:161] input_ids: 2 4135 11740 2344 3 11549 6458 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.667375 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.667593 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.761057 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.761304 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.761462 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.761572 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.763029 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.763186 140698090002304 create_pretraining_data.py:151] tokens: [CLS] old ##password [SEP] [MASK] helbide ##rik ez [SEP]\n",
            "I0629 14:28:04.763431 140698090002304 create_pretraining_data.py:161] input_ids: 2 9686 2380 3 4 1274 52 36 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.763645 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.763862 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.763949 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.764030 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 2263 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.764119 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.764193 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.765066 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.765181 140698090002304 create_pretraining_data.py:151] tokens: [CLS] next ##n [SEP] kirolak [MASK] [MASK] * uztaila ##ren [MASK] ##a - giorg ##io vas ##ari italiar margolari eta arkitektoa . heriotzak - - - - agintari ##ak [SEP]\n",
            "I0629 14:28:04.765411 140698090002304 create_pretraining_data.py:161] input_ids: 2 8068 22 3 74 4 4 14719 773 21 4 16 14722 5124 526 6551 153 438 605 14 2668 14723 70 14722 14722 14722 14722 94 23 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.765640 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.765848 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.765938 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 5 6 10 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.766017 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 86 20 430 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.766101 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.766174 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.767030 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.767133 140698090002304 create_pretraining_data.py:151] tokens: [CLS] gertaerak [SEP] [MASK] [SEP]\n",
            "I0629 14:28:04.767348 140698090002304 create_pretraining_data.py:161] input_ids: 2 72 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.767568 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.767780 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.767867 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.767947 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 14719 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.768030 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.768105 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 14:28:04.768952 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.769061 140698090002304 create_pretraining_data.py:151] tokens: [CLS] orrialde bereziak [SEP] izen - [MASK] hauetan [MASK] : $ 1 $ 2 birzuzenkete ##n zerrenda $ 3 $ 9 1690) [SEP]\n",
            "I0629 14:28:04.769275 140698090002304 create_pretraining_data.py:161] input_ids: 2 189 3137 3 655 14722 4 1678 4 14725 14713 218 14713 422 14304 22 178 14713 187 14713 686 8687 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.769495 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.769707 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.864479 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 6 8 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.864718 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 5994 1645 1645 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.864897 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.865170 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.869458 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.869652 140698090002304 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] by title [SEP] heriotzak - - - - agintari ##ak [SEP]\n",
            "I0629 14:28:04.870024 140698090002304 create_pretraining_data.py:161] input_ids: 2 4 4 1539 9625 3 70 14722 14722 14722 14722 94 23 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.870371 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.870724 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.870842 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.870944 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 2272 9080 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.871060 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.871155 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.872426 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.872573 140698090002304 create_pretraining_data.py:151] tokens: [CLS] unblockip [SEP] zure hobespenak gorde dira [MASK] [SEP]\n",
            "I0629 14:28:04.872892 140698090002304 create_pretraining_data.py:161] input_ids: 2 7403 3 474 2102 883 41 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.873401 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.874418 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.874642 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.874773 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 14723 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.874881 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.874965 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.876312 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.876444 140698090002304 create_pretraining_data.py:151] tokens: [CLS] bol ##d tip [SEP] [MASK] [SEP]\n",
            "I0629 14:28:04.876710 140698090002304 create_pretraining_data.py:161] input_ids: 2 2860 232 1277 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.877016 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.877262 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.877355 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.877458 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 1500 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.877539 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.877607 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.878518 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.878636 140698090002304 create_pretraining_data.py:151] tokens: [CLS] watchnologin ##text [SEP] aldaketa [MASK] [SEP]\n",
            "I0629 14:28:04.878882 140698090002304 create_pretraining_data.py:161] input_ids: 2 7339 191 3 393 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.879196 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.879544 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.970041 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.970332 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 535 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.970508 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.970775 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.972299 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.972450 140698090002304 create_pretraining_data.py:151] tokens: [CLS] ext ##link samp ##le [SEP] notargett ##itle [SEP]\n",
            "I0629 14:28:04.972678 140698090002304 create_pretraining_data.py:161] input_ids: 2 11291 1596 2341 286 3 7699 2024 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.972888 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.973093 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.973177 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.973278 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.973367 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.973464 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:04.974359 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.974545 140698090002304 create_pretraining_data.py:151] tokens: [CLS] [MASK] eta teknologia kirolak jaiotza ##k euskal [MASK] mundua [MASK] 1820 - william te ##cum ##seh sher ##man , sistematiko militarra , [MASK] [MASK] eta idazlea ( h . [SEP] 1891 ) . * 1828 - [MASK] [MASK] verne [MASK] idazle frantziarra * 1828 - antoni ##o can ##ova ##s del cast ##illo espainiar politikaria . * 1925 - jack lem ##mon , aktore [MASK] . [MASK] 1931 - james dean , [MASK] estatubatuarra . heriotzak euskal herria * 2012 - jose lui ##s aka ##rre ##gi , euskal pilotaria ( j . 1923 [MASK] . ##sada * 1921 - pio ##tr kro ##po ##t ##kin , anarkista errusiarra ( j [MASK] [MASK] ) . jaiak eta urteurrena ##k * se ##haska egutegi ##ko [MASK] [MASK] : x ##ant ##ian ##a eta ustaritz / ustaritz ##e . [SEP]\n",
            "I0629 14:28:04.974777 140698090002304 create_pretraining_data.py:161] input_ids: 2 4 14 71 74 86 20 38 4 57 4 4615 14722 417 966 4357 8704 9982 503 14721 11231 768 14721 4 4 14 97 14717 151 14723 3 1267 14718 14723 14719 6649 14722 4 4 6073 4 89 695 14719 6649 14722 813 39 4736 14291 47 1428 9954 11505 399 265 14723 14719 1563 14722 771 6385 4686 14721 110 4 14723 4 693 14722 262 4850 14721 4 114 14723 70 38 55 14719 312 14722 196 662 47 14593 10405 1118 14721 38 3062 14717 195 14723 4505 4 14723 10663 14719 1553 14722 1918 4220 3375 1971 118 982 14721 7098 863 14717 195 4 4 14718 14723 172 14 167 20 14719 112 170 90 17 4 4 14725 484 2800 1747 16 14 8286 14724 8286 44 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.974992 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.975207 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.975294 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 1 2 8 10 20 23 24 37 38 40 60 65 67 73 95 97 112 113 125 126\n",
            "I0629 14:28:04.975395 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 54 14 55 14719 103 4381 2224 6714 47 14721 771 114 14719 110 14718 57 14723 11397 176 161\n",
            "I0629 14:28:04.975488 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 14:28:04.975559 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 14:28:04.976414 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.976535 140698090002304 create_pretraining_data.py:151] tokens: [CLS] ezin [MASK] $ 2 [MASK] eztabaida ) wikilaria ##k [MASK] $ 1 [UNK] orrian egindako [MASK] aldaketa desegin [MASK] geroztik , beste ##ren batek editatu [MASK] edo jada desegin du [MASK] [SEP] azken aldaketa $ 3 ( eztabaida ) wikilaria ##k egin du . [SEP]\n",
            "I0629 14:28:04.976761 140698090002304 create_pretraining_data.py:161] input_ids: 2 367 4 14713 422 4 379 14718 13790 20 4 14713 218 1 1705 567 4 393 1354 4 1353 14721 61 21 140 3002 4 66 2543 1354 69 4 3 160 393 14713 187 14717 379 14718 13790 20 63 69 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.976993 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.977210 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.977298 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 2 5 10 16 19 26 31 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.977392 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 28 14717 737 160 14726 69 14723 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.977479 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:04.977550 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 14:28:04.978444 140698090002304 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 14:28:04.978586 140698090002304 create_pretraining_data.py:151] tokens: [CLS] up ##dat ##ed [SEP] xvii . [MASK] priest taula [SEP]\n",
            "I0629 14:28:04.978808 140698090002304 create_pretraining_data.py:161] input_ids: 2 13127 9627 661 3 980 14723 4 7575 672 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.979019 140698090002304 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:04.979233 140698090002304 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:05.076257 140698090002304 create_pretraining_data.py:161] masked_lm_positions: 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:05.076543 140698090002304 create_pretraining_data.py:161] masked_lm_ids: 415 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:28:05.076719 140698090002304 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 14:28:05.076859 140698090002304 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 14:28:20.369910 140698090002304 create_pretraining_data.py:166] Wrote 20467 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvMUE9sT6P1Y",
        "colab_type": "code",
        "outputId": "48fef8d0-0387-4e03-cccb-1cd1736410b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "!python src/run_pretraining.py \\\n",
        "  --config_file eu.congif.ini \\\n",
        "  --input_file=$GS/pretraining.tf.data \\\n",
        "  --output_dir=$GS/eu.gureBERT \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=1000000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "'''\n",
        "\n",
        "!python bert/run_pretraining.py \\\n",
        "  --input_file=$GS/wordpiece/pretraining.tf.data \\ \\\n",
        "  --output_dir=$GS/wordpiece/model \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=10000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  #--num_train_steps=1000000 \\\n",
        "  #--init_checkpoint=$GS/wordpiece/model \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 14:43:18.416723 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 14:43:18.417868 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:493: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 14:43:18.418495 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:407: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 14:43:18.418647 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:407: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 14:43:18.418800 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 14:43:18.419515 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:414: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0629 14:43:19.794317 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:418: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 14:43:19.957866 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:420: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 14:43:19.958102 139621729412992 run_pretraining.py:420] *** Input Files ***\n",
            "I0629 14:43:19.958192 139621729412992 run_pretraining.py:422]   gs://gurebert/gureBERT/wordpiece/pretraining.tf.data\n",
            "W0629 14:43:20.898190 139621729412992 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 14:43:21.903313 139621729412992 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7efc017958c8>) includes params argument, but params are not passed to Estimator.\n",
            "I0629 14:43:21.904874 139621729412992 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/wordpiece/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.36.212.66:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7efc0dbfc5f8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.36.212.66:8470', '_evaluation_master': 'grpc://10.36.212.66:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7efc0dc06320>}\n",
            "I0629 14:43:21.905195 139621729412992 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0629 14:43:21.905850 139621729412992 run_pretraining.py:459] ***** Running training *****\n",
            "I0629 14:43:21.905941 139621729412992 run_pretraining.py:460]   Batch size = 64\n",
            "I0629 14:43:22.253876 139621729412992 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.36.212.66:8470) for TPU system metadata.\n",
            "2019-06-29 14:43:22.255221: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 14:43:22.268328 139621729412992 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0629 14:43:22.268509 139621729412992 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0629 14:43:22.268592 139621729412992 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0629 14:43:22.268658 139621729412992 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0629 14:43:22.268725 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10173880149761554381)\n",
            "I0629 14:43:22.269498 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13648402031402572782)\n",
            "I0629 14:43:22.269571 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16511697586746770188)\n",
            "I0629 14:43:22.269636 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 4131540690459295465)\n",
            "I0629 14:43:22.269721 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15236588464056244225)\n",
            "I0629 14:43:22.269787 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 11456561882886992321)\n",
            "I0629 14:43:22.269848 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2519461556312082510)\n",
            "I0629 14:43:22.269910 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 11930862125451840624)\n",
            "I0629 14:43:22.269969 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10570494583918704491)\n",
            "I0629 14:43:22.270029 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1546865553820533360)\n",
            "I0629 14:43:22.270090 139621729412992 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1239985580371320602)\n",
            "W0629 14:43:22.276013 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0629 14:43:22.288680 139621729412992 estimator.py:1145] Calling model_fn.\n",
            "W0629 14:43:22.289170 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0629 14:43:22.295006 139621729412992 deprecation.py:323] From bert/run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0629 14:43:22.295165 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0629 14:43:22.319609 139621729412992 deprecation.py:323] From bert/run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0629 14:43:22.319752 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0629 14:43:22.321086 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:393: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0629 14:43:22.326452 139621729412992 deprecation.py:323] From bert/run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0629 14:43:22.403764 139621729412992 run_pretraining.py:117] *** Features ***\n",
            "I0629 14:43:22.403985 139621729412992 run_pretraining.py:119]   name = input_ids, shape = (8, 512)\n",
            "I0629 14:43:22.404090 139621729412992 run_pretraining.py:119]   name = input_mask, shape = (8, 512)\n",
            "I0629 14:43:22.404177 139621729412992 run_pretraining.py:119]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0629 14:43:22.404266 139621729412992 run_pretraining.py:119]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0629 14:43:22.404348 139621729412992 run_pretraining.py:119]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0629 14:43:22.404448 139621729412992 run_pretraining.py:119]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0629 14:43:22.404531 139621729412992 run_pretraining.py:119]   name = segment_ids, shape = (8, 512)\n",
            "W0629 14:43:22.404757 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0629 14:43:22.406776 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0629 14:43:22.440623 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0629 14:43:22.595678 139621729412992 deprecation.py:506] From /content/gureBERT/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0629 14:43:22.616803 139621729412992 deprecation.py:323] From /content/gureBERT/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0629 14:43:26.349247 139621729412992 run_pretraining.py:167] **** Trainable Variables ****\n",
            "I0629 14:43:26.349500 139621729412992 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768)\n",
            "I0629 14:43:26.349629 139621729412992 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0629 14:43:26.349723 139621729412992 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0629 14:43:26.349814 139621729412992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.349900 139621729412992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.349981 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.350063 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.350141 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.350228 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.350304 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.350397 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.350474 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.350554 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.350629 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.350703 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.350777 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.350857 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.350930 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.351008 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.351083 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.351157 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.351237 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.351315 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.351402 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.351481 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.351556 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.351633 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.351707 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.351786 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.351861 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.351933 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.352007 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.352084 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.352159 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.352245 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.352322 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.352409 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.352487 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.352566 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.352641 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.352720 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.352796 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.352873 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.352947 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.353025 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.353099 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.353173 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.353253 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.353330 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.353418 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.353498 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.353572 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.353647 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.353722 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.353802 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.353878 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.353958 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.354033 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.354112 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.354188 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.354304 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.354398 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.354475 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.354550 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.354630 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.354706 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.354785 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.354862 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.354936 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.355012 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.355091 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.355167 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.355252 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.355329 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.355420 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.355497 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.355575 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.355649 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.355722 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.355798 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.355876 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.355950 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.356029 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.356103 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.356176 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.356256 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.356334 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.356437 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.356519 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.356592 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.356670 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.356746 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.356825 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.356901 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.356975 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.357048 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.357126 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.357201 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.357285 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.357358 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.357446 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.357520 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.357598 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.357673 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.357752 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.357827 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.357905 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.357980 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.358057 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.358131 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.358210 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.358285 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.358362 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.358451 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.358530 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.358607 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.358682 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.358756 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.358835 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.358910 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.358987 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.359061 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.431471 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.431774 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.431921 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.432029 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.432135 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.432262 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.432397 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.432513 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.432628 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.432738 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.432848 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.432953 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.433065 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.433170 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.433297 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.433422 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.433540 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.433648 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.433757 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.433862 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.433968 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.434073 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.434182 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.434305 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.434436 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.434545 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.434650 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.434756 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.434868 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.434973 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.435085 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.435191 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.435316 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.435438 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.435553 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.435657 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.435759 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.435863 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.435973 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.436076 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.436186 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.436304 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.436426 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.436533 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.436646 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.436751 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.436861 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.436962 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.437073 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.437179 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.437299 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.437424 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.437534 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.437637 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.437747 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.437855 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.437963 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.438069 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.438172 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.438289 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.438414 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 14:43:26.438525 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.438637 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 14:43:26.438742 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.438853 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 14:43:26.438961 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.439071 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.439174 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.439291 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.439412 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 14:43:26.439526 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 14:43:26.439632 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 14:43:26.439741 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.439848 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.439951 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.440054 139621729412992 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.440166 139621729412992 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.440282 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0629 14:43:26.440411 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0629 14:43:26.440522 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 14:43:26.440629 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 14:43:26.440733 139621729412992 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0629 14:43:26.440838 139621729412992 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0629 14:43:26.440950 139621729412992 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0629 14:43:26.441147 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0629 14:43:26.442971 139621729412992 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0629 14:43:26.451014 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0629 14:43:26.723497 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0629 14:43:40.094371 139621729412992 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0629 14:43:40.735457 139621729412992 estimator.py:1147] Done calling model_fn.\n",
            "I0629 14:43:44.249897 139621729412992 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 14:43:45.621609 139621729412992 monitored_session.py:240] Graph was finalized.\n",
            "I0629 14:43:56.204694 139621729412992 session_manager.py:500] Running local_init_op.\n",
            "I0629 14:43:57.315330 139621729412992 session_manager.py:502] Done running local_init_op.\n",
            "I0629 14:44:09.169803 139621729412992 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://gurebert/gureBERT/wordpiece/model/model.ckpt.\n",
            "W0629 14:44:38.667258 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0629 14:44:40.952161 139621729412992 util.py:98] Initialized dataset iterators in 1 seconds\n",
            "I0629 14:44:40.953270 139621729412992 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-06-29 14:44:40.953657: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 14:44:40.957974 139621729412992 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0629 14:44:40.959899 139621729412992 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0629 14:44:40.963664 139621729412992 tpu_estimator.py:557] Init TPU system\n",
            "I0629 14:44:45.529625 139621729412992 tpu_estimator.py:566] Initialized TPU in 4 seconds\n",
            "I0629 14:44:45.530617 139620579305216 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 14:44:45.531130 139620552566528 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 14:44:46.636756 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 14:44:46.637878 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 14:45:19.172789 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 14:46:19.242447 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (0, 252)\n",
            "I0629 14:47:19.313048 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (0, 504)\n",
            "I0629 14:48:19.384603 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (0, 756)\n",
            "I0629 14:49:18.960929 139621729412992 basic_session_run_hooks.py:262] loss = 8.012552, step = 1000\n",
            "I0629 14:49:18.963466 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 14:49:18.963687 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 14:49:28.226758 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (1, 0)\n",
            "I0629 14:50:28.293938 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (1, 252)\n",
            "I0629 14:51:28.363535 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (1, 504)\n",
            "I0629 14:52:28.432737 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (1, 756)\n",
            "I0629 14:53:27.216873 139621729412992 basic_session_run_hooks.py:260] loss = 6.979651, step = 2000 (248.256 sec)\n",
            "I0629 14:53:27.218307 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.0281\n",
            "I0629 14:53:27.219255 139621729412992 tpu_estimator.py:2160] examples/sec: 257.799\n",
            "I0629 14:53:28.114691 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 14:53:28.115189 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 14:53:29.534754 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (2, 0)\n",
            "I0629 14:54:29.601197 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (2, 252)\n",
            "I0629 14:55:29.668438 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (2, 504)\n",
            "I0629 14:56:29.734620 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (2, 756)\n",
            "I0629 14:57:28.486635 139621729412992 basic_session_run_hooks.py:260] loss = 5.267299, step = 3000 (241.270 sec)\n",
            "I0629 14:57:28.488214 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.14474\n",
            "I0629 14:57:29.366555 139621729412992 tpu_estimator.py:2160] examples/sec: 265.263\n",
            "I0629 14:57:29.368444 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 14:57:29.368669 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 14:57:30.672204 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (3, 0)\n",
            "I0629 14:58:30.737096 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (3, 252)\n",
            "I0629 14:59:30.802705 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (3, 504)\n",
            "I0629 15:00:30.868494 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (3, 756)\n",
            "I0629 15:01:30.695654 139621729412992 basic_session_run_hooks.py:260] loss = 5.717806, step = 4000 (242.209 sec)\n",
            "I0629 15:01:30.697094 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.12867\n",
            "I0629 15:01:30.697449 139621729412992 tpu_estimator.py:2160] examples/sec: 264.235\n",
            "I0629 15:01:30.698603 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:01:30.698791 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:01:32.090096 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (4, 0)\n",
            "I0629 15:02:32.159316 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (4, 252)\n",
            "I0629 15:03:32.229675 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (4, 504)\n",
            "I0629 15:04:32.299462 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (4, 756)\n",
            "I0629 15:05:31.083400 139621729412992 basic_session_run_hooks.py:260] loss = 4.143191, step = 5000 (240.388 sec)\n",
            "I0629 15:05:31.085220 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.15994\n",
            "I0629 15:05:31.085409 139621729412992 tpu_estimator.py:2160] examples/sec: 266.236\n",
            "I0629 15:05:32.114043 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:05:32.114781 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:05:33.523420 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (5, 0)\n",
            "I0629 15:06:33.591883 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (5, 252)\n",
            "I0629 15:07:33.661691 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (5, 504)\n",
            "I0629 15:08:33.731530 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (5, 756)\n",
            "I0629 15:09:32.515052 139621729412992 basic_session_run_hooks.py:260] loss = 4.916213, step = 6000 (241.432 sec)\n",
            "I0629 15:09:32.516788 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.14196\n",
            "I0629 15:09:32.516986 139621729412992 tpu_estimator.py:2160] examples/sec: 265.086\n",
            "I0629 15:09:33.464939 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:09:33.465581 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:09:34.852605 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (6, 0)\n",
            "I0629 15:10:34.919304 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (6, 252)\n",
            "I0629 15:11:34.987476 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (6, 504)\n",
            "I0629 15:12:35.053251 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (6, 756)\n",
            "I0629 15:13:34.809849 139621729412992 basic_session_run_hooks.py:260] loss = 1.476404, step = 7000 (242.295 sec)\n",
            "I0629 15:13:34.811696 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.1272\n",
            "I0629 15:13:34.811857 139621729412992 tpu_estimator.py:2160] examples/sec: 264.141\n",
            "I0629 15:13:34.813434 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:13:34.813604 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:13:36.227687 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (7, 0)\n",
            "I0629 15:14:36.292591 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (7, 252)\n",
            "I0629 15:15:36.358829 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (7, 504)\n",
            "I0629 15:16:36.425096 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (7, 756)\n",
            "I0629 15:17:35.195486 139621729412992 basic_session_run_hooks.py:260] loss = 2.120125, step = 8000 (240.386 sec)\n",
            "I0629 15:17:35.197097 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.15999\n",
            "I0629 15:17:35.197261 139621729412992 tpu_estimator.py:2160] examples/sec: 266.239\n",
            "I0629 15:17:36.081555 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:17:36.082070 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:17:37.515753 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (8, 0)\n",
            "I0629 15:18:37.581182 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (8, 252)\n",
            "I0629 15:19:37.647189 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (8, 504)\n",
            "I0629 15:20:37.713159 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (8, 756)\n",
            "I0629 15:21:36.440223 139621729412992 basic_session_run_hooks.py:260] loss = 0.74855816, step = 9000 (241.245 sec)\n",
            "I0629 15:21:36.441756 139621729412992 tpu_estimator.py:2159] global_step/sec: 4.14517\n",
            "I0629 15:21:37.415345 139621729412992 tpu_estimator.py:2160] examples/sec: 265.291\n",
            "I0629 15:21:37.417267 139621729412992 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0629 15:21:37.417520 139621729412992 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0629 15:21:38.757766 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (9, 0)\n",
            "I0629 15:22:38.823191 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (9, 252)\n",
            "I0629 15:23:38.889420 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (9, 504)\n",
            "I0629 15:24:38.957069 139620552566528 tpu_estimator.py:275] Outfeed finished for iteration (9, 756)\n",
            "I0629 15:25:37.698538 139621729412992 basic_session_run_hooks.py:606] Saving checkpoints for 10000 into gs://gurebert/gureBERT/wordpiece/model/model.ckpt.\n",
            "I0629 15:26:05.393211 139621729412992 basic_session_run_hooks.py:260] loss = 0.70103675, step = 10000 (268.953 sec)\n",
            "I0629 15:26:05.395117 139621729412992 tpu_estimator.py:2159] global_step/sec: 3.71812\n",
            "I0629 15:26:05.395523 139621729412992 tpu_estimator.py:2160] examples/sec: 237.96\n",
            "I0629 15:26:06.430169 139621729412992 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 15:26:06.430505 139621729412992 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 15:26:06.430712 139620579305216 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 15:26:06.430819 139620579305216 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 15:26:06.431037 139621729412992 error_handling.py:96] infeed marked as finished\n",
            "I0629 15:26:06.431169 139621729412992 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 15:26:06.431255 139621729412992 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 15:26:06.431420 139620552566528 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 15:26:06.431507 139620552566528 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 15:26:06.431634 139621729412992 error_handling.py:96] outfeed marked as finished\n",
            "I0629 15:26:06.431729 139621729412992 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 15:26:08.297458 139621729412992 estimator.py:368] Loss for final step: 0.70103675.\n",
            "I0629 15:26:08.298751 139621729412992 error_handling.py:96] training_loop marked as finished\n",
            "I0629 15:26:08.298901 139621729412992 run_pretraining.py:469] ***** Running evaluation *****\n",
            "I0629 15:26:08.298988 139621729412992 run_pretraining.py:470]   Batch size = 8\n",
            "I0629 15:26:09.132520 139621729412992 estimator.py:1145] Calling model_fn.\n",
            "I0629 15:26:09.230249 139621729412992 run_pretraining.py:117] *** Features ***\n",
            "I0629 15:26:09.230603 139621729412992 run_pretraining.py:119]   name = input_ids, shape = (1, 512)\n",
            "I0629 15:26:09.230720 139621729412992 run_pretraining.py:119]   name = input_mask, shape = (1, 512)\n",
            "I0629 15:26:09.230808 139621729412992 run_pretraining.py:119]   name = masked_lm_ids, shape = (1, 20)\n",
            "I0629 15:26:09.230891 139621729412992 run_pretraining.py:119]   name = masked_lm_positions, shape = (1, 20)\n",
            "I0629 15:26:09.230974 139621729412992 run_pretraining.py:119]   name = masked_lm_weights, shape = (1, 20)\n",
            "I0629 15:26:09.231053 139621729412992 run_pretraining.py:119]   name = next_sentence_labels, shape = (1, 1)\n",
            "I0629 15:26:09.231130 139621729412992 run_pretraining.py:119]   name = segment_ids, shape = (1, 512)\n",
            "I0629 15:26:13.235622 139621729412992 run_pretraining.py:167] **** Trainable Variables ****\n",
            "I0629 15:26:13.235882 139621729412992 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768)\n",
            "I0629 15:26:13.236008 139621729412992 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0629 15:26:13.236102 139621729412992 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0629 15:26:13.236192 139621729412992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.236278 139621729412992 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.236360 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.236464 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.236546 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.236627 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.236711 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.236791 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.236867 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.236946 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.237021 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.237096 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.237170 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.237250 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.237326 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.237419 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.237496 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.237570 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.237647 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.237728 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.237804 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.237883 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.237957 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.238035 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.238109 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.238187 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.238262 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.238336 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.238423 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.238502 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.238577 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.238655 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.238736 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.238812 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.238885 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.238964 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.239038 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.239117 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.239191 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.239270 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.239346 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.239438 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.239513 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.239588 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.239660 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.239747 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.239823 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.239902 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.239978 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.240052 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.240126 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.240203 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.240278 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.240356 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.240445 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.240524 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.240599 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.240678 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.240759 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.240834 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.240908 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.240986 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.241061 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.241140 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.241215 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.241289 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.241364 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.241456 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.241532 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.241612 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.241688 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.241775 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.241851 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.241931 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.242007 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.242080 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.242155 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.242232 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.242308 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.242400 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.242478 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.242554 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.242628 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.242712 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.242788 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.242868 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.242942 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.243020 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.243095 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.243173 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.243247 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.243321 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.243408 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.243489 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.243564 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.243642 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.243722 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.243797 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.243869 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.243947 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.244020 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.244098 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.244172 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.244249 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.244323 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.244413 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.244488 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.244561 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.244635 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.244718 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.244794 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.244873 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.244947 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.245020 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.245093 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.245171 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.245246 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.245324 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.245411 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.245509 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.245584 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.314746 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.315032 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.315150 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.315252 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.315360 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.315508 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.315624 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.315745 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.315851 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.315956 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.316069 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.316177 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.316287 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.316413 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.316533 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.316652 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.316772 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.316881 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.316987 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.317091 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.317204 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.317313 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.317446 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.317555 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.317661 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.317780 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.317892 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.317998 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.318111 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.318216 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.318327 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.318455 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.318570 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.318675 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.318795 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.318900 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.319009 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.319113 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.319222 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.319327 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.319450 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.319555 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.319666 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.319781 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.319900 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.320007 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.320116 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.320221 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.320330 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.320455 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.320560 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.320661 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.320782 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.320888 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.320998 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.321103 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.321208 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.321312 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.321439 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 15:26:13.321550 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.321659 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 15:26:13.321774 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.321887 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 15:26:13.321993 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.322100 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.322204 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.322308 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.322425 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 15:26:13.322535 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 15:26:13.322643 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 15:26:13.322763 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.322867 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.322970 139621729412992 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.323072 139621729412992 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.323181 139621729412992 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.323286 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0629 15:26:13.323412 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0629 15:26:13.323522 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 15:26:13.323627 139621729412992 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 15:26:13.323740 139621729412992 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0629 15:26:13.323847 139621729412992 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0629 15:26:13.323957 139621729412992 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0629 15:26:13.718744 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:198: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
            "\n",
            "W0629 15:26:13.735288 139621729412992 deprecation_wrapper.py:119] From bert/run_pretraining.py:202: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "I0629 15:26:14.790006 139621729412992 estimator.py:1147] Done calling model_fn.\n",
            "I0629 15:26:14.807806 139621729412992 evaluation.py:255] Starting evaluation at 2019-06-29T15:26:14Z\n",
            "I0629 15:26:14.808050 139621729412992 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 15:26:15.327312 139621729412992 monitored_session.py:240] Graph was finalized.\n",
            "W0629 15:26:15.328129 139621729412992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0629 15:26:15.489625 139621729412992 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/wordpiece/model/model.ckpt-10000\n",
            "I0629 15:26:38.726282 139621729412992 session_manager.py:500] Running local_init_op.\n",
            "I0629 15:26:38.924824 139621729412992 session_manager.py:502] Done running local_init_op.\n",
            "I0629 15:26:39.397541 139621729412992 tpu_estimator.py:557] Init TPU system\n",
            "I0629 15:26:47.804910 139621729412992 tpu_estimator.py:566] Initialized TPU in 8 seconds\n",
            "I0629 15:26:47.805719 139620775925504 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0629 15:26:47.806060 139620745778944 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0629 15:26:48.016711 139621729412992 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0629 15:26:48.184749 139621729412992 tpu_estimator.py:590] Enqueue next (100) batch(es) of data to infeed.\n",
            "I0629 15:26:48.185075 139621729412992 tpu_estimator.py:594] Dequeue next (100) batch(es) of data from outfeed.\n",
            "I0629 15:26:53.507681 139620745778944 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0629 15:26:55.895613 139621729412992 evaluation.py:167] Evaluation [100/100]\n",
            "I0629 15:26:55.895987 139621729412992 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0629 15:26:55.896078 139621729412992 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0629 15:26:55.896346 139620775925504 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0629 15:26:55.896678 139620775925504 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0629 15:26:55.896860 139621729412992 error_handling.py:96] infeed marked as finished\n",
            "I0629 15:26:55.897046 139621729412992 tpu_estimator.py:602] Stop output thread controller\n",
            "I0629 15:26:55.897107 139621729412992 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0629 15:26:56.829536 139620745778944 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0629 15:26:56.829809 139620745778944 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0629 15:26:56.830069 139621729412992 error_handling.py:96] outfeed marked as finished\n",
            "I0629 15:26:56.830365 139621729412992 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0629 15:26:57.449885 139621729412992 evaluation.py:275] Finished evaluation at 2019-06-29-15:26:57\n",
            "I0629 15:26:57.450251 139621729412992 estimator.py:2039] Saving dict for global step 10000: global_step = 10000, loss = 0.366147, masked_lm_accuracy = 0.91963214, masked_lm_loss = 0.45129755, next_sentence_accuracy = 0.99875, next_sentence_loss = 0.002986016\n",
            "I0629 15:27:02.516640 139621729412992 estimator.py:2099] Saving 'checkpoint_path' summary for global step 10000: gs://gurebert/gureBERT/wordpiece/model/model.ckpt-10000\n",
            "I0629 15:27:03.299861 139621729412992 error_handling.py:96] evaluation_loop marked as finished\n",
            "I0629 15:27:03.300286 139621729412992 run_pretraining.py:483] ***** Eval results *****\n",
            "I0629 15:27:03.300433 139621729412992 run_pretraining.py:485]   global_step = 10000\n",
            "I0629 15:27:03.300760 139621729412992 run_pretraining.py:485]   loss = 0.366147\n",
            "I0629 15:27:03.300865 139621729412992 run_pretraining.py:485]   masked_lm_accuracy = 0.91963214\n",
            "I0629 15:27:03.300944 139621729412992 run_pretraining.py:485]   masked_lm_loss = 0.45129755\n",
            "I0629 15:27:03.301016 139621729412992 run_pretraining.py:485]   next_sentence_accuracy = 0.99875\n",
            "I0629 15:27:03.301085 139621729412992 run_pretraining.py:485]   next_sentence_loss = 0.002986016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNV42_z-O92_",
        "colab_type": "text"
      },
      "source": [
        "### EN-EU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "506576e5-5382-4ec8-f5fc-6c8b8abc4a2e",
        "id": "k-kNc1NmPg55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format('./spModels/en-eu'))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 44999\n",
            "Sample tokens: ['▁styling', 'lainn', '▁mayan', 'ned', 'spots', 'spectre', 'leninis', '▁diaspora', 'commenced', '▁subsidise']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IMHIP2dDPg7M",
        "colab": {}
      },
      "source": [
        "# hau errepasatu. adibidez @ ez dut ondo egiten... [UNK] agertzen da...\n",
        "# adibidez, \n",
        "\n",
        "#import string\n",
        "#def parse_sentencepiece_token(token):\n",
        "#    if token.startswith(\"▁\"):\n",
        "#        return token[1:]\n",
        "#    else:\n",
        "#        if token in string.punctuation:\n",
        "#            return token\n",
        "#        else:\n",
        "#            return \"##\" + token\n",
        "          \n",
        "\n",
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "10fb40df-fdd1-4780-8abc-8d313ef544fb",
        "id": "TYrllb6wPg7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import string\n",
        "\n",
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "# puntuazio sinboloak gehitu \n",
        "ctrl_symbols_end = list(string.punctuation)\n",
        "#ctrl_symbols = [\"[UNK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab + ctrl_symbols_end\n",
        "\n",
        "#bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(32000 - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RQVCJ3izPg72",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab-en_eu.txt\"\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2a9f7004-a6f9-4399-b439-b7b828cdb278",
        "id": "Lm-A4Er3Pg8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "from bert import tokenization\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(\"Nere kotxea aitonaren etxe alboan dago!, eta bere kolorea gorria da. ni@ni.eus erabili \")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ne',\n",
              " '##re',\n",
              " 'ko',\n",
              " '##txea',\n",
              " 'ai',\n",
              " '##ton',\n",
              " '##aren',\n",
              " 'etxe',\n",
              " 'alb',\n",
              " '##oa',\n",
              " '##n',\n",
              " 'dago',\n",
              " '!',\n",
              " ',',\n",
              " 'eta',\n",
              " 'bere',\n",
              " 'kolore',\n",
              " '##a',\n",
              " 'gorria',\n",
              " 'da',\n",
              " '.',\n",
              " 'ni',\n",
              " '@',\n",
              " 'ni',\n",
              " '.',\n",
              " 'eu',\n",
              " '##s',\n",
              " 'erabili']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2989a392-f2cd-49e1-b2d4-c3d4ee5bfc80",
        "id": "q0P25Z_LPg8W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "FILES = \"./corpus/en-eu/2014wiki.eu.sent_splited,./corpus/en-eu/2019wiki-10k.en.sent_splited\"\n",
        "\n",
        "!python bert/create_pretraining_data.py \\\n",
        "  --input_file={FILES} \\\n",
        "  --output_file=$GS/wordpiece/pretraining-en_eu.tf.data \\\n",
        "  --vocab_file=vocab-en_eu.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=666 \\\n",
        "  --do_whole_word_mask=True \\\n",
        "  #--dupe_factor=5"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 17:36:15.284230 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 17:36:15.285029 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 17:36:15.285196 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 17:36:15.285347 140555365861248 deprecation_wrapper.py:119] From /content/gureBERT/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 17:36:15.468693 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 17:36:15.470420 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 17:36:15.470667 140555365861248 create_pretraining_data.py:446] *** Reading from input files ***\n",
            "I0629 17:36:15.470755 140555365861248 create_pretraining_data.py:448]   ./corpus/en-eu/2014wiki.eu.sent_splited\n",
            "I0629 17:36:15.471500 140555365861248 create_pretraining_data.py:448]   ./corpus/en-eu/2019wiki-10k.en.sent_splited\n",
            "I0629 17:37:37.557271 140555365861248 create_pretraining_data.py:457] *** Writing to output files ***\n",
            "I0629 17:37:37.557595 140555365861248 create_pretraining_data.py:459]   gs://gurebert/gureBERT/wordpiece/pretraining-en_eu.tf.data\n",
            "W0629 17:37:37.557804 140555365861248 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0629 17:37:37.559309 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.559506 140555365861248 create_pretraining_data.py:151] tokens: [CLS] [MASK] geroztik , data ##k zehatzago ##ak [MASK] : [SEP] ! dead - [MASK] pages [SEP]\n",
            "I0629 17:37:37.559770 140555365861248 create_pretraining_data.py:161] input_ids: 2 4 8010 45015 752 72 29987 106 4 45019 3 45004 1323 45016 4 4913 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.559995 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.560211 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.560298 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 1 8 14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.560381 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 35309 241 248 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.560500 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.560594 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.561495 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.561639 140555365861248 create_pretraining_data.py:151] tokens: [CLS] post ##comment [SEP] irudiak igo [SEP]\n",
            "I0629 17:37:37.561862 140555365861248 create_pretraining_data.py:161] input_ids: 2 457 14140 3 12947 7423 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.562078 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.562287 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.562369 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.562457 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 7423 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.562544 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.562637 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.563494 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.563615 140555365861248 create_pretraining_data.py:151] tokens: [CLS] mainpage ##text [SEP] imgde ##lete [SEP]\n",
            "I0629 17:37:37.563832 140555365861248 create_pretraining_data.py:161] input_ids: 2 29974 1612 3 34403 34342 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.564043 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.564251 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.564334 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.564419 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.564503 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.564591 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.565460 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.565613 140555365861248 create_pretraining_data.py:151] tokens: [CLS] the eusko ##bar ##ome ##tro , the survey carried out by the universi ##dad del [MASK] ##s vasco ( university [MASK] the [MASK] country ) [MASK] aski ##ng about the [MASK] of eta within the basque population , obtained these results in may 2009 [MASK] 64 [MASK] rejected eta totally , [MASK] % identified themselves as former eta sympathise ##rs who no longer support the group . another [MASK] % agreed with [MASK] ' s ends [MASK] but not thei ##r means [MASK] [SEP] upload [SEP]\n",
            "I0629 17:37:37.565837 140555365861248 create_pretraining_data.py:161] input_ids: 2 11 8154 3174 20240 13863 45015 11 3905 1229 148 31 11 42869 23148 2315 4 19 13976 45011 365 4 11 4 187 45012 4 24259 389 113 11 4 14 40 237 11 1319 214 45015 1972 91 832 16 87 760 4 6193 4 2424 40 7227 45015 4 45008 1588 749 24 567 40 22527 2328 74 111 974 292 11 173 45017 242 4 45008 1773 33 4 45010 294 4601 4 62 58 10026 179 639 4 3 10179 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.566059 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.566271 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.647868 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 16 17 21 23 26 31 45 47 52 69 73 77 83 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.648111 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 34017 19 14 1319 45015 2303 45019 45008 874 524 40 45015 45017 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.648246 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.648340 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.649856 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.650359 140555365861248 create_pretraining_data.py:151] tokens: [CLS] although the grammars of the spoken varieties share many [MASK] , they do possess differences . the entire chinese character corpus since antiquity comprise ##s [MASK] over 20 , 0 ##00 characters , [MASK] which only roughly 10 , 0 ##00 are now commonly in use . however chinese characters should not be confused with chinese words . because most chinese words are made up of two or more characters , there are many more chinese words than characters . a more accurate equivalent for a chinese character is the [MASK] , as characters represent the smallest grammatical units with individual meanings in the chinese language . estimates of the total number of chinese words and lexical ##ized phrases vary greatly [MASK] the \" han ##yu da zi ##dian \" , a compen ##dium of chinese [MASK] , includes 54 , 67 ##8 head entries for characters , including bone oracle versions . the \" zho ##nghu ##a zi ##hai \" ( 1994 ) contains [MASK] , [MASK] [MASK] head entries for character definitions , [MASK] is the largest reference work based pure ##ly on character and its literary variants . the c ##c - ced ##ic ##t project ( 2010 ) contains 97 , 40 ##4 contemporar ##y entries including idiom ##s , technology terms and names of political figures , businesses and products . the 2009 version of the [MASK] [MASK] ' s digital chinese dictionary ( w ##dc ##d ) , based on c ##c - ced ##ic ##t , contains over 84 , 0 ##00 entries . the most comprehensive pure linguistic chinese - language [MASK] , the 12 - volume \" han ##yu da ci ##dian \" , records more than 23 , 0 ##00 head chinese [MASK] and gives over 370 , 0 ##00 definitions . the 1999 revise ##d \" ci ##hai \" , a multi - volume encyclopedi ##c dictionary reference work , gives 122 , 83 ##6 vocabulary entry definitions under [MASK] , [MASK] [MASK] chinese characters , including proper names , phrases and common zoological , geographical , sociological , scientific and technical terms . the 7 ##th [MASK] 2016 ) edition of \" xian ##da ##i han ##yu ci ##dian \" , an authoritative one - volume dictionary on modern standard chinese language as used in mainland china , has 13 , 0 ##00 head characters and define ##s [MASK] , 0 ##00 words . like any other language , [MASK] has absorbed a siz ##able number of loanwords from other cultures . most chinese words are formed out of native chinese morphemes , including words describ ##ing imported objects and ideas . however , direct phonetic borrow ##ing of foreign words has gone on since ancient times . [SEP] urtarrila ##ren 18 [SEP]\n",
            "I0629 17:37:37.650736 140555365861248 create_pretraining_data.py:161] input_ids: 2 192 11 23180 14 11 1846 2319 1956 98 4 45015 76 264 10807 1829 45017 11 1117 257 862 5941 131 10245 3773 19 4 110 330 45015 3884 3683 724 45015 4 47 104 2013 524 45015 3884 3683 36 258 935 16 101 45017 115 257 724 433 58 42 4793 33 257 518 45017 186 78 257 518 36 153 130 14 97 43 81 724 45015 89 36 98 81 257 518 92 724 45017 18 81 3606 1911 27 18 257 862 21 11 4 45015 24 724 1240 11 4450 7060 1527 33 609 6647 16 11 257 190 45017 5774 14 11 367 158 14 257 518 15 12728 1144 3907 1813 2442 4 11 45005 1820 32776 112 5334 10786 45005 45015 18 42663 33414 14 257 4 45015 771 5939 45015 11957 1298 730 6240 27 724 45015 162 13332 7092 1525 45017 11 45005 43931 24769 46 5334 9138 45005 45011 1615 45012 1002 4 45015 4 4 730 6240 27 862 5739 45015 4 21 11 411 1602 150 261 2185 56 34 862 15 67 2052 4859 45017 11 206 165 45016 31590 282 121 840 45011 632 45012 1002 15139 45015 1662 1421 1674 86 6240 162 23766 19 45015 834 590 15 1161 14 293 2127 45015 3999 15 1207 45017 11 760 484 14 11 4 4 45010 294 2230 257 5039 45011 534 13474 48 45012 45015 261 34 206 165 45016 31590 282 121 45015 1002 110 18909 45015 3884 3683 6240 45017 11 78 4640 2185 4037 257 45016 190 4 45015 11 596 45016 1504 45005 1820 32776 112 9052 10786 45005 45015 1038 81 92 1546 45015 3884 3683 730 257 4 15 1871 110 14037 45015 3884 3683 5739 45017 11 1395 24437 48 45005 9052 9138 45005 45015 18 1408 45016 1504 26791 165 5039 1602 150 45015 1871 13898 45015 15927 1572 6988 2894 5739 128 4 45015 4 4 257 724 45015 162 1978 1161 45015 3907 15 251 21232 45015 4769 45015 22262 45015 1493 15 2482 590 45017 11 1001 108 4 1562 45012 2028 14 45005 24523 2311 129 1820 32776 9052 10786 45005 45015 41 14711 63 45016 1504 5039 34 243 482 257 190 24 90 16 1734 375 45015 57 874 45015 3884 3683 730 724 15 2291 19 4 45015 3884 3683 518 45017 256 154 69 190 45015 4 57 5208 18 12356 298 158 14 14587 37 69 2193 45017 78 257 518 36 663 148 14 1196 257 9567 45015 162 518 3417 28 4829 2700 15 2468 45017 115 45015 1030 6373 10256 28 14 616 518 57 5517 34 131 929 493 45017 3 1626 83 577 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.650989 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.651205 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.651293 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 10 26 34 91 122 137 166 168 169 176 232 233 270 293 331 333 334 359 401 412\n",
            "I0629 17:37:37.651376 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 6284 147 14 16743 45017 724 8479 4362 1298 15 1129 5836 5039 724 453 5801 1419 45011 3829 257\n",
            "I0629 17:37:37.651474 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.651565 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.652521 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.652853 140555365861248 create_pretraining_data.py:151] tokens: [CLS] schr ##odi ##nger published in \" anna ##len der physik \" the paper \" \" ( quanti ##zation as an eigenvalue problem ) on wave mechanics and presented what is now known as the schr ##odi ##nger equation . in this paper , he gave a \" derivation [MASK] of the wave equation for time - independent systems and showe ##d that it gave the correct energy eigenvalue ##s for a hydrogen - like atom . this paper has been universal ##ly celebrated as one of the most important achievements of the twentieth century and created a revolution in most areas of quantum mechanics and indeed of all physics and chemistry . a second paper was submitted just four weeks later that solved [MASK] quantum harmonic oscillator , rigid rotor , and dia ##tomic molecule problems and gave a new derivation of the schr ##odi ##nger equation . a third paper , published in may , showe ##d the equivalen ##ce of [MASK] [MASK] to that of heisenberg and gave the treatment of the stark effect . a [MASK] paper in this series showe ##d how to treat problems in which the colony changes with time , as in scatter ##ing problems . in this paper he introduced [MASK] complex solution to the wave equation in order to prevent the occurrence of fourth and sixth order differential equations . ( this was arguably the moment when quantum mechanics switche ##d from real to complex numbers . ) when [MASK] introduced complex numbers in order to lower the order of the differential equations , something magical happened , and all of wave mechanics was at his feet . ( he eventually reduced the order to one . ) these papers were his central achievement and [MASK] at once recognized as havi ##ng great significance [SEP] schr ##odi ##nger was not entirely comfortable with the implications of quantum theory . he wrote about the probability interpretation of quantum mechanics , saying : \" i don ' t like it , and i ' m sor ##ry i ever had [MASK] to do with it . \" ( just in order to ridicule the copenhagen interpretation of quantum mechanics [MASK] cont ##rived the famous thought - experiment called schr ##odi ##nger ' s cat paradox . [MASK] following his work on quantum mechanics , schr ##odi ##nger devote ##d considerable effort to working on a unified field theory that would unite gravity , electromagneti ##sm , and nuclear forces within the basic framework of [MASK] relativ ##ity skip doing the work with an extended correspondence with albert einstein . in 1947 , he announced a result , \" af ##fin ##e field theory , \" [MASK] a talk at the royal irish academy , but the announcement was criticize ##d by einstein as \" pre ##lim ##inary ##methyl and [MASK] to lead to the desired unified theory [MASK] following the failure of his attempt at unification , schr ##odi ##nger gave up his work on unification and turn ##ed to other topics . [SEP]\n",
            "I0629 17:37:37.653126 140555365861248 create_pretraining_data.py:161] input_ids: 2 36468 35027 25036 356 16 45005 5567 13874 2653 22962 45005 11 1792 45005 45005 45011 42344 16194 24 41 27798 948 45012 34 1797 2953 15 1798 305 21 258 127 24 11 36468 35027 25036 1102 45017 16 51 1792 45015 54 770 18 45005 16755 4 14 11 1797 1102 27 102 45016 933 420 15 24772 48 29 39 770 11 3283 638 27798 19 27 18 3224 45016 256 2734 45017 51 1792 57 65 1595 56 3988 24 63 14 11 78 325 8271 14 11 4124 159 15 544 18 2021 16 78 384 14 2137 2953 15 3049 14 84 1771 15 3547 45017 18 215 1792 26 8075 599 271 2210 139 29 9594 4 2137 6914 2801 45015 8086 11203 45015 15 24047 38882 6562 1067 15 770 18 88 16755 14 11 36468 35027 25036 1102 45017 18 459 1792 45015 356 16 87 45015 24772 48 11 20605 1284 14 4 4 17 29 14 14628 15 770 11 709 14 11 22947 634 45017 18 4 1792 16 51 554 24772 48 507 17 1509 1067 16 47 11 1837 614 33 102 45015 24 16 26793 28 1067 45017 16 51 1792 54 600 4 654 2503 17 11 1797 1102 16 320 17 825 11 10271 14 1589 15 4167 320 8549 4789 45017 45011 51 26 8118 11 5019 94 2137 2953 25436 48 37 682 17 654 466 45017 45012 94 4 600 654 466 16 320 17 711 11 320 14 11 8549 4789 45015 1557 15745 4811 45015 15 84 14 1797 2953 26 50 44 3984 45017 45011 54 728 1248 11 320 17 63 45017 45012 91 5603 53 44 521 7372 15 4 50 621 1614 24 23762 389 228 4776 3 36468 35027 25036 26 58 1907 9431 33 11 8867 14 2137 642 45017 54 364 113 11 2705 2932 14 2137 2953 45015 1888 45019 45005 135 2602 45010 223 256 39 45015 15 135 45010 319 8945 1303 135 958 61 4 17 264 33 39 45017 45005 45011 599 16 320 17 10896 11 6976 2932 14 2137 2953 4 16568 35721 11 813 824 45016 3197 156 36468 35027 25036 45010 294 6475 6917 45017 4 230 44 150 34 2137 2953 45015 36468 35027 25036 6037 48 2824 1711 17 1187 34 18 3746 541 642 29 107 18677 10529 45015 32723 9069 45015 15 1368 462 237 11 1387 2886 14 4 12108 1185 10140 3470 11 150 33 41 1433 5453 33 1451 7056 45017 16 2014 45015 54 1094 18 295 45015 45005 9888 44587 66 541 642 45015 45005 4 18 3331 50 11 783 2474 3042 45015 62 11 4452 26 4898 48 31 7056 24 45005 759 32506 35948 28361 15 4 17 438 17 11 6013 3746 642 4 230 11 2392 14 44 1008 50 5681 45015 36468 35027 25036 770 130 44 150 34 5681 15 1032 35 17 69 4867 45017 3\n",
            "I0629 17:37:37.653346 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.653572 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.653660 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 49 124 163 164 179 193 209 249 295 348 367 384 415 422 425 434 453 475 477 485\n",
            "I0629 17:37:37.653741 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45005 11 44 1280 1589 119 18 54 53 4113 54 45012 1368 297 45015 1451 16 45005 1610 45017\n",
            "I0629 17:37:37.653825 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.653895 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 17:37:37.654838 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.655040 140555365861248 create_pretraining_data.py:151] tokens: [CLS] by 1956 , the zappa family had moved to lancaster , a small aerospace and farming town in [MASK] ante ##lope valley of the mo ##ja ##ve desert close to edwards [MASK] force base ; he would later refer [MASK] sun village ( a town close to lancaster ) in the 1973 [MASK] \" village [MASK] the sun \" . zappa ' s mother encouraged him in his [MASK] interests . although she dislike ##d var ##ese ' s music , she was indulge ##nt enough to give her son a ##burnt distance call to the composer as a [MASK] [MASK] birthday present . unfortunately , var ##ese was in europe at [MASK] time , so zappa [MASK] to the composer ' s wife and she suggested he call back later . in a letter var ##ese thank ##ed him for his interest , [MASK] told him about a composition he was working on called \" deserts \" . living in the desert town of lancaster , zappa found this very [MASK] . [MASK] [MASK] [MASK] him to [MASK] if he ever came to new york . the meeting never took place ( var ##ese died in 1965 ) , but [MASK] frame ##d the letter and kep ##t it on display for the rest of his life . [SEP] title ##matches [SEP]\n",
            "I0629 17:37:37.751277 140555365861248 create_pretraining_data.py:161] input_ids: 2 31 1975 45015 11 296 416 61 925 17 14441 45015 18 279 9357 15 2383 652 16 4 25478 34531 2933 14 11 1760 5172 436 3291 471 17 19331 4 333 960 45020 54 107 139 649 4 1692 2119 45011 18 652 471 17 14441 45012 16 11 2509 4 45005 2119 4 11 1692 45005 45017 296 45010 294 1080 2902 164 16 44 4 2759 45017 192 232 9456 48 6488 5015 45010 294 167 45015 232 26 21291 3168 1641 17 1134 143 542 18 26498 2076 891 17 11 1861 24 18 4 4 8764 528 45017 14780 45015 6488 5015 26 16 414 50 4 102 45015 141 296 4 17 11 1861 45010 294 1329 15 232 1273 54 891 346 139 45017 16 18 396 6488 5015 37350 35 164 27 44 1252 45015 4 1444 164 113 18 2117 54 26 1187 34 156 45005 24033 45005 45017 761 16 11 3291 652 14 14441 45015 296 188 51 268 4 45017 4 4 4 164 17 4 155 54 958 461 17 88 710 45017 11 2079 597 448 327 45011 6488 5015 936 16 3003 45012 45015 62 4 7388 48 11 396 15 19517 121 39 34 3422 27 11 1488 14 44 273 45017 3 918 14152 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.751838 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.752216 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.752371 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 19 32 40 44 53 56 69 92 100 101 113 118 145 172 174 175 176 179 190 202\n",
            "I0629 17:37:37.752507 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 11 234 17 18 1821 14 1098 259 622 108 11 4046 15 11391 6488 5015 3632 1973 597 296\n",
            "I0629 17:37:37.752660 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.752769 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.754054 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.754399 140555365861248 create_pretraining_data.py:151] tokens: [CLS] mono ##na terrace , originally designed in 1937 as municipal offices for madison , wisconsin , was completed in 1997 on the original site , us ##ing [MASK] variation of wright ' s final design for the exterior , with the interior design altered by its new purpose as a convention center . the \" as - built \" design was carried out [MASK] wright ' s apprentice ton ##y puttnam . mono ##na terrace was accompanie ##d by controversy throughout the 60 years between [MASK] original design and the completi ##on of the [MASK] . florida southern college , located in lake ##land , florida , constructed 12 ( out of 18 planned ) frank lloyd [MASK] [MASK] between 1941 and 1958 as part of the child of the sun project . it is the world ' s largest single - site collection of frank lloyd wright architecture . [SEP] made innovati ##ve use of new building materials such as pre ##cast concrete blocks , glass bricks , and zinc came ##s ( instead of the traditional lead ) for his lead ##light windows , and he [MASK] used pyr ##ex glass tubing as a major element in the johnson wax headquarters . wright was also one of the first architects to design and install custom - made electric light fittings , including some of the first electric floor lamps , and his very early use of the then - novel spher ##ical glass lamps ##had ##e ( a design previously not possible due to the physical restrictions of gas lighting ) . in 1897 , wright received a patent for \" pri ##sm glass tiles \" that were used in [MASK] [MASK] ##s to direct light toward the interior . wright full ##y embrace ##d glass hamad his designs and found that it fit well into his philosophy of organic architecture . according to wright ' s organic theory , all components of the building should appear unified , as though they belong together . nothing should be attache ##d [MASK] it without consider ##ing the effect on the whole . to unify the house to its site , wright often used large expanse ##s of glass to blur the boundary between the indoor ##s and outdoors . glass allowed for interaction and viewing of [MASK] outdoors while still protecti ##ng from the elements . in 1928 , wright wrote an essay on glass in which [MASK] compared it to the mirror ##s of nature : lakes , rivers and pon ##ds . one of wright ' s earliest uses of glass in his works was to string pan ##es of glass along whole administration in an attempt to create light screens [MASK] join together solid walls . by us ##ing this large amount of glass , wright sought to achieve a balance between the lightness and air ##iness of the glass and the solid , hard walls . arguably , wright ' [MASK] best - known art glass is that of the prairie style . the simple [SEP]\n",
            "I0629 17:37:37.754716 140555365861248 create_pretraining_data.py:161] input_ids: 2 9208 1994 11181 45015 909 949 16 4224 24 2282 4840 27 8213 45015 5180 45015 26 1353 16 1834 34 11 382 769 45015 280 28 4 4568 14 347 45010 294 546 511 27 11 27959 45015 33 11 2266 511 5727 31 67 88 1570 24 18 2196 718 45017 11 45005 24 45016 562 45005 511 26 1229 148 4 347 45010 294 16641 33522 86 7453 45017 9208 1994 11181 26 5541 48 31 3838 705 11 2830 124 105 4 382 511 15 11 20466 191 14 11 4 45017 3433 487 871 45015 750 16 2358 1162 45015 3433 45015 1904 596 45011 148 14 577 2138 45012 1236 2708 4 4 105 3225 15 2622 24 144 14 11 1814 14 11 1692 840 45017 39 21 11 114 45010 294 411 409 45016 769 2245 14 1236 2708 347 1985 45017 3 153 6458 436 101 14 88 990 1983 70 24 759 15704 6597 4907 45015 3334 22916 45015 15 1707 461 19 45011 598 14 11 422 438 45012 27 44 438 5797 4666 45015 15 54 4 90 36533 6157 3334 21702 24 18 219 2337 16 11 3980 10581 3226 45017 347 26 60 63 14 11 73 5549 17 511 15 4484 5467 45016 153 1728 1143 13233 45015 162 75 14 11 73 1728 6035 17997 45015 15 44 268 152 101 14 11 211 45016 1946 13642 1019 3334 17997 7346 66 45011 18 511 1537 58 408 255 17 11 1106 3544 14 605 13635 45012 45017 16 5646 45015 347 498 18 5366 27 45005 18454 9069 3334 30176 45005 29 53 90 16 4 4 19 17 1030 1143 731 11 2266 45017 347 580 86 7915 48 3334 33082 44 3039 15 188 29 39 2720 147 96 44 1896 14 2645 1985 45017 299 17 347 45010 294 2645 642 45015 84 1921 14 11 990 433 991 3746 45015 24 323 76 1705 559 45017 2909 433 42 8500 48 4 39 390 1116 28 11 634 34 11 1022 45017 17 17754 11 368 17 67 769 45015 347 145 90 181 15568 19 14 3334 17 18623 11 5010 105 11 13665 19 15 32010 45017 3334 701 27 5482 15 9096 14 4 32010 120 307 35711 389 37 11 911 45017 16 4069 45015 347 364 41 4085 34 3334 16 47 4 1289 39 17 11 5005 19 14 796 45019 4195 45015 2746 15 32296 11607 45017 63 14 347 45010 294 1405 1404 14 3334 16 44 500 26 17 1722 4655 197 14 3334 354 1022 1732 16 41 1008 17 1157 1143 31132 4 1864 559 2793 3267 45017 31 280 28 51 181 1070 14 3334 45015 347 3223 17 2186 18 2951 105 11 29672 15 234 10635 14 11 3334 15 11 2793 45015 1457 3267 45017 8118 45015 347 45010 4 588 45016 127 491 3334 21 29 14 11 5985 483 45017 11 982 3\n",
            "I0629 17:37:37.754959 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.755170 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.755254 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 28 64 65 86 95 118 119 189 281 283 284 285 299 343 388 409 417 447 455 496\n",
            "I0629 17:37:37.755334 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 18 31 347 11 905 347 1468 6051 90 3411 14183 19 16 17 11 54 796 3267 17 294\n",
            "I0629 17:37:37.755427 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.755497 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 17:37:37.756480 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.756698 140555365861248 create_pretraining_data.py:151] tokens: [CLS] [MASK] translations of the epistle to titus : [SEP] * 1870 - miguel primo de rivera militar eta diktadore espainiarra . ( h . 1930 ) . * 1933 - [MASK] mars ##e idazle kataluniarra [MASK] * 1934 - jacques an ##quet ##il txirrindulari frantziarra , [MASK] frantziako tour [MASK] zuen lehenengo ##a . ( [MASK] . 1987 ) . * 1935 - elvis presley rock abeslari estatubatuarra . ( h . 1977 ) . [MASK] 1941 - gra ##ham chapman umorista [MASK] idazle britainiarra . ecosystems [MASK] . 1989 ) . * 1942 - stephen hawking fisikari britainiarra . * 1942 - junichi ##ro koizumi , japoniak ##o lehen ministroa . * 1947 - david bowie [MASK] [MASK] . heriotzak euskal herria * 2001 - marian ##o ize ##ta idazle nafarra [MASK] ( j . 1915 ) . * 2007 - patx ##i be ##itia aurre ##sk ##ular ##i gipuzkoarra wrest ( j . 1925 ) . mundua * 132 ##4 - marco polo esploratzaile italiarra . [MASK] j . 125 ##4 ) . * 1642 - galileo galilei , italiar filosofo , fisikari eta merchandis . ( j . 1564 [MASK] . jaiak eta urteurrena ##k * se ##haska egutegi ##ko izend ##egia : diag ##ur eta [MASK] [MASK] . jaieguna , ondo ##ko herrian : * bizkaian : muskiz . [SEP]\n",
            "I0629 17:37:37.756947 140555365861248 create_pretraining_data.py:161] input_ids: 2 4 4915 14 11 972 17 6725 45019 3 45013 4686 45016 2415 22493 117 25041 6033 40 24299 6653 45017 45011 504 45017 1592 45012 45017 45013 4889 45016 4 7199 66 818 8061 4 45013 4766 45016 4454 41 19593 2387 9132 4964 45015 4 2317 2684 4 134 2519 46 45017 45011 4 45017 2425 45012 45017 45013 5598 45016 10627 8680 489 3477 1158 45017 45011 504 45017 2818 45012 45017 4 3225 45016 12030 5749 8655 14588 4 818 6642 45017 15372 4 45017 2629 45012 45017 45013 2806 45016 6548 12998 5650 6642 45017 45013 2806 45016 21054 2502 27249 45015 10901 99 515 13823 45017 45013 2014 45016 775 28618 4 4 45017 503 202 385 45013 1055 45016 16090 99 34768 753 818 7175 4 45011 595 45017 3592 45012 45017 45013 732 45016 18859 129 42 18557 5879 6126 10010 129 4827 22738 45011 595 45017 4598 45012 45017 401 45013 13770 1421 45016 8138 11966 8357 4044 45017 4 595 45017 9177 1421 45012 45017 45013 13586 45016 5069 8114 45015 3501 2491 45015 5650 40 39429 45017 45011 595 45017 6815 4 45017 1646 40 1643 72 45013 581 1639 887 68 1689 1616 45019 32228 1738 40 4 4 45017 8931 45015 5552 68 4205 45019 45013 10426 45019 28093 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.757171 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.757382 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.757479 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 1 31 36 47 50 56 76 83 87 88 118 119 133 152 167 169 187 193 210 211\n",
            "I0629 17:37:37.757580 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 2061 2242 45017 474 2269 504 45013 40 45011 504 3477 6642 45017 45017 4044 45011 11629 45012 5819 59\n",
            "I0629 17:37:37.757680 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.757753 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.758699 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.758918 140555365861248 create_pretraining_data.py:151] tokens: [CLS] the largest airport is the gran canaria airport . tenerife has two airports , tenerife north airport and tenerife south airport . the island [MASK] tenerife gather ##s the highest passenger movement of all the canary islands through its two airports . [MASK] two main islands ( [MASK] and gran canaria ) receive the greatest number of rö . tenerife 6 , 20 ##4 , 49 ##9 passengers and gran canaria 5 , 0 ##11 , 176 passengers . [SEP] the port of las palmas is first in freight traffic in the islands , [MASK] the port of santa cruz de tenerife is the [MASK] fishing port with approximate ##ly 7 ##emp 500 tons of fish caught , according to the spanish government publication statistical year ##book of state ports . similarly , it is [MASK] second port in spain as regards ship traffic , only [MASK] ##checked by the port of alge ##cir ##as bay . the port ' s facilities include a border inspection post [MASK] [MASK] ) approved by the european union , which is responsible for inspect ##ing all types of imports from third countries or exports to countries outside the european economic area . the port of los [MASK] [MASK] ( tenerife ) has the greatest number of passengers [MASK] in the canary islands , followed by the port of santa cruz de tenerife . the port of las [MASK] is the [MASK] port in [MASK] [MASK] in passengers and first in number of vehicles transported . [SEP]\n",
            "I0629 17:37:37.854686 140555365861248 create_pretraining_data.py:161] input_ids: 2 11 411 1060 21 11 2074 2130 1060 45017 1650 57 97 2970 45015 1650 276 1060 15 1650 262 1060 45017 11 225 4 1650 5792 19 11 1083 2071 415 14 84 11 966 100 176 67 97 2970 45017 4 97 245 100 45011 4 15 2074 2130 45012 2102 11 2140 158 14 42417 45017 1650 520 45015 330 1421 45015 6442 1617 2930 15 2074 2130 474 45015 3884 6089 45015 23683 2930 45017 3 11 683 14 3412 4329 21 73 16 7474 1801 16 11 100 45015 4 11 683 14 1547 3232 117 1650 21 11 4 1752 683 33 1051 56 1001 44416 4330 6106 14 989 7064 45015 299 17 11 476 125 2446 2410 183 5290 14 172 3767 45017 2066 45015 39 21 4 215 683 16 853 24 11727 1193 1801 45015 104 4 9865 31 11 683 14 43069 43283 449 1397 45017 11 683 45010 294 2320 193 18 1420 11508 457 4 4 45012 3552 31 11 180 363 45015 47 21 1259 27 22910 28 84 954 14 3758 37 459 289 43 2732 17 289 801 11 180 495 238 45017 11 683 14 1556 4 4 45011 1650 45012 57 11 2140 158 14 2930 4 16 11 966 100 45015 778 31 11 683 14 1547 3232 117 1650 45017 11 683 14 3412 4 21 11 4 683 16 4 4 16 2930 15 73 16 158 14 3473 6799 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.855138 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.855450 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.855582 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 25 43 48 58 95 105 112 136 147 148 162 168 169 204 205 215 235 238 241 242\n",
            "I0629 17:37:37.856912 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 14 11 1650 2930 120 73 45015 11 32541 48 2320 45011 26854 7937 18867 947 4329 459 11 100\n",
            "I0629 17:37:37.857119 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.857234 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 17:37:37.858990 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.859275 140555365861248 create_pretraining_data.py:151] tokens: [CLS] atlanta is [MASK] [MASK] by a mayor and [MASK] atlanta city council . the [MASK] council consist ##s of 15 representatives — [MASK] from each of the city ' s 12 districts and three at - large positions . the [MASK] may veto a [MASK] passed by the council , but the council can overrid ##e [MASK] veto with a two - thirds majority . [MASK] mayor ##princip atlanta is kei ##sha lance bottoms [MASK] a democrat elected on a nonpartisan ballot whos ##e first term in office began on january 2 , 2018 ##60.9 [SEP] oharra ezikusi eta [MASK] [MASK] . [SEP]\n",
            "I0629 17:37:37.859703 140555365861248 create_pretraining_data.py:161] input_ids: 2 290 21 4 4 31 18 3107 15 4 290 122 426 45017 11 4 426 667 19 14 622 4888 850 4 37 194 14 11 122 45010 294 596 4080 15 184 50 45016 181 2498 45017 11 4 87 11560 18 4 1827 31 11 426 45015 62 11 426 79 35858 66 4 11560 33 18 97 45016 19856 745 45017 4 3107 36973 290 21 39985 24919 10665 33544 4 18 6180 1141 34 18 21368 12478 2741 66 73 246 16 1012 286 34 608 386 45015 3947 41538 3 11400 28092 40 4 4 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.860047 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.860368 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.860519 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 3 4 9 15 23 41 45 57 66 68 75 79 80 95 100 101 0 0 0 0\n",
            "I0629 17:37:37.860669 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 3008 35 11 122 63 3107 1413 11 11 14 45015 34 18 45017 7209 5954 0 0 0 0\n",
            "I0629 17:37:37.860787 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.860890 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.862283 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.862538 140555365861248 create_pretraining_data.py:151] tokens: [CLS] text ##matches [SEP] heriotzak euskal herria * 1833 - faust ##o el ##huy ##ar , euskal kimikaria , wolfram ##aren aurkitzailea ( j . [MASK] ) . ##descending * 1804 - joseph priest ##ley , kimikari ingelesa , oxi ##gen ##oaren aurkitzailea [MASK] 1916 - ruben dario , poeta nikaragua ##rra sausages j . [MASK] ) . [MASK] 1929 - maria [MASK] [MASK] austriako ##a , espainiako [MASK] [MASK] ( j . 1858 ) . * [MASK] - ju ##r ##gi vi . a , erresuma batuko erregea ( j . 1895 ) . * 1994 - joseph co ##tten , estatubatuar aktorea ( j . ##fame ) . * [MASK] - [MASK] moore , irlandar [MASK] . ( j . 1952 [MASK] . * 2012 - [MASK] tap ##ies , kataluniar margolari eta eskultorea [MASK] [MASK] . 1923 ) . jaiak eta urteurrena ##k * se ##haska egutegi ##ko izend ##egia : ob ##eko / ob ##ek ##a eta gaston . [SEP]\n",
            "I0629 17:37:37.862906 140555365861248 create_pretraining_data.py:161] input_ids: 2 1382 14152 3 503 202 385 45013 6638 45016 14829 99 1462 43349 1072 45015 202 18869 45015 21280 136 27900 45011 595 45017 4 45012 45017 39087 45013 11445 45016 1605 3051 2559 45015 9202 5035 45015 43495 8426 6031 27900 4 5105 45016 15707 22102 45015 4934 8639 4700 22360 595 45017 4 45012 45017 4 3461 45016 1544 4 4 13333 46 45015 2516 4 4 45011 595 45017 10854 45012 45017 45013 4 45016 7990 179 5333 1778 45017 18 45015 2293 7599 3940 45011 595 45017 5614 45012 45017 45013 1615 45016 1605 452 15928 45015 952 1150 45011 595 45017 24414 45012 45017 45013 4 45016 4 6204 45015 4702 4 45017 45011 595 45017 3701 4 45017 45013 702 45016 4 13475 784 45015 9313 4529 40 9789 4 4 45017 5395 45012 45017 1646 40 1643 72 45013 581 1639 887 68 1689 1616 45019 9684 537 45018 9684 302 46 40 5707 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.868937 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.869499 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.869647 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 22 25 28 43 52 55 58 62 63 68 69 77 107 111 113 117 123 128 136 137\n",
            "I0629 17:37:37.869757 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45011 25954 401 45013 45011 6813 45013 30635 46 3100 18566 3701 5024 689 7124 20951 45012 15485 45011 595\n",
            "I0629 17:37:37.869859 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.869937 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.872419 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.872767 140555365861248 create_pretraining_data.py:151] tokens: [CLS] kirkj ##a has a very mild climate , similar to akraberg . it [MASK] the lowest frequency of frosts out [MASK] all weather stations included in [MASK] records [MASK] the [MASK] meteorologica ##l institute , with 36 days of euskaltzale in an average [MASK] . snow ##fall is uncommon demon due to mild temperatures [MASK] relative ##ly low precipitation [MASK] the weather station [MASK] somewh ##at high at 53 meters above sea level , [MASK] could possibl ##y [MASK] [MASK] data , but not as much as the previous stations [MASK] [SEP] nb ##yt ##es [SEP]\n",
            "I0629 17:37:37.873178 140555365861248 create_pretraining_data.py:161] input_ids: 2 19326 46 57 18 268 4182 579 45015 352 17 27060 45017 39 4 11 3871 1618 14 15410 148 4 84 2763 2264 405 16 4 1038 4 11 4 15915 233 1163 45015 33 5606 747 14 15619 16 41 565 4 45017 3019 4170 21 7022 21890 255 17 4182 2447 4 816 56 488 3848 4 11 2763 1092 4 18611 907 157 50 5572 4843 552 244 545 45015 4 208 44401 86 4 4 752 45015 62 58 24 221 24 11 1266 2264 4 3 19415 33371 197 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.873453 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.873722 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.874896 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 14 21 27 29 31 40 44 50 55 60 64 75 79 80 91 0 0 0 0 0\n",
            "I0629 17:37:37.875089 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 57 14 11 31 1675 6937 183 45015 15 45017 21 47 1748 11 45017 0 0 0 0 0\n",
            "I0629 17:37:37.875206 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.875293 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.876377 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.876682 140555365861248 create_pretraining_data.py:151] tokens: [CLS] reactivated in 2016 , the national guard serve ##s as the official primary military and police reserve service of the armed forces . it also double ##s as a force multiplier for law enforcement personnel duri ##ng [MASK] [MASK] and [MASK] reinforce military personnel [MASK] associates being deployed within france and abroad . [SEP] edith and otto ' s children were : both buried in st . alban ' s abbey , mainz . initially buried in the st mauri ##ce monastery , edith ' s tomb since the 16 ##th century has been located in magd ##eburg cathedral . long regarded as a cenotaph , [MASK] lead coff ##in inside [MASK] stone sarc ##ophag ##us with her name on it was found and opened in 2008 by archaeologists duri ##ng work on the building . an inscription recorded that it was [MASK] body of eadgyth , reb ##uri ##ed in 1510 . the fragmented and incomplete bones were examined in 2009 , then brought to bristol , england , for tests in 2010 . the investigation ##s at bristol , apply ##ing isotope tests on tooth enamel , check ##ed [MASK] [MASK] she was born and [MASK] up in wessex and mercia , [MASK] written history indicated . testing on the bones revealed that they are the remains of eadgyth , from study made of the enamel of the teeth in her upper jaw . testing of the enamel [MASK] that [MASK] individual entom ##bed [MASK] magd ##eburg had spent time as a [MASK] in the chalk ##y upland ##s of wessex . the bones are the oldest found of a member of english royalty . following the tests the bones were re - inter ##red in a new titani ##um coff ##in in her tomb at magd ##eburg cathedral ##ligand 22 [MASK] 2010 . [SEP]\n",
            "I0629 17:37:37.876953 140555365861248 create_pretraining_data.py:161] input_ids: 2 14771 16 1562 45015 11 207 2253 1021 19 24 11 734 1267 213 15 1295 2557 240 14 11 1142 462 45017 39 60 2129 19 24 18 333 12872 27 201 4300 2592 940 389 4 4 15 4 4544 213 2592 4 8937 142 3235 237 204 15 4272 45017 3 8102 15 4818 45010 294 560 53 45019 138 3329 16 447 45017 19879 45010 294 8033 45015 31828 45017 1296 3329 16 11 447 7703 1284 9404 45015 8102 45010 294 4904 131 11 713 108 159 57 65 750 16 13099 12329 5617 45017 259 1590 24 18 27128 45015 4 438 16311 189 2562 4 3442 29329 25246 306 33 143 170 34 39 26 188 15 1959 16 549 31 11679 940 389 150 34 11 990 45017 41 12516 947 29 39 26 4 679 14 11302 45015 34475 13730 35 16 17968 45017 11 11634 15 6343 6724 53 5720 16 760 45015 211 791 17 10200 45015 423 45015 27 2231 16 632 45017 11 3056 19 50 10200 45015 2718 28 23550 2231 34 19006 17055 45015 6564 35 4 4 232 26 992 15 4 130 16 7472 15 8617 45015 4 312 348 4879 45017 1918 34 11 6724 2901 29 76 36 11 1029 14 11302 45015 37 434 153 14 11 17055 14 11 7083 16 143 2092 14208 45017 1918 14 11 17055 4 29 4 609 22312 7922 4 13099 12329 61 1872 102 24 18 4 16 11 22419 86 9026 19 14 7472 45017 11 6724 36 11 2771 188 14 18 450 14 335 10362 45017 230 11 2231 11 6724 53 274 45016 988 4724 16 18 88 35600 1380 16311 189 16 143 4904 50 13099 12329 5617 38700 1576 4 632 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.877188 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.877407 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.877496 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 38 39 41 45 46 62 107 112 143 192 193 198 205 223 241 243 247 255 302 304\n",
            "I0629 17:37:37.877599 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 12275 33757 17 94 4770 45019 18 18 11 13724 4916 791 24 37 2901 11 50 2404 34 676\n",
            "I0629 17:37:37.877690 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.877763 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.881098 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.881490 140555365861248 create_pretraining_data.py:151] tokens: [CLS] margaret cameron , duri ##ng the brothers ' visit to the cameron estate in south carolina [MASK] represent ##ing the old south . meanwhile , young ben cameron idoliz ##es a picture of elsie stoneman . when the civil war arrives , the young men of both families enlist in thei ##r respective armies [MASK] the younger stoneman and two of the cameron brothers are killed in combat . meanwhile , the cameron women are rescue ##d by confederat ##e [MASK] who rout a black militia , after an attack [MASK] the cameron home . ben cameron lead ##s a heroic charge at the siege of [MASK] , earning the nickname of \" the ##izen colonel \" , but he is [MASK] wounded and captured . he is then taken to a union hospital in washington , d . c . [MASK] [MASK] his stay at the hospital [MASK] he is told that he will be hanged . also at the hospital , he meet ##s elsie [MASK] , whos ##e picture he has been carrying ; she is working there as a nurse . elsie takes cameron ' s mother , who had travele ##d to washington to tend her son , to see abraham lincoln , and mrs . cameron persuade ##s the president to pardon ben . when lincoln is assassinate ##d at ford ' s theatre , his concilia ##tory post ##war policy expire ##s with him . in the wake of the president ' s death , austin stoneman and other radical republicans are determined to punish the south , employ ##ing harsh measures that griffith depict [MASK] as havi ##ng been typical of the western ##sick . stoneman [MASK] his prot ##ege silas lynch , [MASK] psychopath ##ic mulatto , head to south carolina to observe the implementation of reconstruction policies firsthand . duri ##ng the [SEP] meanwhile , inspired by observing white children pretend ##ing to be ghosts to scare black children , ben fight ##s back by form ##ing the ku klu ##x klan . as a result , elsie , out of loyalty to her father , breaks off her relationship with ben . later , flora cameron goes off alone into the woods to fet ##ch water and is followed by gus , a freed ##man and soldier who is now a captain . he [MASK] [MASK] flora and tells her that he desires to get married . frighten ##ed , she flee ##s into the forest , pursued by gus . trapped on a precipi ##ce , flora warn ##s gus [MASK] will jump if he comes any closer . when he does , she leap ##s to her death . havi ##ng run through the forest look ##ing for her , ben has seen her jump ; he holds her as she dies , then carrie ##s her body back to the cameron home . in response , the klan hunt ##s down gus , trie ##s him , finds him guilty , and lynch ##es him . [SEP]\n",
            "I0629 17:37:37.881785 140555365861248 create_pretraining_data.py:161] input_ids: 2 5006 3384 45015 940 389 11 3493 45010 1973 17 11 3384 4914 16 262 6680 4 1240 28 11 400 262 45017 3213 45015 1046 2802 3384 30533 197 18 3825 14 6609 4558 45017 94 11 510 133 9685 45015 11 1046 392 14 138 1496 8068 16 10026 179 5028 4434 4 11 2896 4558 15 97 14 11 3384 3493 36 1271 16 2372 45017 3213 45015 11 3384 547 36 3020 48 31 13628 66 4 74 35924 18 672 11348 45015 85 41 788 4 11 3384 331 45017 2802 3384 438 19 18 9534 1779 50 11 6662 14 4 45015 6953 11 5418 14 45005 11 20140 8031 45005 45015 62 54 21 4 7529 15 3639 45017 54 21 211 610 17 18 363 3452 16 3055 45015 210 45017 206 45017 4 4 44 1869 50 11 3452 4 54 21 1444 29 54 169 42 10499 45017 60 50 11 3452 45015 54 1598 19 6609 4 45015 2741 66 3825 54 57 65 4011 45020 232 21 1187 89 24 18 9479 45017 6609 1372 3384 45010 294 1080 45015 74 61 43785 48 17 3055 17 1281 143 542 45015 17 626 4931 4613 45015 15 12022 45017 3384 7338 19 11 432 17 9306 2802 45017 94 4613 21 13818 48 50 9022 45010 294 1987 45015 44 21035 5117 457 1668 847 30499 19 33 164 45017 16 11 5713 14 11 432 45010 294 344 45015 9495 4558 15 69 2807 13632 36 1917 17 4955 11 262 45015 2692 28 6632 2677 29 2501 5042 4 24 23762 389 65 1891 14 11 533 42279 45017 4558 4 44 36399 36137 14814 5204 45015 4 24105 282 20774 45015 730 17 262 6680 17 12561 11 3185 14 2539 2267 30502 45017 940 389 11 3 3213 45015 2141 31 9481 635 560 13317 28 17 42 18315 17 36308 672 560 45015 2802 3302 19 346 31 182 28 11 4096 44592 961 3707 45017 24 18 295 45015 6609 45015 148 14 8126 17 143 790 45015 9087 678 143 1475 33 2802 45017 139 45015 7799 3384 3346 678 2024 96 11 6178 17 31443 1822 332 15 21 778 31 8379 45015 18 7267 767 15 10116 74 21 258 18 2221 45017 54 4 4 7799 15 2782 143 29 54 23871 17 1025 1536 45017 29848 35 45015 232 8346 19 96 11 1979 45015 8201 31 8379 45017 10408 34 18 39781 1284 45015 7799 13760 19 8379 4 169 4950 155 54 1823 154 4028 45017 94 54 373 45015 232 13914 19 17 143 344 45017 23762 389 755 176 11 1979 1508 28 27 143 45015 2802 57 494 143 4950 45020 54 2409 143 24 232 19876 45015 211 6146 19 143 679 346 17 11 3384 331 45017 16 1172 45015 11 3707 5118 19 417 8379 45015 31990 19 164 45015 4946 164 7507 45015 15 5204 197 164 45017 3\n",
            "I0629 17:37:37.882016 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.882227 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 17:37:37.882317 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 17 55 81 91 107 115 122 142 143 149 168 272 273 281 282 285 292 396 397 433\n",
            "I0629 17:37:37.882406 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45015 45017 1974 34 6367 764 60 940 389 45015 4558 5042 19 2539 1036 15 18 6581 19 232\n",
            "I0629 17:37:37.882497 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.882585 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0629 17:37:37.883571 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.883696 140555365861248 create_pretraining_data.py:151] tokens: [CLS] unlock ##confirm [SEP] [MASK] [SEP]\n",
            "I0629 17:37:37.883924 140555365861248 create_pretraining_data.py:161] input_ids: 2 23501 20046 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.884141 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.884350 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.884439 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.884517 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 21872 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.884622 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.884697 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.885636 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.885776 140555365861248 create_pretraining_data.py:151] tokens: [CLS] commercial eater ##ies existed duri ##ng the roman period [MASK] with [MASK] of 150 \" thermo ##poli ##a \" , a form of fast food [MASK] , found in pompe ##ii referenc and urban [MASK] of tourist foods [MASK] have existed in china duri ##ng the song dynasty . [SEP] zientzia eta [MASK] [MASK] jaiotza ##k heriotzak - - - - agintari ##ak [SEP]\n",
            "I0629 17:37:37.886005 140555365861248 create_pretraining_data.py:161] input_ids: 2 835 25407 784 2996 940 389 11 525 236 4 33 4 14 3810 45005 18821 12049 46 45005 45015 18 182 14 2877 222 4 45015 188 16 22180 12307 15914 15 1222 4 14 3110 950 4 55 2996 16 375 940 389 11 376 630 45017 3 383 40 4 4 655 72 503 45016 45016 45016 45016 861 106 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.886217 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.886431 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.886516 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 10 12 14 26 32 35 37 39 53 54 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.981138 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45015 475 3810 3816 45015 4337 3345 87 505 509 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.981422 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.981549 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.983632 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.983776 140555365861248 create_pretraining_data.py:151] tokens: [CLS] [MASK] [SEP] unexpected [SEP]\n",
            "I0629 17:37:37.984028 140555365861248 create_pretraining_data.py:161] input_ids: 2 4 3 7473 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.984249 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.984471 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.984573 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.984657 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45013 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.984744 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0629 17:37:37.984815 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.985812 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.986067 140555365861248 create_pretraining_data.py:151] tokens: [CLS] copyrightpage ##name [SEP] duri ##ng jack ##ie mason ' [MASK] october 1964 performance on a show that had been shortened by ten minutes [MASK] to an address by president lynd ##on johnson , sullivan — on - stage but off - cho — signal ##ed mason that he had two minutes left by holding up two fingers . sullivan ' s signal distract ##ed the studio audience , and to television viewers unaware of the circumstances , it seemed as though mason ' s joke ##s were falling flat . mason , motor a bid to regain the audience ' s attention , cri ##ed , \" i ' m gett ##ing fingers here ! \" [MASK] [MASK] his own frantic hand gesture : \" here ' s a finger for [MASK] ! \" videotape ##s of the incident are inc ##on ##clu ##sive as to whet ##her mason ' s ups ##wept hand ( which was just off - camera ) was intended to be an ind ##ece ##nt gesture , but sullivan was convince ##d that it was , and banned mason from future appearances on the program . mason later insist ##ed that he did not know what the \" middle finger \" meant , and that he did not make the gesture anyway . in september 1965 , sullivan — who , according to mason , [MASK] \" deeply apologetic \" — brought mason on the show for a [MASK] surprise grand re ##union \" . \" he [MASK] they were old pal ##s , \" nachman wrote , \" news to mason [MASK] who never got a repeat invitation . \" [MASK] added that his earning power \" . . [MASK] was cut right in [MASK] after [MASK] . i never [MASK] worked my way back until i opened on broadway in 1986 . \" [SEP]\n",
            "I0629 17:37:37.986316 140555365861248 create_pretraining_data.py:161] input_ids: 2 29463 6479 3 940 389 2981 1374 5222 45010 4 676 2779 1031 34 18 343 29 61 65 6262 31 1277 2788 4 17 41 1939 31 432 24253 191 3980 45015 539 850 34 45016 1035 62 678 45016 36566 850 1059 35 5222 29 54 61 97 2788 308 31 3641 130 97 8349 45017 539 45010 294 1059 18568 35 11 2168 1785 45015 15 17 973 14001 8327 14 11 2650 45015 39 4837 24 323 5222 45010 294 24631 19 53 5070 3950 45017 5222 45015 2365 18 11112 17 6814 11 1785 45010 294 1731 45015 33724 35 45015 45005 135 45010 319 10163 28 8349 1219 45004 45005 4 4 44 269 18736 735 14248 45019 45005 1219 45010 294 18 3502 27 4 45004 45005 20872 19 14 11 3680 36 3764 191 43750 18591 24 17 13724 4916 5222 45010 294 33508 12163 735 45011 47 26 599 678 45016 3651 45012 26 1262 17 42 41 36210 18580 3168 14248 45015 62 539 26 7386 48 29 39 26 45015 15 4117 5222 37 1078 5344 34 11 429 45017 5222 139 9215 35 29 54 278 58 1630 305 11 45005 575 3502 45005 1841 45015 15 29 54 278 58 326 11 14248 10448 45017 16 625 3003 45015 539 850 74 45015 299 17 5222 45015 4 45005 5248 26937 45005 850 791 5222 34 11 343 27 18 4 8851 1744 274 7876 45005 45017 45005 54 4 76 53 400 7685 19 45015 45005 11340 364 45015 45005 2426 17 5222 4 74 597 3265 18 5406 10470 45017 45005 4 1216 29 44 6953 263 45005 45017 45017 4 26 2758 516 16 4 85 4 45017 135 597 4 1661 1316 371 346 254 135 1959 34 6673 16 2393 45017 45005 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.986541 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.986774 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.986878 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 9 10 24 42 93 100 117 118 128 132 192 231 244 253 268 277 286 291 293 297\n",
            "I0629 17:37:37.986969 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 45010 294 255 3651 16 45010 15 153 294 763 5222 26 45005 425 45015 5222 45017 573 29 3769\n",
            "I0629 17:37:37.987058 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:37.987132 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:37:37.988137 140555365861248 create_pretraining_data.py:149] *** Example ***\n",
            "I0629 17:37:37.988361 140555365861248 create_pretraining_data.py:151] tokens: [CLS] where formula _ 60 is the universal gas constant and [MASK] = pv / ( r ##t [MASK] is the compressi ##bility factor . in 1972 [MASK] . soave replaced the 1 / term of [MASK] redlich [MASK] kw ##ong equation with a function [UNK] ( [MASK] , settling ) involv ##ing the temperature and the ace ##nt ##ric factor ( the result ##ing equation is also known as the soave - redlich - kw ##ong equation of state ; s ##rk eos ) . the [UNK] function was devise ##d to fit the vapor [MASK] data of hydro ##carbons and the equation does fair ##ly well for these [MASK] . note especially that this [MASK] changes the definition of \" a \" slightly , as [MASK] formula [MASK] 61 is now to the second power . the s ##rk [MASK] may be written as where where formula [MASK] 54 and other parts of the s ##rk eos is defined in the s ##rk eos section . [MASK] downs ##ide of [MASK] s ##rk [MASK] , and other cubic eos , is that the liquid molar volume is significantly less [MASK] than the gas molar volume . penelo ##ux et ali ##os ( 1982 ) proposed a simple correction for this by introduc ##ing a volume translation [SEP] erabiltzaile kontua sortu da , baina ez da saioa hasi . ( e ) k cookie ##ak erabiltzen ditu [MASK] [MASK] eta ezgaituta dauzka ##zu . gaitu itza ##zu mesedez , eta ondoren saiat ##u saioa hasten zure erabiltzaile izen eta pasahitz berriak erabiliz . [SEP]\n",
            "I0629 17:37:37.988616 140555365861248 create_pretraining_data.py:161] input_ids: 2 118 231 45030 2830 21 11 1595 605 1381 15 4 45022 23055 45018 45011 725 121 4 21 11 34542 10963 1470 45017 16 3165 4 45017 12991 1123 11 277 45018 246 14 4 15167 4 15392 16116 1102 33 18 283 1 45011 4 45015 10321 45012 2874 28 11 647 15 11 34092 3168 4948 1470 45011 11 295 28 1102 21 60 127 24 11 12991 45016 15167 45016 15392 16116 1102 14 172 45020 294 11955 5746 45012 45017 11 1 283 26 15735 48 17 2720 11 5680 4 752 14 6044 16282 15 11 1102 373 395 56 147 27 91 4 45017 2485 540 29 51 4 614 11 995 14 45005 18 45005 2183 45015 24 4 231 4 6523 21 258 17 11 215 263 45017 11 294 11955 4 87 42 312 24 118 118 231 4 5939 15 69 738 14 11 294 11955 5746 21 706 16 11 294 11955 5746 1623 45017 4 13842 7182 14 4 294 11955 4 45015 15 69 5925 5746 45015 21 29 11 3115 6687 1504 21 2059 374 4 92 11 605 6687 1504 45017 24360 13350 1901 5892 2550 45011 1845 45012 1427 18 982 9710 27 51 31 4862 28 18 1504 1714 3 3556 12208 921 112 45015 852 196 112 4024 923 45017 45011 284 45012 440 22652 106 1870 1308 4 4 40 12623 14970 10741 45017 24596 44638 10741 5963 45015 40 1529 15691 229 4024 9015 3675 3556 5089 40 10996 3996 5448 45017 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.988836 140555365861248 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:37.989056 140555365861248 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 17:37:38.084238 140555365861248 create_pretraining_data.py:161] masked_lm_positions: 11 18 27 36 38 47 49 96 110 116 127 129 141 149 168 172 175 191 238 239\n",
            "I0629 17:37:38.084521 140555365861248 create_pretraining_data.py:161] masked_lm_ids: 2494 45012 564 11 45016 223 1 854 1983 4378 11 45030 5746 45030 18 11 5746 3606 8581 2429\n",
            "I0629 17:37:38.084726 140555365861248 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0629 17:37:38.085001 140555365861248 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0629 17:38:14.223631 140555365861248 create_pretraining_data.py:166] Wrote 45311 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6b9cc46d-53d6-4313-ad90-e9266d094abf",
        "id": "o1hyibNBPg8o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "!python src/run_pretraining.py \\\n",
        "  --config_file eu.congif.ini \\\n",
        "  --input_file=$GS/pretraining.tf.data \\\n",
        "  --output_dir=$GS/eu.gureBERT \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=1000000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "'''\n",
        "\n",
        "!python bert/run_pretraining.py \\\n",
        "  --input_file=$GS/wordpiece/pretraining-en_eu.tf.data \\ \\\n",
        "  --output_dir=$GS/wordpiece/model-en_eu \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=10000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  #--num_train_steps=1000000 \\\n",
        "  #--init_checkpoint=$GS/wordpiece/model \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 17:39:32.258262 139894899398528 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 17:39:32.259413 139894899398528 deprecation_wrapper.py:119] From bert/run_pretraining.py:493: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 17:39:32.260085 139894899398528 deprecation_wrapper.py:119] From bert/run_pretraining.py:407: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 17:39:32.260240 139894899398528 deprecation_wrapper.py:119] From bert/run_pretraining.py:407: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 17:39:32.260382 139894899398528 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 17:39:32.261122 139894899398528 deprecation_wrapper.py:119] From bert/run_pretraining.py:414: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0629 17:39:34.665977 139894899398528 deprecation_wrapper.py:119] From bert/run_pretraining.py:418: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0629 17:39:34.872255 139894899398528 deprecation_wrapper.py:119] From bert/run_pretraining.py:420: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 17:39:34.872542 139894899398528 run_pretraining.py:420] *** Input Files ***\n",
            "I0629 17:39:34.872672 139894899398528 run_pretraining.py:422]   gs://gurebert/gureBERT/wordpiece/pretraining-en_eu.tf.data\n",
            "W0629 17:39:35.910116 139894899398528 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 17:39:36.915790 139894899398528 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f3b9baca8c8>) includes params argument, but params are not passed to Estimator.\n",
            "I0629 17:39:36.917380 139894899398528 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/wordpiece/model-en_eu', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.2.81.2:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3ba7f31978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.2.81.2:8470', '_evaluation_master': 'grpc://10.2.81.2:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f3ba7f3d390>}\n",
            "I0629 17:39:36.917897 139894899398528 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0629 17:39:36.918648 139894899398528 run_pretraining.py:459] ***** Running training *****\n",
            "I0629 17:39:36.918747 139894899398528 run_pretraining.py:460]   Batch size = 64\n",
            "I0629 17:39:37.244780 139894899398528 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.2.81.2:8470) for TPU system metadata.\n",
            "2019-06-29 17:39:37.246164: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0629 17:39:37.260714 139894899398528 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0629 17:39:37.260965 139894899398528 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0629 17:39:37.261066 139894899398528 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0629 17:39:37.261145 139894899398528 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0629 17:39:37.261217 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5201560055092340784)\n",
            "I0629 17:39:37.261989 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15235922008977912019)\n",
            "I0629 17:39:37.262077 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16587921834020422802)\n",
            "I0629 17:39:37.262155 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 18249457006028921626)\n",
            "I0629 17:39:37.262228 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9769923856279949422)\n",
            "I0629 17:39:37.262314 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14707561106018069366)\n",
            "I0629 17:39:37.262382 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11546342222793515035)\n",
            "I0629 17:39:37.262445 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15225485262720068302)\n",
            "I0629 17:39:37.262507 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 18107303912184056565)\n",
            "I0629 17:39:37.262592 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12015888753985987776)\n",
            "I0629 17:39:37.262656 139894899398528 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6671482654539601268)\n",
            "W0629 17:39:37.269609 139894899398528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0629 17:39:37.290915 139894899398528 estimator.py:1145] Calling model_fn.\n",
            "W0629 17:39:37.291616 139894899398528 deprecation_wrapper.py:119] From bert/run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0629 17:39:37.298169 139894899398528 deprecation.py:323] From bert/run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0629 17:39:37.298433 139894899398528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0629 17:39:37.329426 139894899398528 deprecation.py:323] From bert/run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0629 17:39:37.329706 139894899398528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0629 17:39:37.331389 139894899398528 deprecation_wrapper.py:119] From bert/run_pretraining.py:393: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0629 17:39:37.344640 139894899398528 deprecation.py:323] From bert/run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0629 17:39:37.435457 139894899398528 run_pretraining.py:117] *** Features ***\n",
            "I0629 17:39:37.436039 139894899398528 run_pretraining.py:119]   name = input_ids, shape = (8, 512)\n",
            "I0629 17:39:37.436201 139894899398528 run_pretraining.py:119]   name = input_mask, shape = (8, 512)\n",
            "I0629 17:39:37.436307 139894899398528 run_pretraining.py:119]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0629 17:39:37.436422 139894899398528 run_pretraining.py:119]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0629 17:39:37.436526 139894899398528 run_pretraining.py:119]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0629 17:39:37.436637 139894899398528 run_pretraining.py:119]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0629 17:39:37.436723 139894899398528 run_pretraining.py:119]   name = segment_ids, shape = (8, 512)\n",
            "W0629 17:39:37.437085 139894899398528 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0629 17:39:37.440027 139894899398528 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0629 17:39:37.486718 139894899398528 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0629 17:39:37.666032 139894899398528 deprecation.py:506] From /content/gureBERT/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0629 17:39:37.693248 139894899398528 deprecation.py:323] From /content/gureBERT/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0629 17:39:41.781467 139894899398528 run_pretraining.py:167] **** Trainable Variables ****\n",
            "I0629 17:39:41.781732 139894899398528 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30000, 768)\n",
            "I0629 17:39:41.781862 139894899398528 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0629 17:39:41.781957 139894899398528 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0629 17:39:41.782046 139894899398528 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.782131 139894899398528 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.782213 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.782295 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.782371 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.782461 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.782538 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.782641 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.782722 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.782802 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.782876 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.782953 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.783028 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.783106 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.783180 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.783259 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.783335 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.783417 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.783492 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.783583 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.783662 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.783741 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.783817 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.783895 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.783970 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.784059 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.784135 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.784209 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.784282 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.784360 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.784442 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.784521 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.784610 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.784683 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.784757 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.784834 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.784911 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.784991 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.785065 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.785144 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.785218 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.785295 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.785370 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.785450 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.785523 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.785615 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.785691 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.785768 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.785842 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.785916 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.785991 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.786069 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.786144 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.786223 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.786297 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.786374 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.786453 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.786530 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.786618 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.786693 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.786766 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.786846 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.786922 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.787002 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.787079 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.787152 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.787225 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.787302 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.787377 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.787461 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.787535 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.787626 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.787701 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.787779 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.787853 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.787935 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.788012 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.788091 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.788168 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.788247 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.788322 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.788403 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.788479 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.788569 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.788645 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.788723 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.788799 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.788877 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.788954 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.789033 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.789108 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.789183 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.789257 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.789334 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.789415 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.789493 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.789580 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.789657 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.789731 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.789811 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.789885 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.789964 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.790038 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.790119 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.790195 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.790274 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.790351 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.790430 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.790504 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.790595 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.790671 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.790751 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.790827 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.790901 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.790976 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.791056 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.791130 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.791209 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.791283 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.795525 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.795769 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.795902 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.796009 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.796103 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.796197 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.796299 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.796431 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.796549 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.796678 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.796782 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.796886 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.796998 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.797105 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.797215 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.797324 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.797447 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.797572 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.797692 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.797803 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.797907 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.798011 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.798121 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.798229 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.798337 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.798453 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.798575 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.798672 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.798777 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.798878 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.798988 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.799093 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.799203 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.799306 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.799423 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.799534 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.799663 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.799767 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.799878 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.799986 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.800093 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.800199 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.800303 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.800417 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.800548 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.800677 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.800790 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.800896 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.801007 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.801114 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.801225 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.801328 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.801441 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.801546 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.801675 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.801780 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.801890 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.801997 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.802098 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.802202 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.802312 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0629 17:39:41.802423 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.802535 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0629 17:39:41.802662 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.802772 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0629 17:39:41.802876 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.802987 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.803093 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.803196 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.803299 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0629 17:39:41.803419 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0629 17:39:41.803527 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0629 17:39:41.803658 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.803767 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.803871 139894899398528 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.803974 139894899398528 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.804084 139894899398528 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.804189 139894899398528 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0629 17:39:41.804299 139894899398528 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0629 17:39:41.804412 139894899398528 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0629 17:39:41.804520 139894899398528 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0629 17:39:41.804646 139894899398528 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0629 17:39:41.804749 139894899398528 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0629 17:39:41.804859 139894899398528 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0629 17:39:41.805061 139894899398528 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0629 17:39:41.806880 139894899398528 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0629 17:39:41.814934 139894899398528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0629 17:39:42.161232 139894899398528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0629 17:39:56.503978 139894899398528 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0629 17:39:57.175241 139894899398528 estimator.py:1147] Done calling model_fn.\n",
            "I0629 17:40:00.752754 139894899398528 tpu_estimator.py:499] TPU job name worker\n",
            "I0629 17:40:02.171808 139894899398528 monitored_session.py:240] Graph was finalized.\n",
            "I0629 17:40:09.643931 139894899398528 session_manager.py:500] Running local_init_op.\n",
            "I0629 17:40:10.307748 139894899398528 session_manager.py:502] Done running local_init_op.\n",
            "I0629 17:40:21.564322 139894899398528 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://gurebert/gureBERT/wordpiece/model-en_eu/model.ckpt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrHSiJmsa_up",
        "colab_type": "code",
        "outputId": "392bff36-3475-418b-b04d-bf999c71d9d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# data English da!! baino kodea badabil \n",
        "\n",
        "!python bert/run_squad.py \\\n",
        "  --vocab_file=vocab-en_eu.txt \\\n",
        "  --bert_config_file=bert_config.json \\\n",
        "  --do_lower_case=True \\\n",
        "  --do_train=True \\\n",
        "  --train_file=train-v2.0.json \\\n",
        "  --do_predict=True \\\n",
        "  --predict_file=dev-v2.0.json \\\n",
        "  --train_batch_size=24 \\\n",
        "  --learning_rate=3e-5 \\\n",
        "  --num_train_epochs=0.1 \\\n",
        "  --max_seq_length=384 \\\n",
        "  --doc_stride=128 \\\n",
        "  --output_dir=gs://gurebert/gureBERT/wordpiece/squad/ \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name=$TPU_NAME \\\n",
        "  --version_2_with_negative=True \\\n",
        "  --init_checkpoint=$GS/wordpiece/model-en_eu \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 14:39:54.130017 140691672389504 deprecation_wrapper.py:119] From /content/gureBERT/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 14:39:54.132371 140691672389504 deprecation_wrapper.py:119] From bert/run_squad.py:1283: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0629 14:39:54.133091 140691672389504 deprecation_wrapper.py:119] From bert/run_squad.py:1127: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0629 14:39:54.133235 140691672389504 deprecation_wrapper.py:119] From bert/run_squad.py:1127: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0629 14:39:54.133359 140691672389504 deprecation_wrapper.py:119] From /content/gureBERT/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 14:39:54.134238 140691672389504 deprecation_wrapper.py:119] From bert/run_squad.py:1133: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0629 14:39:56.352167 140691672389504 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 14:39:57.357671 140691672389504 deprecation_wrapper.py:119] From bert/run_squad.py:229: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0629 14:40:09.042412 140691672389504 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7ff51ec86bf8>) includes params argument, but params are not passed to Estimator.\n",
            "I0629 14:40:09.044273 140691672389504 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/squad/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.36.212.66:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff51ec98048>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.36.212.66:8470', '_evaluation_master': 'grpc://10.36.212.66:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7ff52ad42be0>}\n",
            "I0629 14:40:09.044657 140691672389504 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "W0629 14:40:09.045500 140691672389504 deprecation_wrapper.py:119] From bert/run_squad.py:1065: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0629 14:40:09.050876 140691672389504 deprecation_wrapper.py:119] From bert/run_squad.py:431: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0629 14:40:09.051017 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.051101 140691672389504 run_squad.py:432] unique_id: 1000000000\n",
            "I0629 14:40:09.051173 140691672389504 run_squad.py:433] example_index: 0\n",
            "I0629 14:40:09.051244 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.051458 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t is the na ##me of the ac ##t th ##at was a su ##ccess in cre ##ati ##ng bo ##und ##ari ##es for the cr ##ow ##n and the e ##ic for be ##ing sub ##ject ##iv ##e ? [SEP] pit ##t ' s ac ##t was de ##eme ##d a fa ##ilu ##re be ##ca ##use i ##t qui ##ck ##ly be ##ca ##me ap ##par ##ent th ##at the bo ##und ##ari ##es bet ##we ##en go ##ver ##n ##ment con ##tro ##l and the company ' s power ##s we ##re neb ##ul ##ous and hig ##hl ##y sub ##ject ##iv ##e . the go ##ver ##n ##ment fe ##lt ob ##lig ##ed to res ##po ##nd to human ##itar ##ian call ##s for bet ##ter tre ##at ##ment of local pe ##op ##les in br ##it ##is ##h - o ##c ##c ##up ##ie ##d terr ##ito ##rie ##s . ed ##mund bur ##ke , a form ##er ea ##st india company sha ##re ##hold ##er and diploma ##t , was move ##d to ad ##d ##ress the si ##tua ##tion and int ##rodu ##ce ##d a new re ##gula ##ting bill in 1783 . the bill was def ##ea ##ted ami ##d lo ##b ##by ##ing by company lo ##ya ##list ##s and ac ##c ##us ##ation ##s of ne ##po ##tis ##m in the bill ' s re ##com ##men ##dat ##ion ##s for the ap ##po ##in ##t ##ment of coun ##cil ##lo ##rs . [SEP]\n",
            "I0629 14:40:09.051614 140691672389504 run_squad.py:438] token_to_orig_map: 43:0 44:0 45:0 46:0 47:1 48:1 49:2 50:3 51:3 52:3 53:4 54:5 55:5 56:5 57:6 58:6 59:6 60:7 61:7 62:8 63:8 64:8 65:9 66:9 67:9 68:10 69:10 70:10 71:11 72:11 73:12 74:13 75:13 76:13 77:13 78:14 79:14 80:14 81:15 82:15 83:15 84:15 85:16 86:16 87:16 88:17 89:18 90:19 91:19 92:19 93:20 94:20 95:21 96:21 97:22 98:22 99:22 100:23 101:24 102:24 103:24 104:25 105:25 106:25 107:25 108:25 109:26 110:27 111:27 112:27 113:27 114:28 115:28 116:29 117:29 118:29 119:30 120:31 121:31 122:31 123:32 124:33 125:33 126:33 127:34 128:34 129:35 130:36 131:36 132:37 133:37 134:37 135:38 136:39 137:40 138:40 139:40 140:41 141:42 142:42 143:42 144:42 145:42 146:42 147:42 148:42 149:42 150:42 151:42 152:43 153:43 154:43 155:43 156:43 157:44 158:44 159:45 160:45 161:45 162:46 163:47 164:47 165:48 166:48 167:49 168:50 169:51 170:51 171:51 172:51 173:52 174:53 175:53 176:53 177:54 178:55 179:55 180:56 181:57 182:57 183:57 184:58 185:59 186:59 187:59 188:60 189:61 190:61 191:61 192:61 193:62 194:63 195:64 196:64 197:64 198:65 199:66 200:67 201:67 202:68 203:69 204:70 205:71 206:71 207:71 208:72 209:72 210:73 211:73 212:73 213:73 214:74 215:75 216:76 217:76 218:76 219:76 220:77 221:78 222:78 223:78 224:78 225:78 226:79 227:80 228:80 229:80 230:80 231:81 232:82 233:83 234:83 235:83 236:84 237:84 238:84 239:84 240:84 241:84 242:85 243:86 244:87 245:87 246:87 247:87 248:87 249:88 250:89 251:89 252:89 253:89 254:89\n",
            "I0629 14:40:09.051765 140691672389504 run_squad.py:440] token_is_max_context: 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True\n",
            "I0629 14:40:09.051954 140691672389504 run_squad.py:442] input_ids: 2 8955 118 4535 107 1170 2638 242 107 14638 118 2723 2273 5721 77 402 6878 501 14335 14250 1988 1794 10661 153 199 2002 107 10297 5222 22 1110 107 88 6553 2002 414 334 6218 9422 6684 44 14730 3 13116 118 14716 141 14638 118 5721 79 14566 232 77 1996 11574 409 414 1282 13336 125 118 14187 1734 2587 414 1282 2638 11282 4286 6424 2723 2273 107 1794 10661 153 199 3401 10705 25 496 3432 22 1995 2232 14664 205 1110 107 5009 14716 141 12283 47 4403 409 9062 2065 10712 1110 6617 9808 157 6218 9422 6684 44 14723 107 496 3432 22 1995 8489 5550 5841 11097 661 1370 8927 1971 1336 1370 13167 13557 1747 8771 47 2002 3401 1031 6800 2273 1995 242 7557 1503 2889 1981 501 8987 2282 278 37 14722 228 306 306 6357 1404 232 9887 4351 9787 47 14723 718 2926 3361 2321 14721 77 10029 143 10591 2325 374 5009 3308 409 13321 143 1110 12214 118 14721 5721 6394 232 1370 1756 232 5401 107 1186 2214 10868 1110 3456 12929 470 232 77 956 1577 9441 9870 1512 501 9036 14723 107 1512 5721 9593 947 11212 13223 232 1578 532 2044 334 1539 5009 1578 2220 2344 47 1110 14638 306 200 5595 47 242 1742 1971 4695 364 501 107 1512 14716 141 1577 2313 3994 9627 1210 47 2002 107 11282 1971 211 118 1995 242 11645 13048 743 4602 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.052128 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.052305 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.052394 140691672389504 run_squad.py:448] impossible example\n",
            "I0629 14:40:09.060149 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.060290 140691672389504 run_squad.py:432] unique_id: 1000000001\n",
            "I0629 14:40:09.060395 140691672389504 run_squad.py:433] example_index: 1\n",
            "I0629 14:40:09.060465 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.060693 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t do win ##ner ##s of the con ##tine ##nt ##al com ##pe ##ti ##tion get to do ? [SEP] af ##ter the world cu ##p , the mos ##t im ##port ##ant inter ##national fo ##ot ##bal ##l com ##pe ##ti ##tions are the con ##tine ##nt ##al cha ##mp ##ion ##shi ##ps , whi ##ch are organ ##ise ##d by ea ##ch con ##tine ##nt ##al con ##fe ##der ##ation and con ##tes ##ted bet ##we ##en national te ##am ##s . the ##se are the europ ##ean cha ##mp ##ion ##shi ##p ( u ##ef ##a ) , the co ##pa americ ##a ( con ##me ##bol ) , africa ##n cu ##p of nat ##ion ##s ( ca ##f ) , the asian cu ##p ( af ##c ) , the conc ##ac ##af gold cu ##p ( conc ##ac ##af ) and the of ##c nat ##ion ##s cu ##p ( of ##c ) . the fi ##fa con ##fe ##der ##ation ##s cu ##p is con ##tes ##ted by the win ##ner ##s of all si ##x con ##tine ##nt ##al cha ##mp ##ion ##shi ##ps , the current fi ##fa world cu ##p cha ##mp ##ion ##s and the count ##ry whi ##ch is host ##ing the con ##fe ##der ##ation ##s cu ##p . thi ##s is general ##ly re ##gard ##ed as a war ##m - up tour ##name ##nt for the up ##com ##ing fi ##fa world cu ##p and do ##es not carr ##y the sam ##e prestig ##e as the world cu ##p i ##ts ##el ##f . the mos ##t prestig ##ious com ##pe ##ti ##tions in club fo ##ot ##bal ##l are the res ##pe ##ctive con ##tine ##nt ##al cha ##mp ##ion ##shi ##ps , whi ##ch are general ##ly con ##tes ##ted bet ##we ##en national cha ##mp ##ion ##s , for ex ##am ##ple the u ##ef ##a cha ##mp ##ion ##s le ##ag ##ue in europ ##e and the co ##pa liber ##tadore ##s in south americ ##a . the win ##ner ##s of ea ##ch con ##tine ##nt ##al com ##pe ##ti ##tion con ##tes ##t the fi ##fa club world cu ##p . [SEP]\n",
            "I0629 14:40:09.060899 140691672389504 run_squad.py:438] token_to_orig_map: 22:0 23:0 24:1 25:2 26:3 27:3 28:3 29:4 30:5 31:5 32:6 33:6 34:6 35:7 36:7 37:8 38:8 39:8 40:8 41:9 42:9 43:9 44:9 45:10 46:11 47:12 48:12 49:12 50:12 51:13 52:13 53:13 54:13 55:13 56:13 57:14 58:14 59:15 60:16 61:16 62:16 63:17 64:18 65:18 66:19 67:19 68:19 69:19 70:20 71:20 72:20 73:20 74:21 75:22 76:22 77:22 78:23 79:23 80:23 81:24 82:25 83:25 84:25 85:25 86:26 87:26 88:27 89:28 90:29 91:29 92:30 93:30 94:30 95:30 96:30 97:31 98:31 99:31 100:31 101:31 102:31 103:32 104:33 105:33 106:34 107:34 108:35 109:35 110:35 111:35 112:35 113:35 114:36 115:36 116:37 117:37 118:38 119:39 120:39 121:39 122:40 123:40 124:40 125:40 126:40 127:41 128:42 129:43 130:43 131:44 132:44 133:44 134:44 135:44 136:45 137:46 138:46 139:46 140:47 141:48 142:48 143:49 144:49 145:49 146:49 147:49 148:50 149:51 150:52 151:52 152:53 153:53 154:53 155:54 156:54 157:55 158:55 159:55 160:55 161:55 162:56 163:57 164:57 165:58 166:58 167:58 168:58 169:58 170:59 171:59 172:60 173:61 174:61 175:61 176:62 177:63 178:64 179:64 180:64 181:65 182:66 183:67 184:67 185:68 186:68 187:68 188:68 189:69 190:69 191:69 192:69 193:69 194:69 195:70 196:71 197:72 198:72 199:73 200:74 201:74 202:75 203:75 204:75 205:75 206:76 207:77 208:78 209:78 210:79 211:79 212:80 213:81 214:81 215:82 216:83 217:83 218:83 219:83 220:83 221:84 222:84 223:84 224:85 225:85 226:86 227:87 228:87 229:88 230:88 231:88 232:89 233:90 234:91 235:91 236:91 237:91 238:92 239:92 240:92 241:93 242:94 243:95 244:95 245:95 246:96 247:96 248:97 249:98 250:98 251:99 252:100 253:100 254:101 255:102 256:102 257:103 258:104 259:104 260:105 261:105 262:106 263:107 264:108 265:109 266:109 267:110 268:110 269:110 270:110 271:110 272:111 273:112 274:112 275:113 276:113 277:114 278:114 279:114 280:114 281:115 282:116 283:117 284:117 285:117 286:117 287:118 288:119 289:120 290:120 291:120 292:121 293:121 294:121 295:121 296:122 297:122 298:122 299:122 300:122 301:122 302:123 303:123 304:124 305:125 306:125 307:126 308:126 309:126 310:127 311:127 312:127 313:128 314:129 315:129 316:129 317:129 318:129 319:130 320:131 321:131 322:131 323:132 324:133 325:133 326:133 327:134 328:134 329:134 330:134 331:135 332:135 333:135 334:136 335:137 336:137 337:138 338:139 339:140 340:140 341:141 342:141 343:141 344:142 345:143 346:144 347:144 348:144 349:145 350:146 351:146 352:146 353:147 354:148 355:148 356:149 357:149 358:149 359:149 360:150 361:150 362:150 363:150 364:151 365:151 366:151 367:152 368:153 369:153 370:154 371:155 372:156 373:156 374:156\n",
            "I0629 14:40:09.061096 140691672389504 run_squad.py:440] token_is_max_context: 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True 361:True 362:True 363:True 364:True 365:True 366:True 367:True 368:True 369:True 370:True 371:True 372:True 373:True 374:True\n",
            "I0629 14:40:09.081612 140691672389504 run_squad.py:442] input_ids: 2 8955 118 2027 4154 1455 47 242 107 2232 14122 3470 656 5656 889 753 10868 10635 1370 2027 14730 3 8495 1031 107 2114 3535 538 14721 107 13271 118 3376 11420 2800 3508 12126 2817 1535 1194 205 5656 889 753 4699 4438 107 2232 14122 3470 656 2559 10042 1210 5420 11877 14721 7849 1014 4438 10158 10963 232 1539 10591 1014 2232 14122 3470 656 2232 6575 9123 5595 1110 2232 6466 11212 3401 10705 25 3825 966 1949 47 14723 107 1202 4438 107 11240 108 2559 10042 1210 5420 538 14717 1484 5968 16 14718 14721 107 1010 1276 14094 16 14717 2232 2638 14256 14718 14721 7865 22 3535 538 242 6644 1210 47 14717 1150 313 14718 14721 107 2231 3535 538 14717 8495 306 14718 14721 107 4123 1009 6367 4038 3535 538 14717 4123 1009 6367 14718 1110 107 242 306 6644 1210 47 3535 538 14717 242 306 14718 14723 107 1193 4835 2232 6575 9123 5595 47 3535 538 4535 2232 6466 11212 1539 107 4154 1455 47 242 2272 1186 893 2232 14122 3470 656 2559 10042 1210 5420 11877 14721 107 8216 1193 4835 2114 3535 538 2559 10042 1210 47 1110 107 9164 1398 7849 1014 4535 1083 334 107 2232 6575 9123 5595 47 3535 538 14723 5338 47 4535 8342 2587 1577 12367 661 1710 77 4266 364 14722 13127 8984 1753 3470 2002 107 13127 2313 334 1193 4835 2114 3535 538 1110 2027 199 11050 10126 157 107 1782 44 7588 44 1710 107 2114 3535 538 125 2245 553 313 14723 107 13271 118 7588 6155 5656 889 753 4699 501 7629 2817 1535 1194 205 4438 107 8927 889 9864 2232 14122 3470 656 2559 10042 1210 5420 11877 14721 7849 1014 4438 8342 2587 2232 6466 11212 3401 10705 25 3825 2559 10042 1210 47 14721 2002 1965 1949 5496 107 1484 5968 16 2559 10042 1210 47 506 4566 2616 501 11240 44 1110 107 1010 1276 12802 12739 47 501 8438 14094 16 14723 107 4154 1455 47 242 10591 1014 2232 14122 3470 656 5656 889 753 10868 2232 6466 118 107 1193 4835 7629 2114 3535 538 14723 3 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.082339 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.082712 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.082847 140691672389504 run_squad.py:451] start_position: 364\n",
            "I0629 14:40:09.082950 140691672389504 run_squad.py:452] end_position: 373\n",
            "I0629 14:40:09.083049 140691672389504 run_squad.py:454] answer: con ##tes ##t the fi ##fa club world cu ##p\n",
            "I0629 14:40:09.088720 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.088874 140691672389504 run_squad.py:432] unique_id: 1000000002\n",
            "I0629 14:40:09.088957 140691672389504 run_squad.py:433] example_index: 2\n",
            "I0629 14:40:09.089027 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.089195 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t di ##d frederic ##k zu ##gi ##be stu ##dy in de ##ta ##il for thi ##s bo ##ok ? [SEP] in his bo ##ok the cru ##ci ##fi ##xio ##n of jesus , physi ##cia ##n and for ##en ##si ##c pat ##hol ##og ##ist frederic ##k zu ##gi ##be stu ##die ##d the li ##ke ##ly c ##ir ##cum ##sta ##nce ##s of the death of jesus in gre ##at de ##ta ##il . zu ##gi ##be carr ##ie ##d o ##ut a nu ##mb ##er of exp ##eri ##ment ##s o ##ver sev ##eral ye ##ar ##s to test his theo ##rie ##s whi ##le he was a medic ##al ex ##ami ##ner . the ##se stu ##die ##s inc ##lu ##de ##d exp ##eri ##ment ##s in whi ##ch vol ##unt ##ee ##rs with spe ##ci ##fi ##c we ##ight ##s we ##re han ##gin ##g at spe ##ci ##fi ##c ang ##les and the am ##ou ##nt of pu ##ll on ea ##ch hand was mea ##sur ##ed , in cas ##es w ##here the fe ##et we ##re al ##so sec ##ure ##d or not . in the ##se cas ##es the am ##ou ##nt of pu ##ll and the corr ##es ##po ##ndi ##ng pa ##in was fo ##und to be sig ##ni ##fi ##can ##t . [SEP]\n",
            "I0629 14:40:09.089369 140691672389504 run_squad.py:438] token_to_orig_map: 23:0 24:1 25:2 26:2 27:3 28:4 29:4 30:4 31:4 32:4 33:5 34:6 35:6 36:7 37:7 38:7 39:8 40:9 41:9 42:9 43:9 44:10 45:10 46:10 47:10 48:11 49:11 50:12 51:12 52:12 53:13 54:13 55:13 56:14 57:15 58:15 59:15 60:16 61:16 62:16 63:16 64:16 65:16 66:17 67:18 68:19 69:20 70:21 71:22 72:23 73:23 74:24 75:24 76:24 77:24 78:25 79:25 80:25 81:26 82:26 83:26 84:27 85:27 86:28 87:29 88:29 89:29 90:30 91:31 92:31 93:31 94:31 95:32 96:32 97:33 98:33 99:34 100:34 101:34 102:35 103:36 104:37 105:38 106:38 107:38 108:39 109:39 110:40 111:41 112:42 113:43 114:43 115:44 116:44 117:44 118:44 119:45 120:45 121:46 122:46 123:46 124:47 125:47 126:47 127:47 128:48 129:48 130:48 131:48 132:49 133:50 134:50 135:51 136:51 137:51 138:51 139:52 140:53 141:53 142:53 143:53 144:54 145:54 146:54 147:55 148:55 149:56 150:56 151:56 152:57 153:58 154:58 155:58 156:58 157:59 158:59 159:60 160:61 161:62 162:62 163:62 164:63 165:64 166:64 167:65 168:66 169:66 170:67 171:68 172:69 173:69 174:69 175:69 176:70 177:71 178:71 179:72 180:72 181:73 182:74 183:74 184:75 185:75 186:76 187:76 188:77 189:77 190:77 191:78 192:79 193:79 194:80 195:81 196:81 197:82 198:82 199:83 200:84 201:84 202:84 203:85 204:86 205:86 206:87 207:88 208:89 209:89 210:89 211:89 212:89 213:90 214:90 215:91 216:92 217:92 218:93 219:94 220:95 221:95 222:95 223:95 224:95 225:95\n",
            "I0629 14:40:09.089549 140691672389504 run_squad.py:440] token_is_max_context: 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True\n",
            "I0629 14:40:09.089766 140691672389504 run_squad.py:442] input_ids: 2 8955 118 347 232 1853 20 1038 1118 2029 14350 1726 501 79 105 683 2002 5338 47 1794 5861 14730 3 501 13732 1794 5861 107 9847 6479 2674 13667 22 242 805 14721 5346 8852 22 1110 2002 25 1508 306 2592 12656 6012 10961 1853 20 1038 1118 2029 14350 4768 232 107 1179 2321 2587 213 4225 4357 1341 4549 47 242 107 3759 242 805 501 3503 2273 79 105 683 14723 1038 1118 2029 10126 1404 232 228 2285 77 13542 14051 143 242 8285 14425 1995 47 228 3432 6915 14535 10871 320 47 1370 12562 13732 14232 9787 47 7849 286 617 5721 77 8048 656 1965 10714 1455 14723 107 1202 14350 4768 47 11393 11048 609 232 8285 14425 1995 47 501 7849 1014 13190 11667 13566 4602 4479 6076 6479 2674 306 4403 9191 47 4403 409 1086 9584 303 1003 6076 6479 2674 306 9875 1981 1110 107 1566 6132 3470 242 2277 3439 2085 10591 1014 5864 5721 4712 9923 661 14721 501 14399 199 319 5633 107 8489 722 4403 409 166 2157 13527 2841 232 894 11050 14723 501 107 1202 14399 199 107 1566 6132 3470 242 2277 3439 1110 107 10561 199 1971 4787 1988 468 211 5721 2817 10661 1370 414 5385 2243 2674 6504 118 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.089926 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.090081 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.090152 140691672389504 run_squad.py:451] start_position: 56\n",
            "I0629 14:40:09.090217 140691672389504 run_squad.py:452] end_position: 70\n",
            "I0629 14:40:09.090285 140691672389504 run_squad.py:454] answer: the li ##ke ##ly c ##ir ##cum ##sta ##nce ##s of the death of jesus\n",
            "I0629 14:40:09.095024 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.095159 140691672389504 run_squad.py:432] unique_id: 1000000003\n",
            "I0629 14:40:09.095238 140691672389504 run_squad.py:433] example_index: 3\n",
            "I0629 14:40:09.095308 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.095493 140691672389504 run_squad.py:436] tokens: [CLS] how man ##y mill ##ion ##s of tour ##ist ##s do ##es gre ##ece ban ea ##ch ye ##ar ? [SEP] gre ##ece at ##tra ##ct ##s more th ##an 16 mill ##ion tour ##ist ##s ea ##ch ye ##ar , th ##us contrib ##uti ##ng 18 . 2 % to the nat ##ion ' s g ##d ##p in 2008 ac ##co ##rd ##ing to an oe ##c ##d re ##port . the sam ##e sur ##ve ##y show ##ed th ##at the av ##era ##ge tour ##ist exp ##end ##itur ##e whi ##le in gre ##ece was $ 1 , 0 ##73 , ran ##kin ##g gre ##ece 10 ##th in the world . the nu ##mb ##er of jo ##bs di ##re ##ct ##ly or indi ##re ##ct ##ly re ##lat ##ed to the tour ##is ##m sec ##tor we ##re 8 ##40 , 0 ##00 in 2008 and re ##pr ##es ##ente ##d 19 % of the count ##ry ' s tota ##l labor for ##ce . in 2009 , gre ##ece wel ##com ##ed o ##ver 19 . 3 mill ##ion tour ##ist ##s , a ma ##jor inc ##rea ##se from the 17 . 7 mill ##ion tour ##ist ##s the count ##ry wel ##com ##ed in 2008 . [SEP]\n",
            "I0629 14:40:09.095662 140691672389504 run_squad.py:438] token_to_orig_map: 22:0 23:0 24:1 25:1 26:1 27:1 28:2 29:3 30:3 31:4 32:5 33:5 34:6 35:6 36:6 37:7 38:7 39:8 40:8 41:8 42:9 43:9 44:10 45:10 46:10 47:11 48:11 49:11 50:11 51:12 52:13 53:14 54:14 55:14 56:14 57:15 58:15 59:15 60:16 61:17 62:18 63:18 64:18 65:18 66:19 67:20 68:21 69:21 70:21 71:22 72:22 73:22 74:23 75:24 76:24 77:25 78:25 79:25 80:26 81:26 82:27 83:27 84:28 85:29 86:29 87:29 88:30 89:30 90:31 91:31 92:31 93:31 94:32 95:32 96:33 97:34 98:34 99:35 100:36 101:36 102:36 103:36 104:36 105:36 106:37 107:37 108:37 109:38 110:38 111:39 112:39 113:40 114:41 115:42 116:42 117:43 118:44 119:44 120:44 121:45 122:46 123:46 124:47 125:47 126:47 127:47 128:48 129:49 130:49 131:49 132:49 133:50 134:50 135:50 136:51 137:52 138:53 139:53 140:53 141:54 142:54 143:55 144:55 145:56 146:56 147:56 148:56 149:56 150:57 151:58 152:59 153:60 154:60 155:60 156:60 157:60 158:61 159:61 160:62 161:63 162:64 163:64 164:64 165:64 166:65 167:65 168:66 169:67 170:67 171:67 172:68 173:69 174:69 175:70 176:70 177:71 178:71 179:71 180:72 181:72 182:73 183:73 184:73 185:74 186:74 187:75 188:75 189:75 190:75 191:76 192:77 193:77 194:78 195:78 196:78 197:79 198:80 199:81 200:81 201:81 202:82 203:82 204:83 205:83 206:83 207:84 208:85 209:85 210:86 211:86 212:86 213:87 214:88 215:88\n",
            "I0629 14:40:09.095822 140691672389504 run_squad.py:440] token_is_max_context: 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True\n",
            "I0629 14:40:09.096034 140691672389504 run_squad.py:442] input_ids: 2 12790 1036 157 6220 1210 47 242 8984 10961 47 2027 199 3503 6338 3271 10591 1014 10871 320 14730 3 3503 6338 1003 5849 13354 47 6470 2723 34 390 6220 1210 8984 10961 47 10591 1014 10871 320 14721 2723 200 14351 14286 1988 302 14723 422 14714 1370 107 6644 1210 14716 141 279 232 538 501 222 14638 776 6280 334 1370 1420 6315 306 232 1577 11420 14723 107 1782 44 13592 1580 157 2130 661 2723 2273 107 10710 148 1616 8984 10961 8285 5599 8536 44 7849 286 501 3503 6338 5721 14713 218 14721 8182 4700 14721 5921 982 303 3503 6338 471 2255 501 107 2114 14723 107 13542 14051 143 242 245 11083 347 409 13354 2587 894 4229 409 13354 2587 1577 10467 661 1370 107 8984 278 364 13527 4557 4403 409 588 4021 14721 8182 1201 501 222 1110 1577 11658 199 2086 232 453 14714 242 107 9164 1398 14716 141 9328 205 13645 2002 470 14723 501 377 14721 3503 6338 9596 2313 661 228 3432 453 14723 187 6220 1210 8984 10961 47 14721 77 600 11409 11393 3958 1202 13616 107 363 14723 554 6220 1210 8984 10961 47 107 9164 1398 9596 2313 661 501 222 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.096228 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.184712 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.184982 140691672389504 run_squad.py:448] impossible example\n",
            "I0629 14:40:09.191817 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.192018 140691672389504 run_squad.py:432] unique_id: 1000000004\n",
            "I0629 14:40:09.192102 140691672389504 run_squad.py:433] example_index: 4\n",
            "I0629 14:40:09.192173 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.192393 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t is the mai ##n ro ##le of the ca ##bi ##net of go ##ver ##n ##ment mini ##ster ##s ? [SEP] w ##hen the part ##y is re ##pr ##es ##ente ##d by me ##mb ##ers in the lowe ##r house of par ##liam ##ent , the part ##y le ##ader sim ##ulta ##ne ##ous ##ly serv ##es as the le ##ader of the par ##liam ##ent ##ar ##y gr ##ou ##p of th ##at ful ##l part ##y re ##pr ##es ##ent ##ation ; de ##pend ##ing on a minim ##um nu ##mb ##er of sea ##ts hel ##d , west ##min ##ster - base ##d parti ##es ty ##pi ##cal ##ly all ##ow for le ##ader ##s to form front ##ben ##ch te ##am ##s of sen ##ior fe ##llo ##w me ##mb ##ers of the par ##liam ##ent ##ar ##y gr ##ou ##p to serv ##e as cr ##it ##ics of as ##pe ##ct ##s of go ##ver ##n ##ment poli ##cy . w ##hen a part ##y be ##com ##es the lar ##ges ##t part ##y not part of the go ##ver ##n ##ment , the part ##y ' s par ##liam ##ent ##ar ##y gr ##ou ##p form ##s the of ##fi ##cia ##l opp ##osi ##tion , with of ##fi ##cia ##l opp ##osi ##tion front ##ben ##ch te ##am me ##mb ##ers of ##ten form ##ing the of ##fi ##cia ##l opp ##osi ##tion shad ##ow ca ##bi ##net . w ##hen a part ##y ac ##hi ##eves en ##ough sea ##ts in an ele ##ction to form a ma ##jor ##it ##y , the part ##y ' s front ##ben ##ch be ##com ##es the ca ##bi ##net of go ##ver ##n ##ment mini ##ster ##s . [SEP]\n",
            "I0629 14:40:09.192573 140691672389504 run_squad.py:438] token_to_orig_map: 24:0 25:0 26:1 27:2 28:2 29:3 30:4 31:4 32:4 33:4 34:4 35:5 36:6 37:6 38:6 39:7 40:8 41:9 42:9 43:10 44:11 45:12 46:12 47:12 48:12 49:13 50:14 51:14 52:15 53:15 54:16 55:16 56:16 57:16 58:16 59:17 60:17 61:18 62:19 63:20 64:20 65:21 66:22 67:23 68:23 69:23 70:23 71:23 72:24 73:24 74:24 75:25 76:26 77:26 78:27 79:27 80:28 81:28 82:29 83:29 84:29 85:29 86:29 87:29 88:30 89:30 90:30 91:31 92:32 93:33 94:33 95:34 96:34 97:34 98:35 99:36 100:36 101:37 102:37 103:37 104:38 105:38 106:38 107:38 108:38 109:38 110:39 111:39 112:40 113:40 114:40 115:40 116:41 117:41 118:42 119:43 120:43 121:43 122:44 123:45 124:46 125:46 126:46 127:47 128:47 129:47 130:48 131:49 132:49 133:50 134:50 135:50 136:51 137:51 138:51 139:52 140:53 141:54 142:54 143:54 144:54 145:54 146:55 147:55 148:55 149:56 150:57 151:57 152:58 153:59 154:59 155:59 156:60 157:61 158:61 159:61 160:61 161:62 162:63 163:63 164:63 165:63 166:64 167:64 168:64 169:65 170:65 171:66 172:67 173:67 174:68 175:68 176:68 177:69 178:70 179:70 180:70 181:71 182:71 183:72 184:73 185:74 186:75 187:76 188:76 189:76 190:76 191:76 192:77 193:78 194:78 195:78 196:78 197:79 198:79 199:79 200:79 201:79 202:80 203:80 204:80 205:81 206:81 207:82 208:83 209:83 210:83 211:83 212:84 213:84 214:84 215:84 216:85 217:86 218:86 219:86 220:86 221:87 222:87 223:87 224:88 225:88 226:88 227:89 228:89 229:90 230:90 231:90 232:91 233:91 234:92 235:92 236:93 237:94 238:94 239:94 240:94 241:95 242:95 243:95 244:96 245:96 246:97 247:97 248:97 249:97 250:98 251:98 252:99 253:100 254:100 255:101 256:101 257:101 258:102 259:102 260:103 261:103 262:104 263:105 264:106 265:106 266:107 267:108 268:109 269:110 270:110 271:110 272:110 273:110 274:111 275:112 276:112 277:112 278:112 279:113 280:113 281:113 282:114 283:114 284:114 285:115 286:116 287:116 288:116 289:117 290:118 291:118 292:118 293:118 294:119 295:119 296:119 297:119\n",
            "I0629 14:40:09.192744 140691672389504 run_squad.py:440] token_is_max_context: 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True\n",
            "I0629 14:40:09.192934 140691672389504 run_squad.py:442] input_ids: 2 8955 118 4535 107 9798 22 637 286 242 107 1150 1609 3598 242 496 3432 22 1995 13757 6543 47 14730 3 319 9509 107 4453 157 4535 1577 11658 199 2086 232 1539 1112 14051 14168 501 107 11862 93 4309 242 1785 13685 6424 14721 107 4453 157 506 10775 7025 13425 569 10712 2587 11016 199 1710 107 506 10775 242 107 1785 13685 6424 320 157 6459 6132 538 242 2723 2273 5794 205 4453 157 1577 11658 199 6424 5595 14726 79 13968 334 2085 77 8623 801 13542 14051 143 242 6766 2245 4792 232 14721 5151 5860 6543 14722 10934 232 3342 199 2049 3393 13962 2587 2272 5222 2002 506 10775 47 1370 10029 11449 10513 1014 966 1949 47 242 3522 14307 8489 10099 882 1112 14051 14168 242 107 1785 13685 6424 320 157 6459 6132 538 1370 11016 44 1710 10297 2282 12937 242 1710 889 13354 47 242 496 3432 22 1995 10912 6735 14723 319 9509 77 4453 157 414 2313 199 107 3181 2892 118 4453 157 11050 4453 242 107 496 3432 22 1995 14721 107 4453 157 14716 141 1785 13685 6424 320 157 6459 6132 538 10029 47 107 242 2674 8852 205 9570 10833 10868 14721 4479 242 2674 8852 205 9570 10833 10868 11449 10513 1014 966 1949 1112 14051 14168 242 483 10029 334 107 242 2674 8852 205 9570 10833 10868 13217 5222 1150 1609 3598 14723 319 9509 77 4453 157 14638 3445 7033 2795 10179 6766 2245 501 1420 6152 10748 1370 10029 77 600 11409 2282 157 14721 107 4453 157 14716 141 11449 10513 1014 414 2313 199 107 1150 1609 3598 242 496 3432 22 1995 13757 6543 47 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.193108 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.193285 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.193359 140691672389504 run_squad.py:448] impossible example\n",
            "I0629 14:40:09.201660 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.201833 140691672389504 run_squad.py:432] unique_id: 1000000005\n",
            "I0629 14:40:09.201918 140691672389504 run_squad.py:433] example_index: 5\n",
            "I0629 14:40:09.201989 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.202224 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t do ##es the fe ##p ##c stan ##d for ? [SEP] f ##d ##r ' s new de ##al pro ##gr ##am ##s of ##ten con ##tain ##ed e ##qu ##al opp ##ortu ##ni ##ty cla ##use ##s sta ##ting \" no dis ##c ##rim ##ination sha ##ll be made on ac ##count of ra ##ce , col ##or or cre ##ed \" , : 11 bu ##t the tru ##e for ##er ##un ##ner to aff ##ir ##mat ##iv ##e ac ##tion was the inter ##ior sec ##ret ##ar ##y of the tim ##e , haro ##ld l . i ##ck ##es . i ##ck ##es pro ##hibit ##ed dis ##c ##rim ##ination in hiri ##ng for publi ##c work ##s ad ##minis ##tra ##tion fun ##de ##d pro ##ject ##s and o ##vers ##aw not on ##ly the ins ##titu ##tion of a qu ##ota sy ##stem , w ##here con ##tra ##ctor ##s we ##re re ##qu ##ire ##d to em ##p ##lo ##y a fi ##xe ##d per ##cent ##age of black work ##ers , by robert c . we ##a ##ver and clar ##k for ##eman , : 12 bu ##t al ##so the e ##qu ##al pay of wo ##men pro ##pos ##ed by harr ##y hopkins . : 14 ##f ##d ##r ' s lar ##ges ##t contrib ##utio ##n to aff ##ir ##mat ##iv ##e ac ##tion , how ##ever , la ##y in his ex ##ec ##uti ##ve orde ##r 8 ##80 ##2 whi ##ch pro ##hibit ##ed dis ##c ##rim ##ination in the def ##en ##se indus ##try or go ##ver ##n ##ment . : 22 the ex ##ec ##uti ##ve orde ##r pro ##mot ##ed the ide ##a th ##at i ##f ta ##x ##pay ##er fun ##ds we ##re ac ##ce ##pt ##ed th ##ro ##ugh a go ##ver ##n ##ment con ##tra ##ct , the ##n all ta ##x ##pay ##ers sh ##ould ha ##ve an e ##qu ##al opp ##ortu ##ni ##ty to work th ##ro ##ugh the con ##tra ##ctor . : 23 – 4 to en ##for ##ce thi ##s ide ##a , roosevelt cre ##ate ##d the fa ##ir em ##p ##lo ##y ##ment pra ##ct ##ice ##s comm ##itte ##e ( fe [SEP]\n",
            "I0629 14:40:09.202459 140691672389504 run_squad.py:438] token_to_orig_map: 14:0 15:0 16:0 17:0 18:0 19:1 20:2 21:2 22:3 23:3 24:3 25:3 26:4 27:4 28:5 29:5 30:5 31:6 32:6 33:6 34:7 35:7 36:7 37:7 38:8 39:8 40:8 41:9 42:9 43:10 44:10 45:11 46:11 47:11 48:11 49:12 50:12 51:13 52:14 53:15 54:16 55:16 56:17 57:18 58:18 59:18 60:19 61:19 62:20 63:21 64:21 65:21 66:21 67:21 68:21 69:22 70:22 71:23 72:24 73:24 74:25 75:25 76:25 77:25 78:26 79:27 80:27 81:27 82:27 83:27 84:28 85:28 86:29 87:30 88:31 89:31 90:32 91:32 92:32 93:32 94:33 95:34 96:35 97:35 98:35 99:36 100:36 101:37 102:37 103:38 104:38 105:38 106:38 107:39 108:39 109:39 110:40 111:40 112:40 113:41 114:41 115:41 116:41 117:42 118:43 119:43 120:44 121:45 122:45 123:46 124:46 125:47 126:47 127:47 128:47 129:48 130:48 131:48 132:49 133:49 134:49 135:50 136:51 137:51 138:51 139:52 140:53 141:53 142:54 143:55 144:55 145:55 146:56 147:57 148:58 149:58 150:59 151:59 152:59 153:60 154:60 155:61 156:61 157:61 158:61 159:62 160:62 161:63 162:63 163:63 164:63 165:64 166:65 167:65 168:65 169:65 170:66 171:67 172:67 173:67 174:68 175:68 176:68 177:69 178:70 179:71 180:71 181:71 182:72 183:73 184:74 185:74 186:75 187:75 188:75 189:76 190:77 191:77 192:78 193:78 194:78 195:78 196:78 197:79 198:79 199:80 200:80 201:81 202:82 203:82 204:82 205:83 206:84 207:85 208:85 209:86 210:86 211:86 212:87 213:88 214:88 215:89 216:89 217:89 218:89 219:89 220:89 221:89 222:89 223:89 224:90 225:90 226:90 227:91 228:91 229:91 230:92 231:93 232:93 233:93 234:93 235:93 236:94 237:94 238:94 239:95 240:95 241:95 242:96 243:96 244:97 245:98 246:99 247:99 248:99 249:99 250:100 251:100 252:101 253:101 254:101 255:102 256:102 257:103 258:103 259:103 260:104 261:104 262:104 263:104 264:105 265:106 266:107 267:107 268:107 269:108 270:108 271:109 272:110 273:110 274:110 275:110 276:110 277:110 278:110 279:111 280:112 281:112 282:112 283:112 284:113 285:113 286:114 287:114 288:114 289:115 290:116 291:116 292:117 293:117 294:118 295:118 296:119 297:119 298:119 299:119 300:120 301:120 302:121 303:121 304:122 305:122 306:122 307:122 308:123 309:123 310:123 311:124 312:125 313:125 314:125 315:125 316:126 317:126 318:126 319:126 320:127 321:127 322:128 323:129 324:129 325:129 326:129 327:130 328:130 329:131 330:131 331:132 332:133 333:133 334:133 335:134 336:134 337:134 338:134 339:135 340:136 341:137 342:137 343:137 344:138 345:139 346:139 347:139 348:139 349:139 350:139 351:139 352:139 353:140 354:141 355:141 356:141 357:142 358:142 359:143 360:143 361:143 362:144 363:145 364:145 365:145 366:146 367:147 368:147 369:148 370:148 371:148 372:148 373:148 374:149 375:149 376:149 377:149 378:150 379:150 380:150 381:151 382:151\n",
            "I0629 14:40:09.202663 140691672389504 run_squad.py:440] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "I0629 14:40:09.287757 140691672389504 run_squad.py:442] input_ids: 2 8955 118 2027 199 107 8489 538 306 1687 232 2002 14730 3 317 232 93 14716 141 956 79 656 1421 10067 1949 47 242 483 2232 6588 661 88 11439 656 9570 11418 2243 4358 11167 13336 47 5757 9870 14711 738 4730 306 14429 13109 3308 3439 414 8680 2085 14638 6850 242 3309 470 14721 3173 905 894 14335 661 14711 14721 14725 412 1373 118 107 11547 44 2002 143 1055 1455 1370 8064 4225 9760 6684 44 14638 10868 5721 107 3508 14307 13527 2576 320 157 242 107 7040 44 14721 14447 3525 335 14723 125 1734 199 14723 125 1734 199 1421 5062 661 4730 306 14429 13109 501 209 1988 2002 3625 306 12593 47 1756 12940 5849 10868 6228 609 232 1421 9422 47 1110 228 11608 14628 11050 2085 2587 107 9566 9057 10868 242 77 3647 6897 9169 4516 14721 319 5633 2232 5849 3923 47 4403 409 1577 11439 3400 232 1370 3412 538 743 157 77 1193 2566 232 1023 12219 9926 242 2118 12593 14168 14721 1539 201 213 14723 4403 16 3432 1110 2644 20 2002 3132 14721 14725 386 1373 118 166 2157 107 88 11439 656 4347 242 14699 3994 1421 11039 661 1539 6011 157 7166 14723 14725 294 313 232 93 14716 141 3181 2892 118 14351 8351 22 1370 8064 4225 9760 6684 44 14638 10868 14721 12790 6767 14721 290 157 501 13732 1965 8262 14286 1580 6983 93 588 2793 117 7849 1014 1421 5062 661 4730 306 14429 13109 501 107 9593 25 1202 5384 10064 894 496 3432 22 1995 14723 14725 513 107 1965 8262 14286 1580 6983 93 1421 8881 661 107 13686 16 2723 2273 125 313 1927 893 11164 143 6228 14473 4403 409 14638 470 6489 661 2723 745 6101 77 496 3432 22 1995 2232 5849 13354 14721 107 22 2272 1927 893 11164 14168 1978 13700 346 1580 1420 88 11439 656 9570 11418 2243 4358 1370 12593 2723 745 6101 107 2232 5849 3923 14723 14725 479 234 344 1370 2795 14498 470 5338 47 13686 16 14721 3682 14335 4640 232 107 1996 4225 3412 538 743 157 1995 14606 13354 14120 47 11258 8646 44 14717 8489 3\n",
            "I0629 14:40:09.289028 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 14:40:09.289411 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0629 14:40:09.289542 140691672389504 run_squad.py:451] start_position: 367\n",
            "I0629 14:40:09.289627 140691672389504 run_squad.py:452] end_position: 380\n",
            "I0629 14:40:09.289708 140691672389504 run_squad.py:454] answer: fa ##ir em ##p ##lo ##y ##ment pra ##ct ##ice ##s comm ##itte ##e\n",
            "I0629 14:40:09.293003 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.293146 140691672389504 run_squad.py:432] unique_id: 1000000006\n",
            "I0629 14:40:09.293239 140691672389504 run_squad.py:433] example_index: 5\n",
            "I0629 14:40:09.293313 140691672389504 run_squad.py:434] doc_span_index: 1\n",
            "I0629 14:40:09.294897 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t do ##es the fe ##p ##c stan ##d for ? [SEP] the ins ##titu ##tion of a qu ##ota sy ##stem , w ##here con ##tra ##ctor ##s we ##re re ##qu ##ire ##d to em ##p ##lo ##y a fi ##xe ##d per ##cent ##age of black work ##ers , by robert c . we ##a ##ver and clar ##k for ##eman , : 12 bu ##t al ##so the e ##qu ##al pay of wo ##men pro ##pos ##ed by harr ##y hopkins . : 14 ##f ##d ##r ' s lar ##ges ##t contrib ##utio ##n to aff ##ir ##mat ##iv ##e ac ##tion , how ##ever , la ##y in his ex ##ec ##uti ##ve orde ##r 8 ##80 ##2 whi ##ch pro ##hibit ##ed dis ##c ##rim ##ination in the def ##en ##se indus ##try or go ##ver ##n ##ment . : 22 the ex ##ec ##uti ##ve orde ##r pro ##mot ##ed the ide ##a th ##at i ##f ta ##x ##pay ##er fun ##ds we ##re ac ##ce ##pt ##ed th ##ro ##ugh a go ##ver ##n ##ment con ##tra ##ct , the ##n all ta ##x ##pay ##ers sh ##ould ha ##ve an e ##qu ##al opp ##ortu ##ni ##ty to work th ##ro ##ugh the con ##tra ##ctor . : 23 – 4 to en ##for ##ce thi ##s ide ##a , roosevelt cre ##ate ##d the fa ##ir em ##p ##lo ##y ##ment pra ##ct ##ice ##s comm ##itte ##e ( fe ##p ##c ) with the power to in ##ve ##sti ##gat ##e hiri ##ng pra ##ct ##ice ##s by go ##ver ##n ##ment con ##tra ##ctor ##s . : 22 [SEP]\n",
            "I0629 14:40:09.295256 140691672389504 run_squad.py:438] token_to_orig_map: 14:54 15:55 16:55 17:55 18:56 19:57 20:58 21:58 22:59 23:59 24:59 25:60 26:60 27:61 28:61 29:61 30:61 31:62 32:62 33:63 34:63 35:63 36:63 37:64 38:65 39:65 40:65 41:65 42:66 43:67 44:67 45:67 46:68 47:68 48:68 49:69 50:70 51:71 52:71 53:71 54:72 55:73 56:74 57:74 58:75 59:75 60:75 61:76 62:77 63:77 64:78 65:78 66:78 67:78 68:78 69:79 70:79 71:80 72:80 73:81 74:82 75:82 76:82 77:83 78:84 79:85 80:85 81:86 82:86 83:86 84:87 85:88 86:88 87:89 88:89 89:89 90:89 91:89 92:89 93:89 94:89 95:89 96:90 97:90 98:90 99:91 100:91 101:91 102:92 103:93 104:93 105:93 106:93 107:93 108:94 109:94 110:94 111:95 112:95 113:95 114:96 115:96 116:97 117:98 118:99 119:99 120:99 121:99 122:100 123:100 124:101 125:101 126:101 127:102 128:102 129:103 130:103 131:103 132:104 133:104 134:104 135:104 136:105 137:106 138:107 139:107 140:107 141:108 142:108 143:109 144:110 145:110 146:110 147:110 148:110 149:110 150:110 151:111 152:112 153:112 154:112 155:112 156:113 157:113 158:114 159:114 160:114 161:115 162:116 163:116 164:117 165:117 166:118 167:118 168:119 169:119 170:119 171:119 172:120 173:120 174:121 175:121 176:122 177:122 178:122 179:122 180:123 181:123 182:123 183:124 184:125 185:125 186:125 187:125 188:126 189:126 190:126 191:126 192:127 193:127 194:128 195:129 196:129 197:129 198:129 199:130 200:130 201:131 202:131 203:132 204:133 205:133 206:133 207:134 208:134 209:134 210:134 211:135 212:136 213:137 214:137 215:137 216:138 217:139 218:139 219:139 220:139 221:139 222:139 223:139 224:139 225:140 226:141 227:141 228:141 229:142 230:142 231:143 232:143 233:143 234:144 235:145 236:145 237:145 238:146 239:147 240:147 241:148 242:148 243:148 244:148 245:148 246:149 247:149 248:149 249:149 250:150 251:150 252:150 253:151 254:151 255:151 256:151 257:151 258:152 259:153 260:154 261:155 262:156 263:156 264:156 265:156 266:156 267:157 268:157 269:158 270:158 271:158 272:158 273:159 274:160 275:160 276:160 277:160 278:161 279:161 280:161 281:161 282:161 283:161 284:161\n",
            "I0629 14:40:09.295562 140691672389504 run_squad.py:440] token_is_max_context: 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:False 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True\n",
            "I0629 14:40:09.295863 140691672389504 run_squad.py:442] input_ids: 2 8955 118 2027 199 107 8489 538 306 1687 232 2002 14730 3 107 9566 9057 10868 242 77 3647 6897 9169 4516 14721 319 5633 2232 5849 3923 47 4403 409 1577 11439 3400 232 1370 3412 538 743 157 77 1193 2566 232 1023 12219 9926 242 2118 12593 14168 14721 1539 201 213 14723 4403 16 3432 1110 2644 20 2002 3132 14721 14725 386 1373 118 166 2157 107 88 11439 656 4347 242 14699 3994 1421 11039 661 1539 6011 157 7166 14723 14725 294 313 232 93 14716 141 3181 2892 118 14351 8351 22 1370 8064 4225 9760 6684 44 14638 10868 14721 12790 6767 14721 290 157 501 13732 1965 8262 14286 1580 6983 93 588 2793 117 7849 1014 1421 5062 661 4730 306 14429 13109 501 107 9593 25 1202 5384 10064 894 496 3432 22 1995 14723 14725 513 107 1965 8262 14286 1580 6983 93 1421 8881 661 107 13686 16 2723 2273 125 313 1927 893 11164 143 6228 14473 4403 409 14638 470 6489 661 2723 745 6101 77 496 3432 22 1995 2232 5849 13354 14721 107 22 2272 1927 893 11164 14168 1978 13700 346 1580 1420 88 11439 656 9570 11418 2243 4358 1370 12593 2723 745 6101 107 2232 5849 3923 14723 14725 479 234 344 1370 2795 14498 470 5338 47 13686 16 14721 3682 14335 4640 232 107 1996 4225 3412 538 743 157 1995 14606 13354 14120 47 11258 8646 44 14717 8489 538 306 14718 4479 107 12283 1370 501 1580 10142 13530 44 209 1988 14606 13354 14120 47 1539 496 3432 22 1995 2232 5849 3923 47 14723 14725 513 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.296116 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.296328 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.296456 140691672389504 run_squad.py:451] start_position: 239\n",
            "I0629 14:40:09.296542 140691672389504 run_squad.py:452] end_position: 252\n",
            "I0629 14:40:09.296629 140691672389504 run_squad.py:454] answer: fa ##ir em ##p ##lo ##y ##ment pra ##ct ##ice ##s comm ##itte ##e\n",
            "I0629 14:40:09.300376 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.300876 140691672389504 run_squad.py:432] unique_id: 1000000007\n",
            "I0629 14:40:09.300959 140691672389504 run_squad.py:433] example_index: 6\n",
            "I0629 14:40:09.301034 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.301148 140691672389504 run_squad.py:436] tokens: [CLS] the stru ##ct ##ure ##s th ##at sti ##ll rem ##ain are op ##en to who ##m ? [SEP] be ##ca ##use i ##t was des ##igna ##ted as the national cap ##ita ##l , man ##y stru ##ct ##ure ##s we ##re bu ##il ##t aro ##und th ##at tim ##e . ev ##en to ##day , som ##e of the ##m sti ##ll rem ##ain whi ##ch are op ##en to tour ##ist ##s . [SEP]\n",
            "I0629 14:40:09.301255 140691672389504 run_squad.py:438] token_to_orig_map: 20:0 21:0 22:0 23:1 24:1 25:2 26:3 27:3 28:3 29:4 30:5 31:6 32:7 33:7 34:7 35:7 36:8 37:8 38:9 39:9 40:9 41:9 42:10 43:10 44:11 45:11 46:11 47:12 48:12 49:13 50:13 51:14 52:14 53:14 54:15 55:15 56:16 57:16 58:16 59:17 60:17 61:18 62:19 63:19 64:20 65:20 66:21 67:21 68:22 69:22 70:23 71:24 72:24 73:25 74:26 75:26 76:26 77:26\n",
            "I0629 14:40:09.302150 140691672389504 run_squad.py:440] token_is_max_context: 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True\n",
            "I0629 14:40:09.302354 140691672389504 run_squad.py:442] input_ids: 2 107 8133 13354 2841 47 2723 2273 9103 3439 6698 4573 4438 5805 25 1370 8408 364 14730 3 414 1282 13336 125 118 5721 1834 13474 11212 1710 107 3825 10059 1543 205 14721 1036 157 8133 13354 2841 47 4403 409 1373 683 118 1099 10661 2723 2273 7040 44 14723 6388 25 1370 6240 14721 11272 44 242 107 364 9103 3439 6698 4573 7849 1014 4438 5805 25 1370 8984 10961 47 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.302562 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.302795 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.302911 140691672389504 run_squad.py:451] start_position: 74\n",
            "I0629 14:40:09.302996 140691672389504 run_squad.py:452] end_position: 76\n",
            "I0629 14:40:09.303066 140691672389504 run_squad.py:454] answer: tour ##ist ##s\n",
            "I0629 14:40:09.309104 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.309230 140691672389504 run_squad.py:432] unique_id: 1000000008\n",
            "I0629 14:40:09.309309 140691672389504 run_squad.py:433] example_index: 7\n",
            "I0629 14:40:09.309401 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.309568 140691672389504 run_squad.py:436] tokens: [CLS] w ##hen was the mann ac ##t pass ##ed ? [SEP] the bureau ' s fir ##st of ##fi ##cia ##l tas ##k was vi ##si ##ting and makin ##g sur ##ve ##ys of the house ##s of pro ##sti ##tu ##tion in pre ##par ##ation for en ##for ##ci ##ng the \" white s ##la ##ve tra ##ff ##ic ac ##t , \" or mann ac ##t , pass ##ed on jun ##e 25 , 1910 . in 1932 , i ##t was rena ##med the unit ##ed sta ##tes bureau of in ##ve ##sti ##gat ##ion . the fol ##low ##ing ye ##ar i ##t was link ##ed to the bureau of pro ##hibit ##ion and re ##ch ##ris ##ten ##ed the di ##vis ##ion of in ##ve ##sti ##gat ##ion ( doi ) be ##for ##e final ##ly be ##com ##ing an independ ##ent servi ##ce with ##in the de ##par ##t ##ment of justi ##ce in 1935 . in the sam ##e ye ##ar , i ##ts na ##me was of ##fi ##cia ##lly change ##d from the di ##vis ##ion of in ##ve ##sti ##gat ##ion to the pres ##ent - day federal bureau of in ##ve ##sti ##gat ##ion , or f ##bi . [SEP]\n",
            "I0629 14:40:09.395402 140691672389504 run_squad.py:438] token_to_orig_map: 12:0 13:1 14:1 15:1 16:2 17:2 18:3 19:3 20:3 21:3 22:4 23:4 24:5 25:6 26:6 27:6 28:7 29:8 30:8 31:9 32:9 33:9 34:10 35:11 36:12 37:12 38:13 39:14 40:14 41:14 42:14 43:15 44:16 45:16 46:16 47:17 48:18 49:18 50:18 51:18 52:19 53:20 54:20 55:21 56:21 57:21 58:22 59:22 60:22 61:23 62:23 63:23 64:23 65:24 66:25 67:26 68:26 69:26 70:27 71:27 72:28 73:29 74:29 75:30 76:30 77:31 78:31 79:32 80:33 81:33 82:34 83:34 84:35 85:36 86:36 87:37 88:38 89:38 90:39 91:39 92:40 93:41 94:42 95:42 96:42 97:42 98:42 99:42 100:43 101:44 102:44 103:44 104:45 105:45 106:46 107:46 108:47 109:48 110:48 111:49 112:50 113:51 114:52 115:53 116:53 117:53 118:54 119:55 120:55 121:55 122:55 123:55 124:56 125:57 126:57 127:57 128:58 129:59 130:59 131:59 132:59 133:59 134:60 135:60 136:60 137:61 138:61 139:61 140:62 141:62 142:63 143:63 144:63 145:64 146:65 147:65 148:66 149:66 150:67 151:67 152:68 153:69 154:69 155:69 156:69 157:70 158:71 159:71 160:72 161:73 162:73 163:74 164:75 165:76 166:76 167:77 168:77 169:77 170:78 171:78 172:79 173:79 174:80 175:81 176:81 177:81 178:81 179:82 180:82 181:83 182:84 183:85 184:85 185:85 186:86 187:87 188:87 189:87 190:87 191:87 192:88 193:89 194:90 195:90 196:90 197:90 198:91 199:92 200:93 201:94 202:94 203:94 204:94 205:94 206:94 207:95 208:96 209:96 210:96\n",
            "I0629 14:40:09.395768 140691672389504 run_squad.py:440] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True\n",
            "I0629 14:40:09.396109 140691672389504 run_squad.py:442] input_ids: 2 319 9509 5721 107 6412 14638 118 9972 661 14730 3 107 12285 14716 141 13577 2325 242 2674 8852 205 11101 20 5721 573 1508 9870 1110 10751 303 13592 1580 10387 242 107 4309 47 242 1421 10142 101 10868 501 2330 4286 5595 2002 2795 14498 6479 1988 107 14711 3797 141 337 1580 1342 8710 6553 14638 118 14721 14711 894 6412 14638 118 14721 9972 661 2085 8827 44 356 14721 1090 14723 501 766 14721 125 118 5721 14213 3622 107 5829 661 5757 6466 12285 242 501 1580 10142 13530 1210 14723 107 12852 6462 334 10871 320 125 118 5721 4771 661 1370 107 12285 242 1421 5062 1210 1110 1577 1014 9154 483 661 107 347 5773 1210 242 501 1580 10142 13530 1210 14717 13793 14718 414 14498 44 5720 2587 414 2313 334 1420 1618 6424 5791 470 4479 211 107 79 4286 118 1995 242 4205 470 501 1364 14723 501 107 1782 44 10871 320 14721 125 2245 1170 2638 5721 242 2674 8852 1763 12846 232 13616 107 347 5773 1210 242 501 1580 10142 13530 1210 1370 107 3383 6424 14722 6360 2995 12285 242 501 1580 10142 13530 1210 14721 894 317 1609 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.396408 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.396674 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.396806 140691672389504 run_squad.py:451] start_position: 73\n",
            "I0629 14:40:09.396924 140691672389504 run_squad.py:452] end_position: 77\n",
            "I0629 14:40:09.397024 140691672389504 run_squad.py:454] answer: jun ##e 25 , 1910\n",
            "I0629 14:40:09.401278 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.401439 140691672389504 run_squad.py:432] unique_id: 1000000009\n",
            "I0629 14:40:09.401522 140691672389504 run_squad.py:433] example_index: 8\n",
            "I0629 14:40:09.401595 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.401739 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t is the resu ##lt of / e / be ##ing the sam ##e in central , west ##ern and bel ##eari ##c ? [SEP] central , west ##ern , and bale ##ari ##c diff ##er in the le ##xi ##cal inc ##iden ##ce of stre ##ss ##ed / e / and / [UNK] / . us ##ua ##lly , wor ##ds with / [UNK] / in central cat ##alan corr ##es ##po ##nd to / [UNK] / in bale ##ari ##c and / e / in west ##ern cat ##alan . wor ##ds with / e / in bale ##ari ##c al ##most al ##wa ##ys ha ##ve / e / in central and west ##ern cat ##alan as well . [ va ##gue ] as a resu ##lt , central cat ##alan has a mu ##ch hig ##her inc ##iden ##ce of / e / . [SEP]\n",
            "I0629 14:40:09.401882 140691672389504 run_squad.py:438] token_to_orig_map: 27:0 28:0 29:1 30:1 31:1 32:2 33:3 34:3 35:3 36:4 37:4 38:5 39:6 40:7 41:7 42:7 43:8 44:8 45:8 46:9 47:10 48:10 49:10 50:11 51:11 52:11 53:12 54:13 55:13 56:13 57:13 58:14 59:14 60:14 61:14 62:15 63:15 64:16 65:17 66:17 67:17 68:18 69:19 70:20 71:20 72:21 73:21 74:21 75:21 76:22 77:23 78:23 79:23 80:24 81:25 82:25 83:25 84:26 85:27 86:27 87:27 88:28 89:29 90:29 91:30 92:30 93:30 94:31 95:31 96:32 97:33 98:33 99:33 100:34 101:35 102:35 103:35 104:36 105:36 106:37 107:37 108:37 109:38 110:38 111:39 112:39 113:39 114:40 115:41 116:42 117:43 118:43 119:44 120:44 121:45 122:46 123:46 124:46 125:46 126:46 127:46 128:47 129:48 130:49 131:49 132:49 133:50 134:51 135:51 136:52 137:53 138:54 139:54 140:55 141:55 142:56 143:56 144:56 145:57 146:58 147:58 148:58 149:58\n",
            "I0629 14:40:09.402011 140691672389504 run_squad.py:440] token_is_max_context: 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True\n",
            "I0629 14:40:09.402222 140691672389504 run_squad.py:442] input_ids: 2 8955 118 4535 107 11370 5550 242 14724 88 14724 414 334 107 1782 44 501 7124 14721 5151 4605 1110 5981 14338 306 14730 3 7124 14721 5151 4605 14721 1110 6038 153 306 4648 143 501 107 506 4173 13962 11393 8965 470 242 9557 3547 661 14724 88 14724 1110 14724 1 14724 14723 2743 1327 1763 14721 9559 14473 4479 14724 1 14724 501 7124 4506 13245 10561 199 1971 1336 1370 14724 1 14724 501 6038 153 306 1110 14724 88 14724 501 5151 4605 4506 13245 14723 9559 14473 4479 14724 88 14724 501 6038 153 306 166 11111 166 2569 10387 346 1580 14724 88 14724 501 7124 1110 5151 4605 4506 13245 1710 9523 14723 14732 4565 2409 14734 1710 77 11370 5550 14721 7124 4506 13245 4337 77 2357 1014 6617 1797 11393 8965 470 242 14724 88 14724 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.402399 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.402559 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.402631 140691672389504 run_squad.py:451] start_position: 140\n",
            "I0629 14:40:09.402698 140691672389504 run_squad.py:452] end_position: 144\n",
            "I0629 14:40:09.402770 140691672389504 run_squad.py:454] answer: hig ##her inc ##iden ##ce\n",
            "I0629 14:40:09.408075 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.408209 140691672389504 run_squad.py:432] unique_id: 1000000010\n",
            "I0629 14:40:09.408291 140691672389504 run_squad.py:433] example_index: 9\n",
            "I0629 14:40:09.408364 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.408565 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t di ##d the old let ##ter [UNK] [UNK] [UNK] be ##com ##e ? [SEP] old ##er let ##ter ##s of the ru ##ssi ##an alph ##abe ##t inc ##lu ##de [UNK] [UNK] [UNK] , whi ##ch mer ##ge ##d to [UNK] [UNK] [UNK] ( / je / or / [UNK] / ) ; [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] , whi ##ch bot ##h mer ##ge ##d to [UNK] [UNK] [UNK] ( / i / ) ; [UNK] [UNK] [UNK] , whi ##ch mer ##ge ##d to [UNK] [UNK] [UNK] ( / f / ) ; [UNK] [UNK] [UNK] , whi ##ch mer ##ge ##d to [UNK] [UNK] [UNK] ( / u / ) ; [UNK] [UNK] [UNK] , whi ##ch mer ##ge ##d to [UNK] [UNK] [UNK] ( / ju / or / [UNK] / ) ; and [UNK] [UNK] / [UNK] [UNK] [UNK] [UNK] , whi ##ch lat ##er we ##re grap ##hica ##lly res ##ha ##ped int ##o [UNK] [UNK] [UNK] and mer ##ge ##d p ##hon ##et ##ica ##lly to / ja / or / [UNK] / . whi ##le the ##se old ##er let ##ter ##s ha ##ve be ##en aba ##ndo ##ne ##d at one tim ##e or ano ##ther , the ##y may be us ##ed in thi ##s and re ##lat ##ed article ##s . the ye ##rs [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] or ##ig ##ina ##lly indi ##ca ##ted the pro ##n ##un ##cia ##tion of ultra - sh ##or ##t or red ##uc ##ed / u / , / i / . [SEP]\n",
            "I0629 14:40:09.408748 140691672389504 run_squad.py:438] token_to_orig_map: 17:0 18:0 19:1 20:1 21:1 22:2 23:3 24:4 25:4 26:4 27:5 28:5 29:5 30:6 31:6 32:6 33:7 34:7 35:7 36:7 37:8 38:8 39:9 40:9 41:9 42:10 43:11 44:11 45:11 46:12 47:12 48:12 49:12 50:13 51:14 52:14 53:14 54:14 55:14 56:15 57:15 58:15 59:16 60:17 61:17 62:17 63:17 64:18 65:18 66:19 67:19 68:20 69:20 70:20 71:21 72:22 73:22 74:22 75:23 76:23 77:23 78:23 79:23 80:23 81:24 82:24 83:24 84:24 85:25 86:25 87:26 88:26 89:26 90:27 91:28 92:28 93:28 94:29 95:29 96:29 97:29 98:29 99:29 100:30 101:30 102:30 103:30 104:31 105:31 106:32 107:32 108:32 109:33 110:34 111:34 112:34 113:35 114:35 115:35 116:35 117:35 118:35 119:36 120:36 121:36 122:36 123:37 124:37 125:38 126:38 127:38 128:39 129:40 130:40 131:40 132:41 133:41 134:41 135:41 136:42 137:43 138:43 139:43 140:43 141:43 142:44 143:45 144:45 145:45 146:45 147:45 148:45 149:45 150:45 151:46 152:46 153:47 154:47 155:48 156:48 157:49 158:49 159:49 160:50 161:50 162:50 163:51 164:51 165:52 166:52 167:52 168:53 169:54 170:54 171:54 172:55 173:55 174:55 175:55 176:55 177:56 178:57 179:57 180:57 181:58 182:59 183:59 184:59 185:59 186:60 187:60 188:61 189:61 190:62 191:62 192:63 193:63 194:63 195:64 196:64 197:65 198:65 199:66 200:66 201:66 202:66 203:67 204:68 205:69 206:69 207:70 208:71 209:71 210:71 211:72 212:72 213:73 214:74 215:75 216:75 217:76 218:77 219:77 220:78 221:79 222:79 223:79 224:80 225:80 226:80 227:81 228:82 229:82 230:83 231:83 232:83 233:84 234:85 235:85 236:85 237:86 238:86 239:86 240:86 241:87 242:87 243:87 244:88 245:89 246:89 247:89 248:89 249:89 250:90 251:91 252:91 253:91 254:91 255:91 256:92 257:93 258:93 259:93 260:94 261:94 262:94 263:94 264:95 265:95 266:95 267:95\n",
            "I0629 14:40:09.408940 140691672389504 run_squad.py:440] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True\n",
            "I0629 14:40:09.409160 140691672389504 run_squad.py:442] input_ids: 2 8955 118 347 232 107 9686 9362 1031 1 1 1 414 2313 44 14730 3 9686 143 9362 1031 47 242 107 2633 11154 34 5605 10733 118 11393 11048 609 1 1 1 14721 7849 1014 4296 1616 232 1370 1 1 1 14717 14724 2716 14724 894 14724 1 14724 14718 14726 1 1 1 1110 1 1 1 14721 7849 1014 2843 37 4296 1616 232 1370 1 1 1 14717 14724 125 14724 14718 14726 1 1 1 14721 7849 1014 4296 1616 232 1370 1 1 1 14717 14724 317 14724 14718 14726 1 1 1 14721 7849 1014 4296 1616 232 1370 1 1 1 14717 14724 1484 14724 14718 14726 1 1 1 14721 7849 1014 4296 1616 232 1370 1 1 1 14717 14724 2370 14724 894 14724 1 14724 14718 14726 1110 1 1 14724 1 1 1 1 14721 7849 1014 10074 143 4403 409 13306 8963 1763 8927 1902 12450 3456 39 1 1 1 1110 4296 1616 232 299 14330 722 6142 1763 1370 14724 616 14724 894 14724 1 14724 14723 7849 286 107 1202 9686 143 9362 1031 47 346 1580 414 25 14404 9502 569 232 1003 5560 7040 44 894 9122 9762 14721 107 157 3052 414 2743 661 501 5338 47 1110 1577 10467 661 2209 47 14723 107 10871 4602 1 1 1 1110 1 1 1 894 2097 959 1763 4229 1282 11212 107 1421 22 1055 8852 10868 242 12317 14722 1978 905 118 894 5928 6603 661 14724 1484 14724 14721 14724 125 14724 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.409357 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.409564 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.409640 140691672389504 run_squad.py:451] start_position: 43\n",
            "I0629 14:40:09.409706 140691672389504 run_squad.py:452] end_position: 45\n",
            "I0629 14:40:09.409770 140691672389504 run_squad.py:454] answer: [UNK] [UNK] [UNK]\n",
            "I0629 14:40:09.417278 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.417442 140691672389504 run_squad.py:432] unique_id: 1000000011\n",
            "I0629 14:40:09.417522 140691672389504 run_squad.py:433] example_index: 10\n",
            "I0629 14:40:09.417595 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.417817 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t was the pro ##bl ##em with the pa ##lace ' s chi ##m ##ney ##s ? [SEP] buck ##ing ##ham pa ##lace final ##ly be ##ca ##me the prin ##ci ##pa ##l roy ##al res ##iden ##ce in 1837 , on the ac ##cess ##ion of queen victor ##ia , who was the fir ##st mona ##rc ##h to res ##ide the ##re ; her pre ##de ##cess ##or william iv ha ##d die ##d be ##for ##e i ##ts com ##ple ##tion . whi ##le the sta ##te ro ##om ##s we ##re a ri ##ot of gil ##t and col ##our , the ne ##cess ##it ##ies of the new pa ##lace we ##re som ##ew ##ha ##t les ##s lu ##xurio ##us . for one thi ##ng , i ##t was re ##porte ##d the chi ##m ##ney ##s smoke ##d so mu ##ch th ##at the fire ##s ha ##d to be all ##ow ##ed to die down , and cons ##e ##que ##nt ##ly the cou ##rt shi ##ver ##ed in i ##cy magn ##ifi ##ce ##nce . vent ##ila ##tion was so bad th ##at the inter ##ior s ##mel ##led , and w ##hen a de ##cis ##ion was ta ##ke ##n to ins ##tal ##l gas la ##mps , the ##re was a ser ##ious wor ##ry about the bu ##ild - up of gas on the lowe ##r flo ##ors . i ##t was al ##so sai ##d th ##at sta ##ff we ##re la ##x and la ##zy and the pa ##lace was di ##rt ##y . fol ##low ##ing the queen ' s mar ##ria ##ge in 184 ##0 , her hus ##ban ##d , prin ##ce albert , conce ##r ##ne ##d hi ##m ##se ##lf with a re ##org ##ani ##sa ##tion of the house ##hold of ##fi ##ce ##s and sta ##ff , and with the des ##ig ##n fa ##ult ##s of the pa ##lace . the pro ##bl ##em ##s we ##re all re ##ct ##ified by the cl ##ose of 184 ##0 . how ##ever , the bu ##ild ##ers we ##re to ret ##ur ##n with ##in the de ##ca ##de . [SEP]\n",
            "I0629 14:40:09.500083 140691672389504 run_squad.py:438] token_to_orig_map: 20:0 21:0 22:0 23:1 24:1 25:2 26:2 27:3 28:3 29:3 30:4 31:5 32:5 33:5 34:5 35:6 36:6 37:7 38:7 39:7 40:8 41:9 42:9 43:10 44:11 45:12 46:12 47:12 48:13 49:14 50:15 51:15 52:15 53:16 54:17 55:18 56:19 57:19 58:20 59:20 60:20 61:21 62:22 63:22 64:23 65:23 66:23 67:24 68:25 69:25 70:25 71:25 72:26 73:27 74:28 75:28 76:29 77:29 78:30 79:30 80:30 81:31 82:31 83:32 84:32 85:32 86:32 87:33 88:33 89:34 90:35 91:35 92:36 93:36 94:36 95:37 96:37 97:38 98:39 99:39 100:40 101:41 102:41 103:42 104:43 105:43 106:43 107:44 108:45 109:45 110:45 111:45 112:46 113:47 114:48 115:49 116:49 117:50 118:50 119:51 120:51 121:51 122:51 123:52 124:52 125:53 126:53 127:53 128:53 129:54 130:55 131:56 132:56 133:56 134:57 135:57 136:58 137:59 138:59 139:59 140:60 141:61 142:61 143:61 144:61 145:62 146:62 147:63 148:64 149:64 150:65 151:65 152:66 153:67 154:67 155:68 156:68 157:69 158:70 159:71 160:71 161:71 162:72 163:73 164:74 165:74 166:75 167:76 168:76 169:76 170:76 171:76 172:77 173:78 174:78 175:79 176:79 177:79 178:80 179:81 180:81 181:82 182:82 183:82 184:82 185:82 186:83 187:83 188:83 189:84 190:85 191:86 192:87 193:87 194:88 195:89 196:89 197:90 198:90 199:90 200:90 201:91 202:92 203:92 204:93 205:94 206:94 207:94 208:95 209:96 210:96 211:96 212:97 213:98 214:98 215:98 216:99 217:100 218:100 219:100 220:101 221:101 222:102 223:103 224:104 225:104 226:105 227:105 228:106 229:107 230:108 231:108 232:108 233:108 234:109 235:110 236:111 237:112 238:113 239:113 240:114 241:114 242:114 243:115 244:115 245:116 246:117 247:117 248:118 249:118 250:119 251:119 252:120 253:120 254:121 255:121 256:122 257:122 258:123 259:124 260:124 261:125 262:126 263:127 264:127 265:128 266:129 267:129 268:129 269:129 270:130 271:130 272:130 273:131 274:132 275:132 276:132 277:133 278:133 279:133 280:134 281:135 282:135 283:135 284:136 285:137 286:137 287:137 288:137 289:138 290:138 291:139 292:139 293:140 294:140 295:140 296:140 297:141 298:141 299:141 300:141 301:142 302:143 303:144 304:144 305:144 306:144 307:144 308:145 309:146 310:147 311:147 312:148 313:148 314:148 315:148 316:149 317:150 318:150 319:150 320:151 321:152 322:153 323:154 324:154 325:154 326:155 327:155 328:155 329:156 330:157 331:158 332:158 333:158 334:159 335:160 336:160 337:160 338:160 339:161 340:161 341:162 342:163 343:163 344:163 345:164 346:165 347:166 348:166 349:167 350:168 351:168 352:168 353:169 354:169 355:169 356:170 357:171 358:171 359:171 360:172 361:172 362:173 363:174 364:174 365:174 366:175 367:175 368:176 369:177 370:177 371:177 372:177\n",
            "I0629 14:40:09.500573 140691672389504 run_squad.py:440] token_is_max_context: 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True 361:True 362:True 363:True 364:True 365:True 366:True 367:True 368:True 369:True 370:True 371:True 372:True\n",
            "I0629 14:40:09.500948 140691672389504 run_squad.py:442] input_ids: 2 8955 118 5721 107 1421 10473 1962 4479 107 468 13488 14716 141 6063 364 2683 47 14730 3 2490 334 3465 468 13488 5720 2587 414 1282 2638 107 14143 6479 1276 205 5374 656 8927 8965 470 501 5195 14721 2085 107 14638 13588 1210 242 3858 1908 186 14721 8408 5721 107 13577 2325 6428 13098 37 1370 8927 3074 107 409 14726 1320 2330 609 13588 905 417 842 346 232 6623 232 414 14498 44 125 2245 5656 5496 10868 14723 7849 286 107 5757 425 637 13029 47 4403 409 77 2910 1535 242 2257 118 1110 3173 13166 14721 107 1742 13588 2282 6438 242 107 956 468 13488 4403 409 11272 11381 1902 118 2279 47 568 8696 200 14723 2002 5560 5338 1988 14721 125 118 5721 1577 12094 232 107 6063 364 2683 47 7667 232 1265 2357 1014 2723 2273 107 5366 47 346 232 1370 414 2272 5222 661 1370 6623 5143 14721 1110 8443 44 1393 3470 2587 107 6476 2008 3476 3432 661 501 125 6735 8589 9140 470 4549 14723 10294 10977 10868 5721 1265 6392 2723 2273 107 3508 14307 141 9662 13614 14721 1110 319 9509 77 79 14650 1210 5721 1927 2321 22 1370 9566 10455 205 1092 290 13728 14721 107 409 5721 77 10769 6155 9559 1398 5020 107 1373 10496 14722 13127 242 1092 2085 107 11862 93 10371 14229 14723 125 118 5721 166 2157 2406 232 2723 2273 5757 8710 4403 409 290 893 1110 290 6113 1110 107 468 13488 5721 347 2008 157 14723 12852 6462 334 107 3858 14716 141 575 1247 1616 501 14180 214 14721 1320 5371 10321 232 14721 14143 470 641 14721 11021 93 569 232 4417 364 1202 9504 4479 77 1577 2372 14190 696 10868 242 107 4309 13321 242 2674 470 47 1110 5757 8710 14721 1110 4479 107 1834 2097 22 1996 10356 47 242 107 468 13488 14723 107 1421 10473 1962 47 4403 409 2272 1577 13354 7846 1539 107 2618 13077 242 14180 214 14723 12790 6767 14721 107 1373 10496 14168 4403 409 1370 14073 778 22 4479 211 107 79 1282 609 14723 3 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.501215 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.501478 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.501608 140691672389504 run_squad.py:451] start_position: 140\n",
            "I0629 14:40:09.501700 140691672389504 run_squad.py:452] end_position: 146\n",
            "I0629 14:40:09.501791 140691672389504 run_squad.py:454] answer: the chi ##m ##ney ##s smoke ##d\n",
            "I0629 14:40:09.509819 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.509964 140691672389504 run_squad.py:432] unique_id: 1000000012\n",
            "I0629 14:40:09.510045 140691672389504 run_squad.py:433] example_index: 11\n",
            "I0629 14:40:09.510116 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.510341 140691672389504 run_squad.py:436] tokens: [CLS] who has note ##d th ##at bin ##din ##g pre ##ce ##den ##t di ##d not ex ##ist w ##hen the cons ##titu ##tion was w ##rit ##ten ? [SEP] as federal jud ##ge alex ko ##zin ##ski has po ##in ##ted o ##ut , bin ##din ##g pre ##ce ##den ##t as we kno ##w i ##t to ##day sim ##p ##ly di ##d not ex ##ist at the tim ##e the cons ##titu ##tion was fra ##med . jud ##ic ##ia ##l de ##cis ##ion ##s we ##re not cons ##isten ##t ##ly , ac ##cur ##ate ##ly , and fa ##it ##h ##ful ##ly re ##porte ##d on bot ##h sid ##es of the atlanti ##c ( re ##porte ##rs of ##ten sim ##p ##ly re ##w ##ro ##te or fa ##ile ##d to publi ##sh de ##cis ##ion ##s whi ##ch the ##y dis ##lik ##ed ) , and the unit ##ed king ##do ##m la ##ck ##ed a co ##here ##nt cou ##rt hi ##era ##rc ##h ##y prio ##r to the end of the 19 ##th c ##ent ##ur ##y . fu ##rt ##her ##more , engl ##is ##h jud ##ges in the e ##ight ##een ##th c ##ent ##ur ##y sub ##sc ##ri ##be ##d to no ##w - ob ##sol ##et ##e natural law theo ##rie ##s of law , by whi ##ch law was beli ##eved to ha ##ve an ex ##isten ##ce independ ##ent of wha ##t indi ##vid ##ua ##l jud ##ges sai ##d . jud ##ges sa ##w the ##m ##se ##l ##ve ##s as mer ##ely de ##clar ##ing the law whi ##ch ha ##d al ##wa ##ys theo ##ret ##ica ##lly ex ##iste ##d , and not as makin ##g the law . the ##re ##for ##e , a jud ##ge coul ##d re ##ject ano ##ther jud ##ge ' s op ##ini ##on as sim ##p ##ly an inc ##orr ##ec ##t sta ##tem ##ent of the law , in the way th ##at sci ##ent ##ist ##s re ##gula ##r ##ly re ##ject ea ##ch other ' s conc ##lus ##ion ##s as inc ##orr ##ec ##t sta ##tem ##ents of the law ##s of sci ##ence . [SEP]\n",
            "I0629 14:40:09.510559 140691672389504 run_squad.py:438] token_to_orig_map: 31:0 32:1 33:2 34:2 35:3 36:4 37:4 38:4 39:5 40:6 41:6 42:6 43:7 44:7 45:7 46:8 47:8 48:8 49:9 50:9 51:9 52:9 53:10 54:11 55:12 56:12 57:13 58:13 59:14 60:14 61:15 62:15 63:15 64:16 65:16 66:17 67:18 68:18 69:19 70:20 71:21 72:21 73:22 74:23 75:23 76:23 77:24 78:25 79:25 80:25 81:26 82:26 83:26 84:26 85:27 86:27 87:27 88:27 89:28 90:28 91:29 92:30 93:30 94:30 95:30 96:30 97:31 98:31 99:31 100:31 101:31 102:32 103:33 104:33 105:33 106:33 107:33 108:34 109:34 110:34 111:35 112:36 113:36 114:37 115:37 116:38 117:39 118:40 119:40 120:41 121:41 122:41 123:41 124:42 125:42 126:43 127:43 128:43 129:44 130:44 131:44 132:44 133:45 134:46 135:46 136:46 137:47 138:48 139:48 140:49 141:49 142:49 143:49 144:50 145:50 146:51 147:51 148:52 149:52 150:52 151:52 152:52 153:53 154:54 155:55 156:55 157:56 158:56 159:56 160:57 161:57 162:57 163:58 164:59 165:59 166:59 167:60 168:60 169:61 170:61 171:61 172:61 173:61 174:62 175:62 176:63 177:64 178:65 179:66 180:67 181:68 182:68 183:69 184:69 185:69 186:69 187:69 188:70 189:70 190:70 191:70 192:70 193:71 194:71 195:71 196:72 197:72 198:73 199:74 200:75 201:75 202:75 203:75 204:76 205:76 206:76 207:76 208:77 209:77 210:77 211:77 212:77 213:78 214:79 215:79 216:79 217:79 218:79 219:79 220:79 221:80 222:81 223:82 224:82 225:82 226:83 227:84 228:84 229:85 230:86 231:86 232:87 233:88 234:89 235:89 236:90 237:91 238:91 239:92 240:93 241:93 242:93 243:94 244:94 245:95 246:96 247:96 248:97 249:97 250:97 251:97 252:98 253:98 254:99 255:99 256:99 257:100 258:100 259:101 260:101 261:102 262:102 263:102 264:102 265:102 266:102 267:103 268:104 269:104 270:105 271:105 272:105 273:106 274:107 275:108 276:108 277:109 278:109 279:110 280:110 281:110 282:111 283:111 284:111 285:111 286:112 287:112 288:112 289:112 290:113 291:114 292:115 293:116 294:116 295:117 296:118 297:118 298:119 299:119 300:119 301:119 302:119 303:120 304:121 305:121 306:122 307:122 308:123 309:123 310:124 311:124 312:125 313:125 314:125 315:125 316:126 317:126 318:126 319:127 320:128 321:128 322:128 323:129 324:130 325:130 326:130 327:130 328:131 329:131 330:131 331:132 332:133 333:134 334:134 335:135 336:136 337:137 338:138 339:138 340:139 341:139 342:139 343:139 344:140 345:140 346:140 347:140 348:141 349:141 350:142 351:142 352:143 353:143 354:143 355:144 356:144 357:144 358:144 359:145 360:146 361:146 362:146 363:146 364:147 365:147 366:147 367:148 368:149 369:150 370:150 371:151 372:152 373:152 374:152\n",
            "I0629 14:40:09.510764 140691672389504 run_squad.py:440] token_is_max_context: 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True 361:True 362:True 363:True 364:True 365:True 366:True 367:True 368:True 369:True 370:True 371:True 372:True 373:True 374:True\n",
            "I0629 14:40:09.600628 140691672389504 run_squad.py:442] input_ids: 2 8408 4337 13383 232 2723 2273 6380 6179 303 2330 470 4317 118 347 232 11050 1965 10961 319 9509 107 8443 9057 10868 5721 319 13316 483 14730 3 1710 2995 12886 1616 4642 1436 10215 6941 4337 649 211 11212 228 2285 14721 6380 6179 303 2330 470 4317 118 1710 4403 14189 882 125 118 1370 6240 7025 538 2587 347 232 11050 1965 10961 1003 107 7040 44 107 8443 9057 10868 5721 2040 3622 14723 12886 6553 186 205 79 14650 1210 47 4403 409 11050 8443 11463 118 2587 14721 14638 12387 4640 2587 14721 1110 1996 2282 37 11742 2587 1577 12094 232 2085 2843 37 13050 199 242 107 12032 306 14717 1577 12094 4602 242 483 7025 538 2587 1577 882 745 425 894 1996 4780 232 1370 3625 1754 79 14650 1210 47 7849 1014 107 157 4730 6363 661 14718 14721 1110 107 5829 661 2171 733 364 290 1734 661 77 1010 5633 3470 6476 2008 4417 148 13098 37 157 9014 93 1370 107 9479 242 107 453 2255 213 6424 778 157 14723 4285 2008 1797 4739 14721 6490 278 37 12886 2892 501 107 88 9191 10280 2255 213 6424 778 157 6218 6715 83 2029 232 1370 738 882 14722 5841 14104 722 44 2664 10554 14232 9787 47 242 10554 14721 1539 7849 1014 10554 5721 13362 12681 1370 346 1580 1420 1965 11463 470 1618 6424 242 8955 118 4229 10434 1327 205 12886 2892 2406 232 14723 12886 2892 457 882 107 364 1202 205 1580 47 1710 4296 14479 79 12435 334 107 10554 7849 1014 346 232 166 2569 10387 14232 2576 6142 1763 1965 3616 232 14721 1110 11050 1710 10751 303 107 10554 14723 107 409 14498 44 14721 77 12886 1616 9455 232 1577 9422 9122 9762 12886 1616 14716 141 5805 4354 184 1710 7025 538 2587 1420 11393 14076 8262 118 5757 10719 6424 242 107 10554 14721 501 107 12460 2723 2273 8948 6424 10961 47 1577 9441 93 2587 1577 9422 10591 1014 5150 14716 141 4123 12651 1210 47 1710 11393 14076 8262 118 5757 10719 10366 242 107 10554 47 242 8948 6229 14723 3 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.601201 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.601662 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.601914 140691672389504 run_squad.py:451] start_position: 32\n",
            "I0629 14:40:09.602031 140691672389504 run_squad.py:452] end_position: 38\n",
            "I0629 14:40:09.602256 140691672389504 run_squad.py:454] answer: federal jud ##ge alex ko ##zin ##ski\n",
            "I0629 14:40:09.607782 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.607910 140691672389504 run_squad.py:432] unique_id: 1000000013\n",
            "I0629 14:40:09.607994 140691672389504 run_squad.py:433] example_index: 12\n",
            "I0629 14:40:09.608066 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.608241 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t sh ##or ##t ##com ##ing was not ##ice ##able , from the star ##t , for we ##s ##ley clar ##k ? [SEP] in se ##pt ##ember 2003 , ret ##ire ##d four - star general we ##s ##ley clar ##k an ##no ##un ##ce ##d his int ##ent ##ion to ru ##n in the president ##ia ##l pri ##mary ele ##ction for the de ##mo ##c ##rati ##c part ##y no ##min ##ation . his ca ##mp ##ai ##g ##n fo ##cuse ##d on the ##me ##s of le ##ader ##shi ##p and patr ##io ##tis ##m ; ea ##r ##ly ca ##mp ##ai ##g ##n ad ##s re ##lie ##d he ##avi ##ly on bio ##gr ##a ##ph ##y . his lat ##e star ##t le ##ft hi ##m with re ##lat ##iv ##ely fe ##w de ##ta ##ile ##d poli ##cy pro ##pos ##als . thi ##s we ##ak ##ness was ap ##par ##ent in his fir ##st fe ##w deb ##ate ##s , alt ##hou ##gh he so ##on pres ##ente ##d a ran ##ge of po ##si ##tion paper ##s , inc ##lu ##din ##g a ma ##jor ta ##x - re ##lie ##f plan . nev ##ert ##hel ##ess , the de ##mo ##c ##rat ##s di ##d not flo ##ck to sup ##port his ca ##mp ##ai ##g ##n . [SEP]\n",
            "I0629 14:40:09.608429 140691672389504 run_squad.py:438] token_to_orig_map: 26:0 27:1 28:1 29:1 30:2 31:2 32:3 33:3 34:3 35:4 36:4 37:4 38:5 39:6 40:6 41:6 42:7 43:7 44:8 45:8 46:8 47:8 48:8 49:9 50:10 51:10 52:10 53:11 54:12 55:12 56:13 57:14 58:15 59:15 60:15 61:16 62:16 63:17 64:17 65:18 66:19 67:20 68:20 69:20 70:20 71:20 72:21 73:21 74:22 75:22 76:22 77:22 78:23 79:24 80:24 81:24 82:24 83:24 84:25 85:25 86:25 87:26 88:27 89:27 90:27 91:28 92:29 93:29 94:29 95:29 96:30 97:31 98:31 99:31 100:31 101:31 102:32 103:32 104:32 105:33 106:33 107:33 108:33 109:33 110:34 111:34 112:35 113:35 114:35 115:36 116:36 117:36 118:37 119:38 120:38 121:38 122:38 123:38 124:38 125:39 126:40 127:40 128:41 129:41 130:42 131:42 132:43 133:43 134:44 135:45 136:45 137:45 138:45 139:46 140:46 141:47 142:47 143:47 144:47 145:48 146:48 147:49 148:49 149:49 150:49 151:50 152:50 153:51 154:51 155:51 156:52 157:53 158:53 159:53 160:54 161:55 162:56 163:56 164:57 165:57 166:58 167:58 168:58 169:58 170:59 171:59 172:59 173:60 174:61 175:61 176:62 177:62 178:62 179:63 180:64 181:64 182:65 183:66 184:66 185:66 186:67 187:67 188:67 189:68 190:68 191:68 192:68 193:69 194:70 195:70 196:71 197:71 198:71 199:71 200:71 201:71 202:72 203:72 204:73 205:73 206:73 207:73 208:73 209:74 210:75 211:75 212:75 213:75 214:75 215:76 216:76 217:77 218:78 219:78 220:79 221:80 222:80 223:81 224:82 225:82 226:82 227:82 228:82 229:82\n",
            "I0629 14:40:09.608600 140691672389504 run_squad.py:440] token_is_max_context: 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True\n",
            "I0629 14:40:09.608817 140691672389504 run_squad.py:442] input_ids: 2 8955 118 1978 905 118 2313 334 5721 11050 14120 8814 14721 13616 107 5643 118 14721 2002 4403 47 772 2644 20 14730 3 501 112 6489 12768 1048 14721 14073 3400 232 8013 14722 5643 8342 4403 47 772 2644 20 1420 539 1055 470 232 13732 3456 6424 1210 1370 2633 22 501 107 12122 186 205 10034 5494 6152 10748 2002 107 79 1015 306 14123 306 4453 157 738 5860 5595 14723 13732 1150 10042 1605 303 22 2817 11523 232 2085 107 2638 47 242 506 10775 5420 538 1110 2599 526 4695 364 14726 10591 93 2587 1150 10042 1605 303 22 1756 47 1577 14454 232 617 4720 2587 2085 6202 10067 16 1184 157 14723 13732 10074 44 5643 118 506 12888 4417 364 4479 1577 10467 6684 14479 8489 882 79 105 4780 232 10912 6735 1421 11039 13641 14723 5338 47 4403 23 11200 5721 11282 4286 6424 501 13732 13577 2325 8489 882 4260 4640 47 14721 5961 10150 10220 617 1265 184 3383 2086 232 77 5921 1616 242 649 1508 10868 3922 47 14721 11393 11048 6179 303 77 600 11409 1927 893 14722 1577 14454 313 4121 14723 14666 4272 6376 6464 14721 107 79 1015 306 11502 47 347 232 11050 10371 1734 1370 12818 11420 13732 1150 10042 1605 303 22 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.608977 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.609131 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.609201 140691672389504 run_squad.py:451] start_position: 139\n",
            "I0629 14:40:09.609266 140691672389504 run_squad.py:452] end_position: 149\n",
            "I0629 14:40:09.609337 140691672389504 run_squad.py:454] answer: fe ##w de ##ta ##ile ##d poli ##cy pro ##pos ##als\n",
            "I0629 14:40:09.614747 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.614871 140691672389504 run_squad.py:432] unique_id: 1000000014\n",
            "I0629 14:40:09.614953 140691672389504 run_squad.py:433] example_index: 13\n",
            "I0629 14:40:09.615023 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.615192 140691672389504 run_squad.py:436] tokens: [CLS] the di ##ges ##ti ##ve cha ##mb ##ers of the c ##ten ##op ##hor ##a and the c ##ni ##daria serv ##e as wha ##t ? [SEP] am ##ong the other p ##h ##yl ##a , the c ##ten ##op ##hor ##a and the c ##ni ##daria , whi ##ch inc ##lu ##des sea an ##em ##one ##s , cor ##als , and jel ##ly ##fi ##sh , are rad ##ia ##lly sy ##mme ##tric and ha ##ve di ##ges ##ti ##ve cha ##mb ##ers with a sing ##le op ##en ##ing , whi ##ch serv ##es as bot ##h the mo ##ut ##h and the an ##us . bot ##h ha ##ve dis ##tin ##ct ti ##ss ##ue ##s , bu ##t the ##y are not organiz ##ed int ##o organ ##s . the ##re are on ##ly t ##w ##o mai ##n ger ##m la ##yer ##s , the ec ##to ##der ##m and end ##od ##er ##m , with on ##ly sca ##tter ##ed cell ##s bet ##we ##en the ##m . as su ##ch , the ##se anima ##l ##s are som ##et ##im ##es call ##ed di ##p ##lo ##bla ##sti ##c . the tin ##y pla ##co ##zoa ##ns are sim ##ila ##r , bu ##t the ##y do not ha ##ve a per ##mane ##nt di ##ges ##ti ##ve cha ##mb ##er . [SEP]\n",
            "I0629 14:40:09.615367 140691672389504 run_squad.py:438] token_to_orig_map: 28:0 29:0 30:1 31:2 32:3 33:3 34:3 35:3 36:3 37:4 38:5 39:5 40:5 41:5 42:5 43:6 44:7 45:8 46:8 47:8 48:8 49:9 50:9 51:10 52:10 53:10 54:11 55:12 56:12 57:12 58:12 59:12 60:13 61:13 62:13 63:14 64:15 65:15 66:15 67:15 68:15 69:16 70:17 71:17 72:17 73:18 74:18 75:18 76:19 77:20 78:20 79:21 80:21 81:21 82:21 83:22 84:22 85:22 86:23 87:24 88:25 89:25 90:26 91:26 92:26 93:26 94:27 95:27 96:28 97:28 98:29 99:30 100:30 101:31 102:32 103:32 104:32 105:33 106:34 107:35 108:35 109:35 110:36 111:36 112:37 113:37 114:38 115:38 116:38 117:39 118:39 119:39 120:39 121:39 122:40 123:40 124:41 125:41 126:42 127:43 128:44 129:44 130:45 131:45 132:46 133:46 134:46 135:47 136:47 137:48 138:49 139:49 140:50 141:50 142:50 143:51 144:51 145:52 146:52 147:53 148:53 149:53 150:53 151:54 152:55 153:55 154:55 155:55 156:56 157:57 158:57 159:57 160:57 161:57 162:58 163:59 164:59 165:60 166:60 167:60 168:61 169:61 170:62 171:62 172:62 173:63 174:63 175:63 176:64 177:65 178:65 179:65 180:66 181:66 182:67 183:67 184:67 185:68 186:69 187:69 188:69 189:69 190:70 191:70 192:71 193:71 194:71 195:71 196:71 197:71 198:71 199:72 200:73 201:73 202:74 203:74 204:74 205:74 206:75 207:76 208:76 209:76 210:76 211:77 212:77 213:78 214:78 215:79 216:80 217:81 218:81 219:82 220:83 221:83 222:83 223:84 224:84 225:84 226:84 227:85 228:85 229:85 230:85\n",
            "I0629 14:40:09.615530 140691672389504 run_squad.py:440] token_is_max_context: 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True\n",
            "I0629 14:40:09.615706 140691672389504 run_squad.py:442] input_ids: 2 107 347 2892 753 1580 2559 14051 14168 242 107 213 483 2889 8170 16 1110 107 213 2243 6537 11016 44 1710 8955 118 14730 3 1566 14004 107 5150 299 37 12799 16 14721 107 213 483 2889 8170 16 1110 107 213 2243 6537 14721 7849 1014 11393 11048 3519 6766 1420 1962 6484 47 14721 4423 13641 14721 1110 10301 2587 2674 1754 14721 4438 12943 186 1763 9169 10988 13719 1110 346 1580 347 2892 753 1580 2559 14051 14168 4479 77 13426 286 5805 25 334 14721 7849 1014 11016 199 1710 2843 37 107 688 2285 37 1110 107 1420 200 14723 2843 37 346 1580 4730 10612 13354 2089 3547 2616 47 14721 1373 118 107 157 4438 11050 8418 661 3456 39 10158 47 14723 107 409 4438 2085 2587 365 882 39 9798 22 732 364 290 13297 47 14721 107 9320 387 9123 364 1110 9479 11345 143 364 14721 4479 2085 2587 10450 14426 661 8155 47 3401 10705 25 107 364 14723 1710 402 1014 14721 107 1202 13112 205 47 4438 11272 722 2702 199 8771 661 347 538 743 4496 10142 306 14723 107 14033 157 5709 776 14185 2827 4438 7025 10977 93 14721 1373 118 107 157 2027 11050 346 1580 77 1023 13681 3470 347 2892 753 1580 2559 14051 143 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.615898 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.616091 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.616165 140691672389504 run_squad.py:451] start_position: 99\n",
            "I0629 14:40:09.616229 140691672389504 run_squad.py:452] end_position: 108\n",
            "I0629 14:40:09.616293 140691672389504 run_squad.py:454] answer: bot ##h the mo ##ut ##h and the an ##us\n",
            "I0629 14:40:09.619283 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.619424 140691672389504 run_squad.py:432] unique_id: 1000000015\n",
            "I0629 14:40:09.619504 140691672389504 run_squad.py:433] example_index: 14\n",
            "I0629 14:40:09.619573 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.619697 140691672389504 run_squad.py:436] tokens: [CLS] w ##hen di ##d bud ##d ##hi ##s ##m le ##a ##ve z ##he ##ji ##ang ? [SEP] in mid - 20 ##15 the go ##ver ##n ##ment of z ##he ##ji ##ang re ##co ##g ##nis ##ed folk re ##lig ##ion as \" c ##iv ##il re ##lig ##ion \" begi ##nni ##ng the re ##gis ##tra ##tion of more th ##an t ##wen ##ty tho ##us ##and folk re ##lig ##ious as ##socia ##tions . bud ##d ##hi ##s ##m has an im ##port ##ant pres ##ence sin ##ce i ##ts arri ##val in z ##he ##ji ##ang 1 , 8 ##00 ye ##ar ##s ag ##o . [SEP]\n",
            "I0629 14:40:09.619814 140691672389504 run_squad.py:438] token_to_orig_map: 19:0 20:1 21:1 22:1 23:1 24:2 25:3 26:3 27:3 28:3 29:4 30:5 31:5 32:5 33:5 34:6 35:6 36:6 37:6 38:6 39:7 40:8 41:8 42:8 43:9 44:10 45:10 46:10 47:10 48:11 49:11 50:11 51:11 52:12 53:12 54:12 55:13 56:14 57:14 58:14 59:14 60:15 61:16 62:17 63:17 64:18 65:18 66:18 67:19 68:19 69:19 70:20 71:21 72:21 73:21 74:22 75:22 76:22 77:22 78:23 79:23 80:23 81:23 82:23 83:24 84:25 85:26 86:26 87:26 88:27 89:27 90:28 91:28 92:29 93:29 94:30 95:30 96:31 97:32 98:32 99:32 100:32 101:33 102:33 103:33 104:33 105:34 106:34 107:34 108:35 109:35 110:35\n",
            "I0629 14:40:09.705862 140691672389504 run_squad.py:440] token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True\n",
            "I0629 14:40:09.706263 140691672389504 run_squad.py:442] input_ids: 2 319 9509 347 232 10599 232 3445 47 364 506 16 1580 521 4129 6630 9131 14730 3 501 13432 14722 466 1799 107 496 3432 22 1995 242 521 4129 6630 9131 1577 776 303 3477 661 8022 1577 11097 1210 1710 14711 213 6684 683 1577 11097 1210 14711 2925 14612 1988 107 1577 13403 5849 10868 242 6470 2723 34 365 4630 4358 2328 200 2047 8022 1577 11097 6155 1710 12707 4699 14723 10599 232 3445 47 364 4337 1420 3376 11420 2800 3383 6229 1275 470 125 2245 11587 13084 501 521 4129 6630 9131 218 14721 588 1201 10871 320 47 3255 39 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.706532 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.706744 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.710296 140691672389504 run_squad.py:448] impossible example\n",
            "I0629 14:40:09.718778 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.718970 140691672389504 run_squad.py:432] unique_id: 1000000016\n",
            "I0629 14:40:09.719053 140691672389504 run_squad.py:433] example_index: 15\n",
            "I0629 14:40:09.719124 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.719355 140691672389504 run_squad.py:436] tokens: [CLS] who em ##b ##ra ##ce ##d his fa ##mi ##ly ac ##co ##rd ##ing to the new test ##amen ##t ? [SEP] the ha ##gi ##og ##ra ##ph ##y of mar ##y and the hol ##y fa ##mi ##ly can be con ##tra ##ste ##d with other material in the go ##spe ##l ##s . the ##se re ##ferences inc ##lu ##de an inc ##iden ##t whi ##ch can be inter ##pr ##et ##ed as jesus re ##ject ##ing his fa ##mi ##ly in the new test ##amen ##t : \" and his mot ##her and his brother ##s arri ##ve ##d , and stan ##din ##g o ##utsi ##de , the ##y sent in a mess ##age aski ##ng for hi ##m . . . and lo ##ok ##ing at tho ##se who sat in a c ##ir ##cle aro ##und hi ##m , jesus sai ##d , ' the ##se are my mot ##her and my brother ##s . who ##ever do ##es the will of god is my brother , and sist ##er , and mot ##her ' . \" [ 3 : 31 - 35 ] other vers ##es su ##gg ##est a con ##fl ##ic ##t bet ##we ##en jesus and his fa ##mi ##ly , inc ##lu ##din ##g an at ##tem ##pt to ha ##ve jesus res ##tra ##ine ##d be ##ca ##use \" he is o ##ut of his min ##d \" , and the fa ##mo ##us qu ##ote : \" a pro ##ph ##et is not with ##out hon ##or ex ##ce ##pt in his o ##w ##n to ##w ##n , am ##ong his re ##lat ##iv ##es and in his o ##w ##n ho ##me . \" a le ##ading bibli ##cal scho ##lar comment ##ed : \" the ##re are cle ##ar sig ##ns not on ##ly th ##at jesus ' s fa ##mi ##ly re ##ject ##ed his mess ##age du ##ring his publi ##c mini ##st ##ry bu ##t th ##at he in tur ##n spu ##r ##ne ##d the ##m publi ##cl ##y \" . [SEP]\n",
            "I0629 14:40:09.719568 140691672389504 run_squad.py:438] token_to_orig_map: 23:0 24:1 25:1 26:1 27:1 28:1 29:1 30:2 31:3 32:3 33:4 34:5 35:6 36:6 37:7 38:7 39:7 40:8 41:9 42:10 43:10 44:10 45:10 46:11 47:12 48:13 49:14 50:15 51:16 52:16 53:16 54:16 55:16 56:17 57:17 58:18 59:18 60:19 61:19 62:19 63:20 64:21 65:21 66:21 67:22 68:22 69:23 70:24 71:25 72:25 73:25 74:25 75:26 76:27 77:28 78:28 79:28 80:29 81:30 82:30 83:30 84:31 85:32 86:33 87:34 88:34 89:34 90:34 91:35 92:35 93:36 94:37 95:37 96:38 97:39 98:40 99:40 100:41 101:41 102:41 103:41 104:42 105:43 106:43 107:43 108:44 109:44 110:44 111:44 112:45 113:45 114:46 115:47 116:48 117:49 118:49 119:50 120:50 121:51 122:52 123:52 124:53 125:53 126:53 127:54 128:55 129:55 130:55 131:56 132:57 133:57 134:58 135:59 136:60 137:61 138:62 139:62 140:62 141:63 142:63 143:64 144:64 145:64 146:65 147:66 148:66 149:66 150:67 151:67 152:67 153:68 154:69 155:70 156:70 157:71 158:72 159:73 160:73 161:73 162:74 163:74 164:75 165:75 166:76 167:77 168:78 169:79 170:80 171:81 172:82 173:82 174:83 175:84 176:84 177:84 178:85 179:86 180:86 181:86 182:86 183:86 184:86 185:86 186:86 187:86 188:86 189:86 190:86 191:87 192:88 193:88 194:89 195:89 196:89 197:90 198:91 199:91 200:91 201:91 202:92 203:92 204:92 205:93 206:94 207:95 208:96 209:96 210:96 211:96 212:97 213:97 214:97 215:97 216:98 217:99 218:99 219:99 220:100 221:101 222:101 223:102 224:103 225:103 226:103 227:103 228:104 229:104 230:104 231:105 232:105 233:106 234:107 235:107 236:108 237:109 238:110 239:110 240:110 241:110 242:111 243:112 244:113 245:113 246:113 247:114 248:114 249:114 250:115 251:115 252:116 253:116 254:116 255:117 256:118 257:119 258:119 259:120 260:120 261:121 262:121 263:121 264:122 265:123 266:124 267:124 268:124 269:125 270:125 271:125 272:125 273:126 274:126 275:127 276:128 277:128 278:128 279:128 280:129 281:130 282:131 283:132 284:132 285:132 286:133 287:133 288:133 289:133 290:134 291:135 292:135 293:136 294:136 295:137 296:137 297:138 298:138 299:138 300:139 301:139 302:139 303:140 304:141 305:141 306:142 307:142 308:143 309:144 310:144 311:145 312:145 313:146 314:146 315:146 316:147 317:147 318:147 319:148 320:148 321:148 322:149 323:150 324:150 325:151 326:151 327:152 328:153 329:153 330:154 331:154 332:154 333:155 334:155 335:156 336:156 337:157 338:158 339:159 340:159 341:160 342:160 343:160 344:160 345:161 346:161 347:162 348:162 349:162 350:162 351:162\n",
            "I0629 14:40:09.719761 140691672389504 run_squad.py:440] token_is_max_context: 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True\n",
            "I0629 14:40:09.719963 140691672389504 run_squad.py:442] input_ids: 2 8408 3412 532 75 470 232 13732 1996 4246 2587 14638 776 6280 334 1370 107 956 12562 9431 118 14730 3 107 346 1118 6012 75 1184 157 242 575 157 1110 107 6650 157 1996 4246 2587 4736 414 2232 5849 14036 232 4479 5150 2046 501 107 496 6332 205 47 14723 107 1202 1577 7753 11393 11048 609 1420 11393 8965 118 7849 1014 4736 414 3508 11658 722 661 1710 805 1577 9422 334 13732 1996 4246 2587 501 107 956 12562 9431 118 14725 14711 1110 13732 4522 1797 1110 13732 7968 47 11587 1580 232 14721 1110 1687 6179 303 228 14534 609 14721 107 157 2316 501 77 13399 9926 5485 1988 2002 4417 364 14723 14723 14723 1110 1578 5861 334 1003 2328 1202 8408 10803 501 77 213 4225 12082 1099 10661 4417 364 14721 805 2406 232 14721 14716 107 1202 4438 2028 4522 1797 1110 2028 7968 47 14723 8408 6767 2027 199 107 3188 242 5608 4535 2028 7968 14721 1110 12655 143 14721 1110 4522 1797 14716 14723 14711 14732 187 14725 481 14722 758 14734 5150 5655 199 402 13270 6006 77 2232 11399 6553 118 3401 10705 25 805 1110 13732 1996 4246 2587 14721 11393 11048 6179 303 1420 1003 10719 6489 1370 346 1580 805 8927 5849 1975 232 414 1282 13336 14711 617 4535 228 2285 242 13732 2414 232 14711 14721 1110 107 1996 1015 200 3647 6244 14725 14711 77 1421 1184 722 4535 11050 4479 11204 1084 905 1965 470 6489 501 13732 228 882 22 1370 882 22 14721 1566 14004 13732 1577 10467 6684 199 1110 501 13732 228 882 22 1316 2638 14723 14711 77 506 10511 3536 13962 5566 4659 9138 661 14725 14711 107 409 4438 4856 320 5385 2827 11050 2085 2587 2723 2273 805 14716 141 1996 4246 2587 1577 9422 661 13732 13399 9926 69 11114 13732 3625 306 13757 2325 1398 1373 118 2723 2273 617 501 3559 22 13504 93 569 232 107 364 3625 11872 157 14711 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.720137 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.720313 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.720397 140691672389504 run_squad.py:448] impossible example\n",
            "I0629 14:40:09.726361 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.726525 140691672389504 run_squad.py:432] unique_id: 1000000017\n",
            "I0629 14:40:09.726614 140691672389504 run_squad.py:433] example_index: 16\n",
            "I0629 14:40:09.726686 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.726872 140691672389504 run_squad.py:436] tokens: [CLS] w ##here do ##es a bill go on ##ce the president sig ##ns i ##t int ##o e ##ff ##ec ##t ? [SEP] af ##ter the president sig ##ns a bill int ##o law ( or con ##gre ##ss en ##ac ##ts i ##t o ##ver his ve ##to ) , i ##t is del ##iv ##ere ##d to the of ##fi ##ce of the federal re ##gis ##ter ( of ##r ) of the national archi ##ve ##s and re ##co ##rd ##s ad ##minis ##tra ##tion ( na ##ra ) w ##here i ##t is as ##si ##g ##ne ##d a law nu ##mb ##er , and pre ##par ##ed for publi ##ca ##tion as a s ##li ##p law . publi ##c law ##s , bu ##t not pri ##va ##te law ##s , are al ##so gi ##ven leg ##al sta ##tu ##tory cit ##ation by the of ##r . at the end of ea ##ch ses ##sion of con ##gre ##ss , the s ##li ##p law ##s are com ##pil ##ed int ##o bo ##und vol ##ume ##s call ##ed the unit ##ed sta ##tes sta ##tu ##tes at lar ##ge , and the ##y are kno ##w ##n as ses ##sion law ##s . the sta ##tu ##tes at lar ##ge pres ##ent a chr ##on ##olo ##gi ##cal arra ##ng ##ement of the law ##s in the ex ##ac ##t orde ##r th ##at the ##y ha ##ve be ##en en ##ac ##ted . [SEP]\n",
            "I0629 14:40:09.805978 140691672389504 run_squad.py:438] token_to_orig_map: 24:0 25:0 26:1 27:2 28:3 29:3 30:4 31:5 32:6 33:6 34:7 35:8 36:8 37:9 38:9 39:9 40:10 41:10 42:10 43:11 44:11 45:12 46:12 47:13 48:14 49:14 50:14 51:14 52:15 53:15 54:16 55:17 56:17 57:17 58:17 59:18 60:19 61:20 62:20 63:20 64:21 65:22 66:23 67:24 68:24 69:24 70:25 71:25 72:25 73:25 74:26 75:27 76:28 77:29 78:29 79:29 80:30 81:31 82:31 83:31 84:31 85:32 86:32 87:32 88:32 89:33 90:33 91:33 92:33 93:34 94:34 95:35 96:35 97:36 98:37 99:37 100:37 101:37 102:37 103:38 104:39 105:40 106:40 107:40 108:40 109:41 110:42 111:42 112:42 113:43 114:44 115:44 116:44 117:45 118:46 119:47 120:47 121:47 122:48 123:48 124:49 125:49 126:50 127:50 128:50 129:51 130:51 131:52 132:53 133:53 134:53 135:54 136:54 137:54 138:55 139:56 140:56 141:57 142:57 143:58 144:58 145:59 146:59 147:59 148:60 149:60 150:61 151:62 152:63 153:63 154:63 155:64 156:65 157:66 158:67 159:68 160:68 161:69 162:69 163:70 164:71 165:71 166:71 167:71 168:72 169:73 170:73 171:73 172:74 173:74 174:75 175:76 176:76 177:76 178:77 179:77 180:78 181:78 182:79 183:79 184:79 185:80 186:80 187:81 188:82 189:82 190:83 191:83 192:84 193:84 194:84 195:85 196:86 197:86 198:86 199:87 200:88 201:88 202:89 203:90 204:90 205:90 206:91 207:92 208:92 209:93 210:93 211:93 212:94 213:95 214:95 215:95 216:96 217:97 218:97 219:98 220:98 221:99 222:100 223:100 224:100 225:100 226:100 227:101 228:101 229:101 230:102 231:103 232:104 233:104 234:105 235:106 236:107 237:107 238:107 239:108 240:108 241:109 242:109 243:110 244:110 245:111 246:111 247:112 248:112 249:113 250:113 251:113 252:113\n",
            "I0629 14:40:09.806463 140691672389504 run_squad.py:440] token_is_max_context: 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True\n",
            "I0629 14:40:09.806842 140691672389504 run_squad.py:442] input_ids: 2 319 5633 2027 199 77 1512 496 2085 470 107 12122 5385 2827 125 118 3456 39 88 8710 8262 118 14730 3 8495 1031 107 12122 5385 2827 77 1512 3456 39 10554 14717 894 2232 6992 3547 2795 1009 2245 125 118 228 3432 13732 6282 387 14718 14721 125 118 4535 1428 6684 4224 232 1370 107 242 2674 470 242 107 2995 1577 13403 1031 14717 242 93 14718 242 107 3825 5149 1580 47 1110 1577 776 6280 47 1756 12940 5849 10868 14717 1170 75 14718 319 5633 125 118 4535 1710 1508 303 569 232 77 10554 13542 14051 143 14721 1110 2330 4286 661 2002 3625 1282 10868 1710 77 141 1287 538 10554 14723 3625 306 10554 47 14721 1373 118 11050 10034 2398 425 10554 47 14721 4438 166 2157 1191 11467 1562 656 5757 101 11551 5559 5595 1539 107 242 93 14723 1003 107 9479 242 10591 1014 14489 6007 242 2232 6992 3547 14721 107 141 1287 538 10554 47 4438 5656 4452 661 3456 39 1794 10661 13190 4519 47 8771 661 107 5829 661 5757 6466 5757 101 6466 1003 3181 1616 14721 1110 107 157 4438 14189 882 22 1710 14489 6007 10554 47 14723 107 5757 101 6466 1003 3181 1616 3383 6424 77 12933 184 4343 1118 13962 4510 1988 13855 242 107 10554 47 501 107 1965 1009 118 6983 93 2723 2273 107 157 346 1580 414 25 2795 1009 11212 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.807127 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.807414 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.807556 140691672389504 run_squad.py:451] start_position: 55\n",
            "I0629 14:40:09.807660 140691672389504 run_squad.py:452] end_position: 92\n",
            "I0629 14:40:09.807757 140691672389504 run_squad.py:454] answer: del ##iv ##ere ##d to the of ##fi ##ce of the federal re ##gis ##ter ( of ##r ) of the national archi ##ve ##s and re ##co ##rd ##s ad ##minis ##tra ##tion ( na ##ra )\n",
            "I0629 14:40:09.814934 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.815080 140691672389504 run_squad.py:432] unique_id: 1000000018\n",
            "I0629 14:40:09.815162 140691672389504 run_squad.py:433] example_index: 17\n",
            "I0629 14:40:09.815233 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.815461 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t other begi ##nni ##ng ##s of or ##ig ##ination do som ##e of the last na ##me ##s of the greek ##s sha ##re ? [SEP] greek sur ##name ##s we ##re wid ##ely in us ##e by the 9 ##th c ##ent ##ur ##y sup ##p ##lan ##ting the an ##ci ##ent tradi ##tion of us ##ing the fa ##ther [UNK] s na ##me , how ##ever greek sur ##name ##s are mos ##t comm ##only patr ##on ##y ##mi ##cs . comm ##only , greek mal ##e sur ##name ##s end in - s , whi ##ch is the comm ##on end ##ing for greek mas ##c ##uli ##ne prope ##r no ##un ##s in the no ##min ##ati ##ve cas ##e . ex ##ce ##pt ##ion ##ally , som ##e end in - o ##u , indi ##ca ##ting the gen ##it ##iv ##e cas ##e of thi ##s prope ##r no ##un for patr ##on ##y ##mi ##c re ##ason ##s . alt ##hou ##gh sur ##name ##s in mai ##n ##land gre ##ece are sta ##tic to ##day , d ##y ##nam ##ic and chan ##gin ##g patr ##on ##y ##mi ##c us ##age sur ##vi ##ve ##s in mid ##dle na ##me ##s w ##here the gen ##it ##iv ##e of fa ##ther ' s fir ##st na ##me is comm ##only the mid ##dle na ##me ( thi ##s us ##age ha ##vin ##g be ##en pass ##ed on to the ru ##ssi ##ans ) . in cy ##pr ##us , by con ##tra ##st , sur ##name ##s fol ##low the an ##ci ##ent tradi ##tion of be ##ing gi ##ven ac ##co ##rd ##ing to the fa ##ther [UNK] s na ##me . final ##ly , in ad ##di ##tion to greek - deri ##ve ##d sur ##name ##s man ##y ha ##ve latin , turk ##is ##h and italia ##n or ##ig ##in . [SEP]\n",
            "I0629 14:40:09.815644 140691672389504 run_squad.py:438] token_to_orig_map: 29:0 30:1 31:1 32:1 33:2 34:2 35:3 36:3 37:4 38:5 39:5 40:6 41:7 42:8 43:8 44:9 45:9 46:9 47:9 48:10 49:10 50:10 51:10 52:11 53:12 54:12 55:12 56:13 57:13 58:14 59:15 60:15 61:16 62:17 63:17 64:17 65:17 66:18 67:18 68:18 69:19 70:19 71:20 72:21 73:21 74:21 75:22 76:23 77:23 78:24 79:24 80:25 81:25 82:25 83:25 84:25 85:25 86:26 87:26 88:26 89:27 90:28 91:28 92:29 93:29 94:29 95:30 96:31 97:32 98:32 99:32 100:33 101:33 102:34 103:35 104:36 105:36 106:37 107:37 108:38 109:39 110:40 111:40 112:40 113:40 114:41 115:41 116:42 117:42 118:42 119:43 120:44 121:45 122:45 123:45 124:45 125:46 126:46 127:46 128:47 129:47 130:47 131:47 132:47 133:47 134:48 135:48 136:49 137:50 138:51 139:51 140:51 141:51 142:52 143:52 144:52 145:53 146:54 147:54 148:54 149:54 150:55 151:55 152:56 153:57 154:57 155:58 156:58 157:59 158:59 159:60 160:61 161:61 162:61 163:61 164:61 165:62 166:62 167:62 168:62 169:63 170:63 171:63 172:64 173:64 174:64 175:65 176:66 177:66 178:66 179:67 180:67 181:68 182:69 183:69 184:70 185:70 186:70 187:71 188:71 189:71 190:71 191:72 192:73 193:73 194:73 195:74 196:74 197:74 198:74 199:74 200:75 201:75 202:76 203:76 204:76 205:76 206:77 207:78 208:78 209:79 210:79 211:79 212:80 213:80 214:81 215:82 216:82 217:82 218:82 219:83 220:84 221:84 222:84 223:84 224:85 225:85 226:86 227:86 228:87 229:88 230:88 231:89 232:90 233:90 234:91 235:91 236:92 237:92 238:92 239:93 240:93 241:94 242:94 243:94 244:95 245:95 246:96 247:96 248:97 249:98 250:99 251:100 252:100 253:100 254:100 255:100 256:101 257:102 258:102 259:102 260:102 261:103 262:104 263:104 264:104 265:104 266:105 267:105 268:105 269:106 270:106 271:107 272:108 273:108 274:108 275:109 276:109 277:110 278:111 279:111 280:112 281:112 282:113 283:113 284:113 285:113 286:114 287:115 288:116 289:116 290:116 291:116 292:117 293:117 294:117 295:118 296:118 297:118 298:119 299:120 300:120 301:120 302:121 303:122 304:122 305:122 306:122 307:122 308:123 309:123 310:123 311:124 312:124 313:125 314:125 315:126 316:126 317:127 318:127 319:127 320:128 321:129 322:129 323:130 324:130 325:130 326:130\n",
            "I0629 14:40:09.815821 140691672389504 run_squad.py:440] token_is_max_context: 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True\n",
            "I0629 14:40:09.816011 140691672389504 run_squad.py:442] input_ids: 2 8955 118 5150 2925 14612 1988 47 242 894 2097 13109 2027 11272 44 242 107 8440 1170 2638 47 242 107 3886 47 3308 409 14730 3 3886 13592 1753 47 4403 409 12822 14479 501 2743 44 1539 107 686 2255 213 6424 778 157 12818 538 1960 9870 107 1420 6479 6424 12577 10868 242 2743 334 107 1996 9762 1 141 1170 2638 14721 12790 6767 3886 13592 1753 47 4438 13271 118 11258 8124 2599 184 157 4246 13649 14723 11258 8124 14721 3886 2071 44 13592 1753 47 9479 501 14722 141 14721 7849 1014 4535 107 11258 184 9479 334 2002 3886 6699 306 9453 569 10884 93 738 1055 47 501 107 738 5860 14250 1580 14399 44 14723 1965 470 6489 1210 13451 14721 11272 44 9479 501 14722 228 49 14721 4229 1282 9870 107 5658 2282 6684 44 14399 44 242 5338 47 10884 93 738 1055 2002 2599 184 157 4246 306 1577 12513 47 14723 5961 10150 10220 13592 1753 47 501 9798 22 1245 3503 6338 4438 5757 3335 1370 6240 14721 198 157 13487 6553 1110 13844 9584 303 2599 184 157 4246 306 2743 9926 13592 3802 1580 47 501 13432 13304 1170 2638 47 319 5633 107 5658 2282 6684 44 242 1996 9762 14716 141 13577 2325 1170 2638 4535 11258 8124 107 13432 13304 1170 2638 14717 5338 47 2743 9926 346 3440 303 414 25 9972 661 2085 1370 107 2633 11154 4692 14718 14723 501 5620 11658 200 14721 1539 2232 5849 2325 14721 13592 1753 47 12852 6462 107 1420 6479 6424 12577 10868 242 414 334 1191 11467 14638 776 6280 334 1370 107 1996 9762 1 141 1170 2638 14723 5720 2587 14721 501 1756 845 10868 1370 3886 14722 13351 1580 232 13592 1753 47 1036 157 346 1580 2432 14721 13773 278 37 1110 633 22 894 2097 211 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.816176 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.816331 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.816424 140691672389504 run_squad.py:451] start_position: 311\n",
            "I0629 14:40:09.816489 140691672389504 run_squad.py:452] end_position: 326\n",
            "I0629 14:40:09.816554 140691672389504 run_squad.py:454] answer: man ##y ha ##ve latin , turk ##is ##h and italia ##n or ##ig ##in .\n",
            "I0629 14:40:09.819461 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.819581 140691672389504 run_squad.py:432] unique_id: 1000000019\n",
            "I0629 14:40:09.819661 140691672389504 run_squad.py:433] example_index: 18\n",
            "I0629 14:40:09.819731 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.819840 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t is the fun ##ction of a \" be ##er en ##gine \" ? [SEP] a \" be ##er en ##gine \" is a de ##vic ##e for pu ##mp ##ing be ##er , or ##ig ##ina ##lly manu ##ally opera ##ted and ty ##pi ##cal ##ly us ##ed to dis ##pen ##se be ##er from a cas ##k or con ##tain ##er in a pu ##b ' s base ##ment or cell ##ar . [SEP]\n",
            "I0629 14:40:09.819945 140691672389504 run_squad.py:438] token_to_orig_map: 17:0 18:1 19:1 20:1 21:2 22:2 23:2 24:3 25:4 26:5 27:5 28:5 29:6 30:7 31:7 32:7 33:8 34:8 35:8 36:9 37:9 38:9 39:9 40:10 41:10 42:11 43:11 44:12 45:13 46:13 47:13 48:13 49:14 50:14 51:15 52:16 53:16 54:16 55:17 56:17 57:18 58:19 59:20 60:20 61:21 62:22 63:22 64:22 65:23 66:24 67:25 68:25 69:25 70:25 71:26 72:26 73:27 74:28 75:28 76:28\n",
            "I0629 14:40:09.820047 140691672389504 run_squad.py:440] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True\n",
            "I0629 14:40:09.820240 140691672389504 run_squad.py:442] input_ids: 2 8955 118 4535 107 6228 10748 242 77 14711 414 143 2795 14324 14711 14730 3 77 14711 414 143 2795 14324 14711 4535 77 79 14562 44 2002 2277 10042 334 414 143 14721 894 2097 959 1763 7017 13451 1519 11212 1110 2049 3393 13962 2587 2743 661 1370 4730 1107 1202 414 143 13616 77 14399 20 894 2232 6588 143 501 77 2277 532 14716 141 10934 1995 894 8155 320 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.820418 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.820578 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.820652 140691672389504 run_squad.py:451] start_position: 51\n",
            "I0629 14:40:09.820720 140691672389504 run_squad.py:452] end_position: 75\n",
            "I0629 14:40:09.820785 140691672389504 run_squad.py:454] answer: to dis ##pen ##se be ##er from a cas ##k or con ##tain ##er in a pu ##b ' s base ##ment or cell ##ar\n",
            "I0629 14:40:09.825257 140691672389504 run_squad.py:431] *** Example ***\n",
            "I0629 14:40:09.825397 140691672389504 run_squad.py:432] unique_id: 1000000020\n",
            "I0629 14:40:09.825494 140691672389504 run_squad.py:433] example_index: 19\n",
            "I0629 14:40:09.825564 140691672389504 run_squad.py:434] doc_span_index: 0\n",
            "I0629 14:40:09.825720 140691672389504 run_squad.py:436] tokens: [CLS] wha ##t co ##ats uran ##ium metal in li ##qu ##id ? [SEP] uran ##ium metal re ##ac ##ts with al ##most all non - metal element ##s ( with an ex ##ce ##pt ##ion of the noble gas ##es ) and the ##ir com ##pou ##nds , with re ##ac ##ti ##vi ##ty inc ##rea ##sing with te ##mp ##eratu ##re . hy ##d ##ro ##ch ##lo ##ric and nit ##ric ac ##id ##s dis ##sol ##ve uran ##ium , bu ##t non - o ##xi ##di ##zin ##g ac ##id ##s other th ##an hy ##d ##ro ##ch ##lo ##ric ac ##id at ##ta ##ck the element ver ##y slo ##w ##ly . w ##hen fine ##ly di ##vid ##ed , i ##t can re ##ac ##t with col ##d water ; in air , uran ##ium metal be ##com ##es co ##ate ##d with a da ##rk la ##yer of uran ##ium o ##xi ##de . uran ##ium in or ##es is ext ##ra ##ct ##ed che ##mi ##cal ##ly and con ##vert ##ed int ##o uran ##ium dio ##xi ##de or other che ##mi ##cal form ##s us ##able in indus ##try . [SEP]\n",
            "I0629 14:40:09.913772 140691672389504 run_squad.py:438] token_to_orig_map: 14:0 15:0 16:1 17:2 18:2 19:2 20:3 21:4 22:4 23:5 24:6 25:6 26:6 27:7 28:7 29:8 30:8 31:9 32:10 33:10 34:10 35:10 36:11 37:12 38:13 39:14 40:14 41:14 42:15 43:16 44:16 45:17 46:17 47:17 48:17 49:18 50:19 51:19 52:19 53:19 54:19 55:20 56:20 57:20 58:21 59:22 60:22 61:22 62:22 63:22 64:23 65:23 66:23 67:23 68:23 69:23 70:24 71:25 72:25 73:26 74:26 75:26 76:27 77:27 78:27 79:28 80:28 81:28 82:29 83:29 84:30 85:30 86:30 87:30 88:30 89:30 90:30 91:31 92:31 93:31 94:32 95:33 96:33 97:34 98:34 99:34 100:34 101:34 102:34 103:35 104:35 105:36 106:36 107:36 108:37 109:38 110:39 111:39 112:40 113:40 114:40 115:40 116:41 117:41 118:42 119:42 120:43 121:43 122:43 123:43 124:44 125:44 126:45 127:46 128:46 129:46 130:47 131:48 132:48 133:49 134:49 135:50 136:51 137:51 138:52 139:52 140:53 141:54 142:54 143:54 144:55 145:55 146:55 147:56 148:57 149:58 150:58 151:59 152:59 153:60 154:61 155:61 156:62 157:62 158:62 159:62 160:63 161:63 162:64 163:65 164:65 165:66 166:67 167:67 168:67 169:67 170:68 171:68 172:68 173:68 174:69 175:70 176:70 177:70 178:71 179:71 180:72 181:72 182:73 183:73 184:73 185:74 186:75 187:76 188:76 189:76 190:77 191:77 192:78 193:78 194:79 195:80 196:80 197:80\n",
            "I0629 14:40:09.914185 140691672389504 run_squad.py:440] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True\n",
            "I0629 14:40:09.915653 140691672389504 run_squad.py:442] input_ids: 2 8955 118 1010 6321 14532 6190 1248 501 1179 11439 1567 14730 3 14532 6190 1248 1577 1009 2245 4479 166 11111 2272 1727 14722 1248 12167 47 14717 4479 1420 1965 470 6489 1210 242 107 3645 1092 199 14718 1110 107 4225 5656 13640 8891 14721 4479 1577 1009 753 3802 4358 11393 3958 6594 4479 966 10042 4252 409 14723 5663 232 745 1014 743 3648 1110 8447 3648 14638 1567 47 4730 14104 1580 14532 6190 14721 1373 118 1727 14722 228 4173 845 10215 303 14638 1567 47 5150 2723 34 5663 232 745 1014 743 3648 14638 1567 1003 105 1734 107 12167 3485 157 10196 882 2587 14723 319 9509 10241 2587 347 10434 661 14721 125 118 4736 1577 1009 118 4479 3173 232 8391 14726 501 4512 14721 14532 6190 1248 414 2313 199 1010 4640 232 4479 77 28 14319 290 13297 242 14532 6190 228 4173 609 14723 14532 6190 501 894 199 4535 11291 75 13354 661 4627 4246 13962 2587 1110 2232 11789 661 3456 39 14532 6190 1205 4173 609 894 5150 4627 4246 13962 10029 47 2743 8814 501 5384 10064 14723 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.916015 140691672389504 run_squad.py:444] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.916276 140691672389504 run_squad.py:446] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0629 14:40:09.916419 140691672389504 run_squad.py:448] impossible example\n",
            "Traceback (most recent call last):\n",
            "  File \"bert/run_squad.py\", line 1283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"bert/run_squad.py\", line 1200, in main\n",
            "    output_fn=train_writer.process_feature)\n",
            "  File \"bert/run_squad.py\", line 317, in convert_examples_to_features\n",
            "    query_tokens = tokenizer.tokenize(example.question_text)\n",
            "  File \"/content/gureBERT/bert/tokenization.py\", line 173, in tokenize\n",
            "    for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
            "  File \"/content/gureBERT/bert/tokenization.py\", line 331, in tokenize\n",
            "    if len(chars) > self.max_input_chars_per_word:\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUcKt7Ehn6lV",
        "colab_type": "text"
      },
      "source": [
        "# gureBERT (japanese) erabiliz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqPX8zgt6lAO",
        "colab_type": "code",
        "outputId": "2b3f9ffa-60e5-4e7a-a7ed-3e01aecc88a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "#################################################################\n",
        "# gureBERT japanese\n",
        "\n",
        "!cd /content/\n",
        "\n",
        "!git clone --recursive https://github.com/zmwebdev/bert-japanese\n",
        "%cd bert-japanese\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert-japanese'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)   \u001b[K\rremote: Counting objects:  22% (2/9)   \u001b[K\rremote: Counting objects:  33% (3/9)   \u001b[K\rremote: Counting objects:  44% (4/9)   \u001b[K\rremote: Counting objects:  55% (5/9)   \u001b[K\rremote: Counting objects:  66% (6/9)   \u001b[K\rremote: Counting objects:  77% (7/9)   \u001b[K\rremote: Counting objects:  88% (8/9)   \u001b[K\rremote: Counting objects: 100% (9/9)   \u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects:  12% (1/8)   \u001b[K\rremote: Compressing objects:  25% (2/8)   \u001b[K\rremote: Compressing objects:  37% (3/8)   \u001b[K\rremote: Compressing objects:  50% (4/8)   \u001b[K\rremote: Compressing objects:  62% (5/8)   \u001b[K\rremote: Compressing objects:  75% (6/8)   \u001b[K\rremote: Compressing objects:  87% (7/8)   \u001b[K\rremote: Compressing objects: 100% (8/8)   \u001b[K\rremote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "Receiving objects:   0% (1/170)   \rReceiving objects:   1% (2/170)   \rReceiving objects:   2% (4/170)   \rReceiving objects:   3% (6/170)   \rReceiving objects:   4% (7/170)   \rReceiving objects:   5% (9/170)   \rReceiving objects:   6% (11/170)   \rReceiving objects:   7% (12/170)   \rReceiving objects:   8% (14/170)   \rReceiving objects:   9% (16/170)   \rReceiving objects:  10% (17/170)   \rReceiving objects:  11% (19/170)   \rReceiving objects:  12% (21/170)   \rReceiving objects:  13% (23/170)   \rReceiving objects:  14% (24/170)   \rReceiving objects:  15% (26/170)   \rReceiving objects:  16% (28/170)   \rReceiving objects:  17% (29/170)   \rReceiving objects:  18% (31/170)   \rReceiving objects:  19% (33/170)   \rReceiving objects:  20% (34/170)   \rReceiving objects:  21% (36/170)   \rReceiving objects:  22% (38/170)   \rReceiving objects:  23% (40/170)   \rReceiving objects:  24% (41/170)   \rReceiving objects:  25% (43/170)   \rReceiving objects:  26% (45/170)   \rReceiving objects:  27% (46/170)   \rReceiving objects:  28% (48/170)   \rReceiving objects:  29% (50/170)   \rReceiving objects:  30% (51/170)   \rReceiving objects:  31% (53/170)   \rReceiving objects:  32% (55/170)   \rReceiving objects:  33% (57/170)   \rReceiving objects:  34% (58/170)   \rReceiving objects:  35% (60/170)   \rReceiving objects:  36% (62/170)   \rReceiving objects:  37% (63/170)   \rReceiving objects:  38% (65/170)   \rReceiving objects:  39% (67/170)   \rReceiving objects:  40% (68/170)   \rReceiving objects:  41% (70/170)   \rReceiving objects:  42% (72/170)   \rReceiving objects:  43% (74/170)   \rReceiving objects:  44% (75/170)   \rReceiving objects:  45% (77/170)   \rReceiving objects:  46% (79/170)   \rReceiving objects:  47% (80/170)   \rReceiving objects:  48% (82/170)   \rReceiving objects:  49% (84/170)   \rReceiving objects:  50% (85/170)   \rReceiving objects:  51% (87/170)   \rReceiving objects:  52% (89/170)   \rReceiving objects:  53% (91/170)   \rReceiving objects:  54% (92/170)   \rReceiving objects:  55% (94/170)   \rReceiving objects:  56% (96/170)   \rReceiving objects:  57% (97/170)   \rReceiving objects:  58% (99/170)   \rReceiving objects:  59% (101/170)   \rReceiving objects:  60% (102/170)   \rReceiving objects:  61% (104/170)   \rReceiving objects:  62% (106/170)   \rReceiving objects:  63% (108/170)   \rReceiving objects:  64% (109/170)   \rReceiving objects:  65% (111/170)   \rReceiving objects:  66% (113/170)   \rReceiving objects:  67% (114/170)   \rReceiving objects:  68% (116/170)   \rReceiving objects:  69% (118/170)   \rReceiving objects:  70% (119/170)   \rReceiving objects:  71% (121/170)   \rReceiving objects:  72% (123/170)   \rReceiving objects:  73% (125/170)   \rReceiving objects:  74% (126/170)   \rReceiving objects:  75% (128/170)   \rReceiving objects:  76% (130/170)   \rReceiving objects:  77% (131/170)   \rReceiving objects:  78% (133/170)   \rReceiving objects:  79% (135/170)   \rReceiving objects:  80% (136/170)   \rReceiving objects:  81% (138/170)   \rReceiving objects:  82% (140/170)   \rReceiving objects:  83% (142/170)   \rReceiving objects:  84% (143/170)   \rremote: Total 170 (delta 1), reused 6 (delta 1), pack-reused 161\u001b[K\n",
            "Receiving objects:  85% (145/170)   \rReceiving objects:  86% (147/170)   \rReceiving objects:  87% (148/170)   \rReceiving objects:  88% (150/170)   \rReceiving objects:  89% (152/170)   \rReceiving objects:  90% (153/170)   \rReceiving objects:  91% (155/170)   \rReceiving objects:  92% (157/170)   \rReceiving objects:  93% (159/170)   \rReceiving objects:  94% (160/170)   \rReceiving objects:  95% (162/170)   \rReceiving objects:  96% (164/170)   \rReceiving objects:  97% (165/170)   \rReceiving objects:  98% (167/170)   \rReceiving objects:  99% (169/170)   \rReceiving objects: 100% (170/170)   \rReceiving objects: 100% (170/170), 261.38 KiB | 3.27 MiB/s, done.\n",
            "Resolving deltas:   0% (0/92)   \rResolving deltas:   5% (5/92)   \rResolving deltas:   9% (9/92)   \rResolving deltas:  17% (16/92)   \rResolving deltas:  20% (19/92)   \rResolving deltas:  21% (20/92)   \rResolving deltas:  22% (21/92)   \rResolving deltas:  23% (22/92)   \rResolving deltas:  43% (40/92)   \rResolving deltas:  45% (42/92)   \rResolving deltas:  51% (47/92)   \rResolving deltas:  54% (50/92)   \rResolving deltas:  83% (77/92)   \rResolving deltas:  86% (80/92)   \rResolving deltas:  91% (84/92)   \rResolving deltas:  94% (87/92)   \rResolving deltas: 100% (92/92)   \rResolving deltas: 100% (92/92), done.\n",
            "Submodule 'bert' (https://github.com/google-research/bert.git) registered for path 'bert'\n",
            "Cloning into '/content/gureBERT/bert-japanese/bert'...\n",
            "remote: Enumerating objects: 329, done.        \n",
            "remote: Total 329 (delta 0), reused 0 (delta 0), pack-reused 329        \n",
            "Receiving objects: 100% (329/329), 239.27 KiB | 3.52 MiB/s, done.\n",
            "Resolving deltas: 100% (188/188), done.\n",
            "Submodule path 'bert': checked out 'f39e881b169b9d53bea03d2d341b31707a6c052b'\n",
            "/content/gureBERT/bert-japanese\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d68MKTOjQxVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrhObi5n8xYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/data-download-and-extract.py\n",
        "!bash src/file-preprocessing.sh\n",
        "\n",
        "#!gsutil cp -r gs://gurebert/gureBERT/data/wiki/AA/wiki_00 data/wiki/AA/wiki_00\n",
        "\n",
        "# Oharra: esaldiak lerro bakarrean jarri behar dira\n",
        "#!cat data/wiki/AA/wiki_00"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m0o4LUCJ8ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wiki-eu.vocab eta wiki-eu.model sortzen ditu\n",
        "\n",
        "!pip install sentencepiece\n",
        "!python3 src/train-sentencepiece.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJFc3jAkOyHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp -r data/ gs://gurebert/gureBERT/\n",
        "!gsutil cp -r model/ gs://gurebert/gureBERT/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_qxPQAgP1sA",
        "colab_type": "code",
        "outputId": "358ed8f7-dd3c-4ef6-aee1-74edd53a49ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "!mkdir model\n",
        "!gsutil cp gs://gurebert/gureBERT/model/wiki-eu.vocab model\n",
        "!gsutil cp gs://gurebert/gureBERT/model/wiki-eu.model model\n",
        "!head -n 50 model/wiki-eu.vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>\t0\n",
            "<s>\t0\n",
            "</s>\t0\n",
            "[PAD]\t0\n",
            "[CLS]\t0\n",
            "[SEP]\t0\n",
            "[MASK]\t0\n",
            ",\t-2.87014\n",
            ".\t-2.98912\n",
            "▁eta\t-3.5406\n",
            "▁ziren\t-4.15683\n",
            "-\t-4.26943\n",
            "an\t-4.4068\n",
            "▁da\t-4.40907\n",
            "a\t-4.47601\n",
            "▁zen\t-4.51427\n",
            "ko\t-4.52925\n",
            "▁\t-4.67755\n",
            "▁zeuden\t-4.76555\n",
            "▁bat\t-4.84966\n",
            "▁\"\t-4.95145\n",
            "\"\t-4.95743\n",
            "▁1\t-5.04005\n",
            "ak\t-5.2203\n",
            "▁izan\t-5.32102\n",
            "▁zuen\t-5.34175\n",
            "en\t-5.36722\n",
            "ren\t-5.38606\n",
            ":\t-5.53342\n",
            "k\t-5.56319\n",
            "▁(\t-5.58916\n",
            "▁zituzten\t-5.58963\n",
            "▁zuten\t-5.59859\n",
            "▁etxek\t-5.66741\n",
            ")\t-5.66971\n",
            "▁du\t-5.70751\n",
            "▁zituen\t-5.72891\n",
            "▁enpresak\t-5.73378\n",
            "▁bere\t-5.7694\n",
            "▁2\t-5.78944\n",
            "▁dago\t-5.79301\n",
            "▁2009\t-5.8295\n",
            "▁ere\t-5.84878\n",
            "▁2007\t-5.87422\n",
            "n\t-5.87535\n",
            "▁pertsona\t-5.88342\n",
            "▁biztanle\t-5.93646\n",
            "aren\t-5.97756\n",
            "▁dira\t-5.97904\n",
            "▁ditu\t-6.0186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl457PDf8r-y",
        "colab_type": "code",
        "outputId": "320787c0-4f0e-4d29-f1af-3adecece4172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#https://github.com/zmwebdev/bert-japanese/blob/master/notebook/check-trained-tokenizer.ipynb\n",
        "!pip install sentencepiece\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "import tokenization_sentencepiece as tokenization\n",
        "\n",
        "text1 = \"Nere kotxea aitonaren etxe alboan dago\"\n",
        "text2 = \"Gorria da gure etxearen inguruan dagoen lorearen kolorea\"\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    model_file=\"model/wiki-eu.model\",\n",
        "    vocab_file=\"model/wiki-eu.vocab\",\n",
        "    do_lower_case=True)\n",
        "\n",
        "tokenizer.tokenize(text1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.82)\n",
            "Loaded a trained SentencePiece model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁nere', '▁kotxe', 'a', '▁aitona', 'ren', '▁etxe', '▁alboan', '▁dago']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ExyRkNqGpvi",
        "colab_type": "code",
        "outputId": "96fb8001-d24d-47ea-ae3b-dc80469fca0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "tokenizer.tokenize(text2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁gorria',\n",
              " '▁da',\n",
              " '▁gure',\n",
              " '▁etxearen',\n",
              " '▁inguruan',\n",
              " '▁dagoen',\n",
              " '▁lore',\n",
              " 'aren',\n",
              " '▁kolorea']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u9yUjU0HHwx",
        "colab_type": "code",
        "outputId": "e63d3131-22f9-47e5-8437-1527aac1446f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#do_lower_case=False jarrita tokenizazioa okerragoa da\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    model_file=\"model/wiki-eu.model\",\n",
        "    vocab_file=\"model/wiki-eu.vocab\",\n",
        "    do_lower_case=False)\n",
        "\n",
        "tokenizer.tokenize(text1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁', 'N', 'ere', '▁kotxe', 'a', '▁aitona', 'ren', '▁etxe', '▁alboan', '▁dago']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZMfqyxoHZnG",
        "colab_type": "code",
        "outputId": "d0cf6574-bcd8-4e35-e295-d0fee84fe64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "tokenizer.tokenize(text2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁',\n",
              " 'G',\n",
              " 'orri',\n",
              " 'a',\n",
              " '▁da',\n",
              " '▁gure',\n",
              " '▁etxearen',\n",
              " '▁inguruan',\n",
              " '▁dagoen',\n",
              " '▁lore',\n",
              " 'aren',\n",
              " '▁kolorea']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wosrOAdz2gQs",
        "colab_type": "code",
        "outputId": "d85a3585-d40c-43f9-930b-13475e0b2c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "# convert to WordPiece (kodean errorea dago). begiratu https://github.com/kwonmha/bert-vocab-builder/pull/4#issue-291306156\n",
        "\n",
        "!git clone https://github.com/kwonmha/bert-vocab-builder.git\n",
        "\n",
        "!python bert-vocab-builder/subword_builder.py \\\n",
        "--corpus_filepattern model/wiki-eu.vocab \\\n",
        "--output_filename model/wiki-eu-wordpiece.vocab\n",
        "#--min_count {minimum_subtoken_counts}\n",
        "\n",
        "# https://github.com/kwonmha/bert-vocab-builder/pull/4#issue-291306156"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'bert-vocab-builder' already exists and is not an empty directory.\n",
            "Traceback (most recent call last):\n",
            "  File \"bert-vocab-builder/subword_builder.py\", line 33, in <module>\n",
            "    import text_encoder\n",
            "  File \"/content/bert-japanese/bert-vocab-builder/text_encoder.py\", line 647\n",
            "    <<<<<<< HEAD\n",
            "     ^\n",
            "SyntaxError: invalid syntax\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBDprGbXRt4l",
        "colab_type": "code",
        "outputId": "1093ee9f-3fc7-48f5-96de-55fe03096bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "#Creating data for pretraining\n",
        "#Create .tfrecord files for pretraining. For longer sentence data, replace the value of max_seq_length with 512.\n",
        "!cat creating_data_for_pretraining.sh\n",
        "!bash creating_data_for_pretraining.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for DIR in $( find data/wiki/ -mindepth 1 -type d ); do \n",
            "  python3 src/create_pretraining_data.py \\\n",
            "    --input_file=${DIR}/all.txt \\\n",
            "    --output_file=${DIR}/all-maxseq128.tfrecord \\\n",
            "    --model_file=./model/wiki-eu.model \\\n",
            "    --vocab_file=./model/wiki-eu.vocab \\\n",
            "    --do_lower_case=True \\\n",
            "    --max_seq_length=128 \\\n",
            "    --max_predictions_per_seq=20 \\\n",
            "    --masked_lm_prob=0.15 \\\n",
            "    --random_seed=12345 \\\n",
            "    --dupe_factor=5\n",
            "done"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlBYBR1_G9Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy to gs\n",
        "!gsutil cp -r data gs://gurebert/gureBERT\n",
        "!gsutil cp -r model gs://gurebert/gureBERT\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29sDsZvBgFLj",
        "colab_type": "code",
        "outputId": "d69c0256-6298-492b-9d3e-b1fa8e086803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# Pre-Training\n",
        "# https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/pretraining.ipynb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "#Check TPU devices\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.16.115.170:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 5460104680638402674),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16051411686286296001),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1933247979704415977),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11046566402910946519),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13421296677076221120),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5352660506872180047),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7874427115731877787),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15986719012258325089),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7612787266547945911),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6745551487070946188),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14981037038192380193)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AzYwFXw8iJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DATA_GCS = 'gs://gurebert/gureBERT/data/wiki'\n",
        "TARGET_DIRS = [\n",
        "  'AA',\n",
        "  'AB',\n",
        "  'AC'\n",
        "]\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "#MAX_SEQ_LEN = 512\n",
        "\n",
        "\n",
        "INPUT_FILE = ','.join( [ '{}/{}/all-maxseq{}.tfrecord'.format(INPUT_DATA_GCS, elem, MAX_SEQ_LEN) for elem in TARGET_DIRS] )\n",
        "\n",
        "OUTPUT_GCS = 'gs://gurebert/gureBERT/model'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRRjRvl19NDV",
        "colab_type": "code",
        "outputId": "7135ff35-5f86-46c3-afe0-5f4e4312dddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Adding whole word masking aldaketa egin dute BERT-en. update egin behar da!!!\n",
        "# Begiratu https://github.com/google-research/bert/commit/0fce551b55caabcfba52c61e18f34b541aef186a\n",
        "\n",
        "!python src/run_pretraining.py \\\n",
        "  --input_file={INPUT_FILE} \\\n",
        "  --output_dir={OUTPUT_GCS} \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  --num_tpu_cores=8 \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length={MAX_SEQ_LEN} \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=1400000 \\\n",
        "  --num_warmup_steps=10000 \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --learning_rate=1e-4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0625 19:38:15.208214 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0625 19:38:15.209448 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:497: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0625 19:38:15.210220 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:412: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0625 19:38:15.210398 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:412: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0625 19:38:15.210564 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0625 19:38:15.211427 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:419: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0625 19:38:16.644103 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:423: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0625 19:38:17.434032 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:425: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0625 19:38:17.434276 140282338469760 run_pretraining.py:425] *** Input Files ***\n",
            "I0625 19:38:17.434366 140282338469760 run_pretraining.py:427]   gs://gurebert/gureBERT/data/wiki/AA/all-maxseq128.tfrecord\n",
            "I0625 19:38:17.434440 140282338469760 run_pretraining.py:427]   gs://gurebert/gureBERT/data/wiki/AB/all-maxseq128.tfrecord\n",
            "I0625 19:38:17.434512 140282338469760 run_pretraining.py:427]   gs://gurebert/gureBERT/data/wiki/AC/all-maxseq128.tfrecord\n",
            "W0625 19:38:18.462627 140282338469760 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0625 19:38:19.468373 140282338469760 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f95d11146a8>) includes params argument, but params are not passed to Estimator.\n",
            "I0625 19:38:19.469875 140282338469760 estimator.py:209] Using config: {'_model_dir': 'gs://gurebert/gureBERT/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.16.115.170:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f95d10b3588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.16.115.170:8470', '_evaluation_master': 'grpc://10.16.115.170:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f95dd593be0>}\n",
            "I0625 19:38:19.470217 140282338469760 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0625 19:38:19.470873 140282338469760 run_pretraining.py:464] ***** Running training *****\n",
            "I0625 19:38:19.470968 140282338469760 run_pretraining.py:465]   Batch size = 64\n",
            "I0625 19:38:23.258418 140282338469760 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.16.115.170:8470) for TPU system metadata.\n",
            "2019-06-25 19:38:23.259746: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0625 19:38:23.274165 140282338469760 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0625 19:38:23.274393 140282338469760 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0625 19:38:23.274487 140282338469760 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0625 19:38:23.274553 140282338469760 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0625 19:38:23.274617 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5460104680638402674)\n",
            "I0625 19:38:23.275380 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1933247979704415977)\n",
            "I0625 19:38:23.275457 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11046566402910946519)\n",
            "I0625 19:38:23.275525 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13421296677076221120)\n",
            "I0625 19:38:23.275590 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5352660506872180047)\n",
            "I0625 19:38:23.275657 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7874427115731877787)\n",
            "I0625 19:38:23.275721 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15986719012258325089)\n",
            "I0625 19:38:23.275784 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7612787266547945911)\n",
            "I0625 19:38:23.275844 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6745551487070946188)\n",
            "I0625 19:38:23.275904 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14981037038192380193)\n",
            "I0625 19:38:23.275966 140282338469760 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16051411686286296001)\n",
            "W0625 19:38:23.282766 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0625 19:38:23.299888 140282338469760 estimator.py:1145] Calling model_fn.\n",
            "W0625 19:38:23.300519 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:342: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0625 19:38:23.306573 140282338469760 deprecation.py:323] From src/run_pretraining.py:373: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0625 19:38:23.306786 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0625 19:38:23.335085 140282338469760 deprecation.py:323] From src/run_pretraining.py:390: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0625 19:38:23.335338 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0625 19:38:23.336897 140282338469760 deprecation_wrapper.py:119] From src/run_pretraining.py:398: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0625 19:38:23.342769 140282338469760 deprecation.py:323] From src/run_pretraining.py:405: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0625 19:38:23.426860 140282338469760 run_pretraining.py:122] *** Features ***\n",
            "I0625 19:38:23.427149 140282338469760 run_pretraining.py:124]   name = input_ids, shape = (8, 128)\n",
            "I0625 19:38:23.427274 140282338469760 run_pretraining.py:124]   name = input_mask, shape = (8, 128)\n",
            "I0625 19:38:23.427362 140282338469760 run_pretraining.py:124]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0625 19:38:23.427445 140282338469760 run_pretraining.py:124]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0625 19:38:23.427529 140282338469760 run_pretraining.py:124]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0625 19:38:23.427610 140282338469760 run_pretraining.py:124]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0625 19:38:23.427688 140282338469760 run_pretraining.py:124]   name = segment_ids, shape = (8, 128)\n",
            "W0625 19:38:23.427910 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:172: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0625 19:38:23.430187 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:411: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0625 19:38:23.466429 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/modeling.py:492: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0625 19:38:23.524997 140282338469760 deprecation.py:506] From /content/bert-japanese/src/../bert/modeling.py:359: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0625 19:38:23.546822 140282338469760 deprecation.py:323] From /content/bert-japanese/src/../bert/modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0625 19:38:27.546400 140282338469760 run_pretraining.py:172] **** Trainable Variables ****\n",
            "I0625 19:38:27.546652 140282338469760 run_pretraining.py:178]   name = bert/embeddings/word_embeddings:0, shape = (32000, 768)\n",
            "I0625 19:38:27.546783 140282338469760 run_pretraining.py:178]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0625 19:38:27.546877 140282338469760 run_pretraining.py:178]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0625 19:38:27.546964 140282338469760 run_pretraining.py:178]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.547065 140282338469760 run_pretraining.py:178]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.547148 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547240 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547317 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547397 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547473 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547553 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547628 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.547708 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.547784 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.547859 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.547933 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.548025 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.548103 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.548182 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548265 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.548341 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.548416 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548494 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548570 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548649 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548723 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548801 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.548876 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.548953 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.549039 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.549117 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.549198 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.549278 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.549353 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.549464 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.549555 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.549630 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.549705 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.549783 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.549859 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.549938 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550027 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.550108 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550183 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.550268 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550342 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.550415 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.550496 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.550579 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.550655 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.550734 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.550811 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.550886 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.550960 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551053 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551129 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551213 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551289 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551367 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551442 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.551520 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.551597 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.551673 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.551748 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.551826 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.551902 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.551980 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552069 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.552144 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.552223 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552304 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552379 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552458 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552532 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552613 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552688 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.552767 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.552842 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.552917 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.552991 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.553102 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.553179 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.553265 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553341 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.553414 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.553490 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.553569 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553642 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.553722 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553798 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.553876 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.553951 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.554041 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.554118 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.554199 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.554275 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.554352 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.554429 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.554506 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.554581 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.554655 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.554730 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.554809 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.554883 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.554962 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555051 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.555130 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555211 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.555290 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555366 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.555443 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.555517 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.555596 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.555672 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.555750 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.555824 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.555898 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.555972 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.556064 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.556140 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.556225 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.556300 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.586935 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.587312 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.587467 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.587575 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.587675 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.587777 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.587887 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.587991 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.588119 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.588230 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.588329 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.588429 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.588537 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.588637 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.588743 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.588844 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.588951 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.589072 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.589181 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.589300 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.589403 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.589504 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.589615 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.589721 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.589831 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.589935 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.590058 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.590166 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590286 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.590391 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590503 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.590605 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590713 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.590820 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.590930 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.591052 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.591159 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.591277 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.591384 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.591493 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.591601 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.591722 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.591833 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.591942 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592070 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592176 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592298 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592424 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592534 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592635 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.592743 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.592846 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.592947 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.593068 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.593182 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.593296 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.593404 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.593508 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.593610 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.593711 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.593819 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0625 19:38:27.593922 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.594047 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0625 19:38:27.594153 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.594272 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0625 19:38:27.594377 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.594484 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.594584 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.594686 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.594788 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0625 19:38:27.594895 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0625 19:38:27.594999 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0625 19:38:27.595129 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.595242 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.595343 140282338469760 run_pretraining.py:178]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.595443 140282338469760 run_pretraining.py:178]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.595552 140282338469760 run_pretraining.py:178]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.595655 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0625 19:38:27.595763 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0625 19:38:27.595866 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0625 19:38:27.595969 140282338469760 run_pretraining.py:178]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0625 19:38:27.596090 140282338469760 run_pretraining.py:178]   name = cls/predictions/output_bias:0, shape = (32000,)\n",
            "I0625 19:38:27.596205 140282338469760 run_pretraining.py:178]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0625 19:38:27.596317 140282338469760 run_pretraining.py:178]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0625 19:38:27.596510 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0625 19:38:27.598321 140282338469760 deprecation_wrapper.py:119] From /content/bert-japanese/src/../bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0625 19:38:27.606181 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0625 19:38:32.398603 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0625 19:38:41.775387 140282338469760 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0625 19:38:42.164427 140282338469760 estimator.py:1147] Done calling model_fn.\n",
            "I0625 19:38:46.161324 140282338469760 tpu_estimator.py:499] TPU job name worker\n",
            "I0625 19:38:47.572438 140282338469760 monitored_session.py:240] Graph was finalized.\n",
            "W0625 19:38:47.744163 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0625 19:38:47.907891 140282338469760 saver.py:1280] Restoring parameters from gs://gurebert/gureBERT/model/model.ckpt-610000\n",
            "W0625 19:39:14.770554 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0625 19:39:17.352350 140282338469760 session_manager.py:500] Running local_init_op.\n",
            "I0625 19:39:17.997242 140282338469760 session_manager.py:502] Done running local_init_op.\n",
            "I0625 19:39:29.001356 140282338469760 basic_session_run_hooks.py:606] Saving checkpoints for 610000 into gs://gurebert/gureBERT/model/model.ckpt.\n",
            "W0625 19:39:56.630694 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0625 19:39:58.038233 140282338469760 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0625 19:39:58.039453 140282338469760 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-06-25 19:39:58.039894: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0625 19:39:58.044787 140282338469760 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0625 19:39:58.047351 140282338469760 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0625 19:39:58.051084 140282338469760 tpu_estimator.py:557] Init TPU system\n",
            "I0625 19:40:05.458847 140282338469760 tpu_estimator.py:566] Initialized TPU in 7 seconds\n",
            "I0625 19:40:05.459836 140281195702016 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0625 19:40:05.460223 140281178138368 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0625 19:40:06.142425 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:40:06.143549 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:40:40.743851 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0625 19:41:40.791741 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (0, 892)\n",
            "I0625 19:41:49.880066 140282338469760 basic_session_run_hooks.py:262] loss = 0.6504045, step = 611000\n",
            "I0625 19:41:49.883220 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:41:49.883571 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:42:40.830728 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (1, 646)\n",
            "I0625 19:43:05.396509 140282338469760 basic_session_run_hooks.py:260] loss = 0.50328124, step = 612000 (75.516 sec)\n",
            "I0625 19:43:05.398332 140282338469760 tpu_estimator.py:2159] global_step/sec: 13.2422\n",
            "I0625 19:43:05.399384 140282338469760 tpu_estimator.py:2160] examples/sec: 847.5\n",
            "I0625 19:43:05.400921 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:43:05.401206 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:43:40.897238 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (2, 509)\n",
            "I0625 19:44:14.648333 140282338469760 basic_session_run_hooks.py:260] loss = 1.0970724, step = 613000 (69.252 sec)\n",
            "I0625 19:44:14.650186 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4401\n",
            "I0625 19:44:14.650439 140282338469760 tpu_estimator.py:2160] examples/sec: 924.163\n",
            "I0625 19:44:15.609725 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:44:15.610541 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:44:40.963851 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (3, 359)\n",
            "I0625 19:45:24.749946 140282338469760 basic_session_run_hooks.py:260] loss = 1.6517268, step = 614000 (70.102 sec)\n",
            "I0625 19:45:24.751514 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.2651\n",
            "I0625 19:45:24.751736 140282338469760 tpu_estimator.py:2160] examples/sec: 912.964\n",
            "I0625 19:45:24.752844 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:45:24.753101 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:45:40.979294 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (4, 223)\n",
            "I0625 19:46:34.015049 140282338469760 basic_session_run_hooks.py:260] loss = 1.376927, step = 615000 (69.265 sec)\n",
            "I0625 19:46:34.016614 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4373\n",
            "I0625 19:46:34.016878 140282338469760 tpu_estimator.py:2160] examples/sec: 923.986\n",
            "I0625 19:46:34.874679 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:46:34.875390 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:46:40.998494 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (5, 73)\n",
            "I0625 19:47:41.048521 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (5, 965)\n",
            "I0625 19:47:44.127715 140282338469760 basic_session_run_hooks.py:260] loss = 1.4815391, step = 616000 (70.113 sec)\n",
            "I0625 19:47:44.129233 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.2628\n",
            "I0625 19:47:44.129462 140282338469760 tpu_estimator.py:2160] examples/sec: 912.818\n",
            "I0625 19:47:44.130714 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:47:44.130944 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:48:41.099297 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (6, 829)\n",
            "I0625 19:48:54.730155 140282338469760 basic_session_run_hooks.py:260] loss = 0.98817366, step = 617000 (70.602 sec)\n",
            "I0625 19:48:54.731993 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.1637\n",
            "I0625 19:48:54.732534 140282338469760 tpu_estimator.py:2160] examples/sec: 906.479\n",
            "I0625 19:48:54.733717 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:48:54.733918 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:49:41.108434 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (7, 671)\n",
            "I0625 19:50:03.977603 140282338469760 basic_session_run_hooks.py:260] loss = 1.0366956, step = 618000 (69.247 sec)\n",
            "I0625 19:50:03.979490 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.441\n",
            "I0625 19:50:03.979758 140282338469760 tpu_estimator.py:2160] examples/sec: 924.222\n",
            "I0625 19:50:03.981185 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:50:03.981428 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:50:41.152918 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (8, 534)\n",
            "I0625 19:51:13.211040 140282338469760 basic_session_run_hooks.py:260] loss = 0.9602766, step = 619000 (69.233 sec)\n",
            "I0625 19:51:13.215205 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4434\n",
            "I0625 19:51:13.215537 140282338469760 tpu_estimator.py:2160] examples/sec: 924.379\n",
            "I0625 19:51:14.192095 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:51:14.192481 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:51:41.192183 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (9, 383)\n",
            "I0625 19:52:23.419371 140282338469760 basic_session_run_hooks.py:606] Saving checkpoints for 620000 into gs://gurebert/gureBERT/model/model.ckpt.\n",
            "W0625 19:52:46.765204 140282338469760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "I0625 19:52:51.234167 140282338469760 basic_session_run_hooks.py:260] loss = 0.8747845, step = 620000 (98.023 sec)\n",
            "I0625 19:52:51.235810 140282338469760 tpu_estimator.py:2159] global_step/sec: 10.2019\n",
            "I0625 19:52:51.236226 140282338469760 tpu_estimator.py:2160] examples/sec: 652.923\n",
            "I0625 19:52:51.237428 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:52:51.237631 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:52:52.456125 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (10, 0)\n",
            "I0625 19:53:52.494190 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (10, 892)\n",
            "I0625 19:54:00.463160 140282338469760 basic_session_run_hooks.py:260] loss = 1.1868722, step = 621000 (69.229 sec)\n",
            "I0625 19:54:00.464810 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.4448\n",
            "I0625 19:54:02.102097 140282338469760 tpu_estimator.py:2160] examples/sec: 924.469\n",
            "I0625 19:54:02.104723 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:54:02.105079 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:54:52.541748 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (11, 731)\n",
            "I0625 19:55:11.311046 140282338469760 basic_session_run_hooks.py:260] loss = 0.7853336, step = 622000 (70.848 sec)\n",
            "I0625 19:55:11.313117 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.1147\n",
            "I0625 19:55:11.313430 140282338469760 tpu_estimator.py:2160] examples/sec: 903.339\n",
            "I0625 19:55:11.314776 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:55:11.315082 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0625 19:55:52.561795 140281178138368 tpu_estimator.py:275] Outfeed finished for iteration (12, 595)\n",
            "I0625 19:56:21.423934 140282338469760 basic_session_run_hooks.py:260] loss = 1.150667, step = 623000 (70.113 sec)\n",
            "I0625 19:56:21.425779 140282338469760 tpu_estimator.py:2159] global_step/sec: 14.2628\n",
            "I0625 19:56:21.426093 140282338469760 tpu_estimator.py:2160] examples/sec: 912.816\n",
            "I0625 19:56:21.427527 140282338469760 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0625 19:56:21.427800 140282338469760 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MGl6QPM4B9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################\n",
        "##########################################\n",
        "\n",
        "# BERT-ekin konpatiblea den vocab sortzeko prozedura. wordpiece _ -> ##. \n",
        "# Honen abantaila BERT kodearen aldaketak eta run_squad.py, ... ezer aldatu gabe ibiliko dela da.\n",
        "# berez ez da wordpiece, sentencepiece baizik baina sintaktikoki konpatiblea\n",
        "# ikusi: https://colab.research.google.com/drive/1-uLyGTnz2K4gx3su6Qc00in9QCa4b5Kh#scrollTo=9S4CiOh3RzFW\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "559f9136-b061-48b5-fc01-bac5a987c93f",
        "id": "psBPQwy2y8UO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format('model/wiki-eu'))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 31999\n",
            "Sample tokens: ['.036', '▁ukuilu', 'lok', '▁ahaltsua', '▁batzuei', '▁erabili', '▁chapman', '▁tv', '▁pe', '▁polemika']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mia3KTjty8Vp",
        "colab": {}
      },
      "source": [
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "63bd9984-804d-47ea-dbff-1dc6aa131f45",
        "id": "9LbsNuney8V-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab\n",
        "\n",
        "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(32000 - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jxBd_J30y8WV",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c9f23d61-8211-4ac2-cef4-f963e7f8db16",
        "id": "OgRdyQpEy8Wh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from bert import tokenization\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(\"Nere kotxea aitonaren etxe alboan dago\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nere', 'kotxe', '##a', 'aitona', '##ren', 'etxe', 'alboan', 'dago']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bfd2e057-bd2b-4704-8cf8-ada627a37600",
        "id": "dAZRoG-Jy8W1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#!gsutil cp -r gs://gurebert/gureBERT/data/wiki/AA/wiki_00 wiki_00\n",
        "# ez du wiki-eu.model behar, vocab.txt-ekin nahikoa da\n",
        "\n",
        "!python bert/create_pretraining_data.py \\\n",
        "  --input_file=wiki_00 \\\n",
        "  --output_file=/tmp/tf_examples.tfrecord \\\n",
        "  --vocab_file=vocab.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=128 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=12345 \\\n",
        "  --dupe_factor=5\n",
        "\n",
        "#############################################################\n",
        "#############################################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf_examples.tfrecord  tmpcqwLhN\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}